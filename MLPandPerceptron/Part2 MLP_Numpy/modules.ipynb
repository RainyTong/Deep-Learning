{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer with forward and backward functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Module initialisation.\n",
    "        Args:\n",
    "            in_features: input dimension\n",
    "            out_features: output dimension\n",
    "        TODO:\n",
    "        1) Initialize weights self.params['weight'] using normal distribution with mean = 0 and \n",
    "        std = 0.0001.\n",
    "        2) Initialize biases self.params['bias'] with 0. \n",
    "        3) Initialize gradients with zeros.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        self.sum_grads = {}\n",
    "        self.params['weight'] = np.random.normal(loc=0, scale=0.0001, size=(in_features, out_features))\n",
    "        self.params['bias'] = 0\n",
    "        self.grads['weight'] = np.zeros((in_features, out_features))\n",
    "        self.grads['bias'] = 0\n",
    "        self.sum_grads['weight'] = np.zeros((in_features, out_features))\n",
    "        self.sum_grads['bias'] = 0\n",
    "        self.x = 0\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass (i.e., compute output from input).\n",
    "        Args:\n",
    "            x: input to the module\n",
    "        Returns:\n",
    "            out: output of the module\n",
    "        Hint: Similarly to pytorch, you can store the computed values inside the object\n",
    "        and use them in the backward pass computation. This is true for *all* forward methods of *all* modules in this class\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = x\n",
    "       \n",
    "        out = np.dot(self.x, self.params['weight']) + self.params['bias']\n",
    "       \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass (i.e., compute gradient).\n",
    "        Args:\n",
    "            dout: gradients of the previous module\n",
    "        Returns:\n",
    "            dx: gradients with respect to the input of the module\n",
    "        TODO:\n",
    "        Implement backward pass of the module. Store gradient of the loss with respect to \n",
    "        layer parameters in self.grads['weight'] and self.grads['bias']. \n",
    "        \"\"\"\n",
    "\n",
    "        db = dout\n",
    "        dw = np.dot(self.x.T, dout)\n",
    "        dx = np.dot(dout, self.params['weight'].T)  \n",
    "      \n",
    "        self.grads['weight'] = dw\n",
    "        self.grads['bias'] = db\n",
    "        self.sum_grads['weight'] += self.grads['weight']\n",
    "        self.sum_grads['bias'] += self.grads['bias']\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Layer with forward and backward functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(object):\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x: input to the module\n",
    "        Returns:\n",
    "            out: output of the module\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        z = np.zeros(len(x))\n",
    "        out = np.maximum(x, z)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        Args:\n",
    "            dout: gradients of the previous module\n",
    "        Returns:\n",
    "            dx: gradients with respect to the input of the module\n",
    "        \"\"\"\n",
    "        #dx = dout*derivative\n",
    "        # dx = (self.x > 0)*dout\n",
    "        g = np.zeros((1, len(self.x[0])))\n",
    "        for i in range(len(self.x[0])):\n",
    "            if self.x[0][i] > 0:\n",
    "                g[0][i] = 1\n",
    "            else:\n",
    "                g[0][i] = 0\n",
    "       \n",
    "        dx = g * dout\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftMax Layer with forward and backward functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(object):\n",
    "    \n",
    "    def exp_normalize(self, x):\n",
    "        b = x.max()\n",
    "        y = np.exp(x - b)\n",
    "        return y / y.sum()  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x: input to the module\n",
    "        Returns:\n",
    "            out: output of the module\n",
    "    \n",
    "        TODO:\n",
    "        Implement forward pass of the module. \n",
    "        To stabilize computation you should use the so-called Max Trick\n",
    "        https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "        \n",
    "        \"\"\"\n",
    "        out = self.exp_normalize(x)\n",
    "        return out\n",
    "\n",
    "    def backward(self, out, label_t):\n",
    "        \"\"\"\n",
    "        Backward pass. \n",
    "        Args:\n",
    "            dout: gradients of the previous module\n",
    "        Returns:\n",
    "            dx: gradients with respect to the input of the module\n",
    "        \"\"\"\n",
    "       \n",
    "        dx = out - label_t\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropy Layer with forward and backward functions:\n",
    "Note that in my code, the backward function of CrossEntropy layer is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x: input to the module\n",
    "            y: labels of the input\n",
    "        Returns:\n",
    "            out: cross entropy loss\n",
    "        \"\"\"\n",
    "        out = np.sum(- y * np.log(x))\n",
    "        return out\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        Args:\n",
    "            x: input to the module\n",
    "            y: labels of the input\n",
    "        Returns:\n",
    "            dx: gradient of the loss with respect to the input x.\n",
    "        \"\"\"\n",
    "        dx = -y/x\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
