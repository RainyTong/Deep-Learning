{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1 PyTorch LSTM\n",
    "\n",
    "### Task1\n",
    "\n",
    "As you’ve seen in the previous assignment, RNNs are unable to memorise long sequences. So, in this first part of assignment 3, you will improve on your palindrome task by implementing an LSTM instead.\n",
    "\n",
    "In this default case, I implement the LSTM model with the Palindromes' length of **11**. (T = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset import PalindromeDataset\n",
    "from lstm import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, batch_targets):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy, i.e., the average of correct predictions\n",
    "    of the network.\n",
    "    Args:\n",
    "        predictions: 2D float array of size [number_of_data_samples, n_classes]\n",
    "        labels: 2D int array of size [number_of_data_samples, n_classes] with one-hot encoding of ground-truth labels\n",
    "    Returns:\n",
    "        accuracy: scalar float, the accuracy of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = batch_targets.size(0)\n",
    "    correct = (predicted == batch_targets).sum().item()\n",
    "    accuracy = 100.0 * correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, input_length):\n",
    "\n",
    "    # Initialize the model that we are going to use\n",
    "    model = LSTM(input_length, config.input_dim, config.num_hidden, config.num_classes, config.batch_size)  # fixme\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize the dataset and data loader (leave the +1)\n",
    "    dataset = PalindromeDataset(input_length+1)\n",
    "    data_loader = DataLoader(dataset, config.batch_size, num_workers=1)\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # fixme\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)  # fixme\n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    loss = 0.0\n",
    "\n",
    "\n",
    "    for step, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
    "\n",
    "        # Add more code here ...\n",
    "        optimizer.zero_grad() \n",
    "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)   \n",
    "\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # the following line is to deal with exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=config.max_norm)\n",
    "\n",
    "        # Add more code here ...\n",
    "\n",
    "        loss += loss.item()   # fixme\n",
    "        accu = 0.0  # fixme\n",
    "        \n",
    "\n",
    "        if step % 10 == 0:\n",
    "            # print acuracy/loss here\n",
    "            print('[step: %5d] loss: %.4f' %\n",
    "                          (step, loss / 10))\n",
    "            losses.append(loss / 10)\n",
    "            loss = 0.0\n",
    "            accu = accuracy(outputs, batch_targets)\n",
    "            accuracies.append(accu)\n",
    "            print('Accuracy on training dataset: %.3f %%' % (accu))\n",
    "\n",
    "\n",
    "        if step == config.train_steps:\n",
    "            # If you receive a PyTorch data-loader error, check this bug report:\n",
    "            # https://github.com/pytorch/pytorch/pull/9655\n",
    "            break\n",
    "\n",
    "    print('Done training.')\n",
    "    \n",
    "    return model, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parse training configuration\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Model params\n",
    "    parser.add_argument('--input_length', type=int, default=10, help='Length of an input sequence')\n",
    "    parser.add_argument('--input_dim', type=int, default=1, help='Dimensionality of input sequence')\n",
    "    parser.add_argument('--num_classes', type=int, default=10, help='Dimensionality of output sequence')\n",
    "    parser.add_argument('--num_hidden', type=int, default=128, help='Number of hidden units in the model')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Number of examples to process in a batch')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate')\n",
    "    parser.add_argument('--train_steps', type=int, default=10000, help='Number of training steps')\n",
    "    parser.add_argument('--max_norm', type=float, default=10.0)\n",
    "\n",
    "    config = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/wangyutong/Repository/store/CS/大三下/深度学习/lab/Assignment3/Part1/lstm.py:72: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y = F.softmax(output)\n",
      "__main__:35: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:     0] loss: 0.4605\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:    10] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:    20] loss: 0.4600\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:    30] loss: 0.4612\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:    40] loss: 0.4604\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:    50] loss: 0.4609\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:    60] loss: 0.4603\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:    70] loss: 0.4586\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:    80] loss: 0.4559\n",
      "Accuracy on training dataset: 20.312 %\n",
      "[step:    90] loss: 0.4529\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:   100] loss: 0.4403\n",
      "Accuracy on training dataset: 22.656 %\n",
      "[step:   110] loss: 0.4295\n",
      "Accuracy on training dataset: 28.125 %\n",
      "[step:   120] loss: 0.4281\n",
      "Accuracy on training dataset: 29.688 %\n",
      "[step:   130] loss: 0.4268\n",
      "Accuracy on training dataset: 30.469 %\n",
      "[step:   140] loss: 0.4304\n",
      "Accuracy on training dataset: 28.125 %\n",
      "[step:   150] loss: 0.4079\n",
      "Accuracy on training dataset: 53.125 %\n",
      "[step:   160] loss: 0.4377\n",
      "Accuracy on training dataset: 27.344 %\n",
      "[step:   170] loss: 0.4618\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:   180] loss: 0.4553\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:   190] loss: 0.4292\n",
      "Accuracy on training dataset: 28.906 %\n",
      "[step:   200] loss: 0.4022\n",
      "Accuracy on training dataset: 50.000 %\n",
      "[step:   210] loss: 0.3987\n",
      "Accuracy on training dataset: 53.906 %\n",
      "[step:   220] loss: 0.4597\n",
      "Accuracy on training dataset: 17.188 %\n",
      "[step:   230] loss: 0.4561\n",
      "Accuracy on training dataset: 17.969 %\n",
      "[step:   240] loss: 0.4457\n",
      "Accuracy on training dataset: 21.094 %\n",
      "[step:   250] loss: 0.4323\n",
      "Accuracy on training dataset: 28.906 %\n",
      "[step:   260] loss: 0.4053\n",
      "Accuracy on training dataset: 37.500 %\n",
      "[step:   270] loss: 0.4064\n",
      "Accuracy on training dataset: 45.312 %\n",
      "[step:   280] loss: 0.3916\n",
      "Accuracy on training dataset: 55.469 %\n",
      "[step:   290] loss: 0.3928\n",
      "Accuracy on training dataset: 53.906 %\n",
      "[step:   300] loss: 0.4154\n",
      "Accuracy on training dataset: 35.938 %\n",
      "[step:   310] loss: 0.4021\n",
      "Accuracy on training dataset: 45.312 %\n",
      "[step:   320] loss: 0.3894\n",
      "Accuracy on training dataset: 56.250 %\n",
      "[step:   330] loss: 0.3784\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:   340] loss: 0.3631\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:   350] loss: 0.3752\n",
      "Accuracy on training dataset: 60.938 %\n",
      "[step:   360] loss: 0.3586\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:   370] loss: 0.3608\n",
      "Accuracy on training dataset: 73.438 %\n",
      "[step:   380] loss: 0.3569\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:   390] loss: 0.3610\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:   400] loss: 0.3523\n",
      "Accuracy on training dataset: 71.875 %\n",
      "[step:   410] loss: 0.3438\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:   420] loss: 0.3507\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:   430] loss: 0.4091\n",
      "Accuracy on training dataset: 41.406 %\n",
      "[step:   440] loss: 0.4210\n",
      "Accuracy on training dataset: 35.156 %\n",
      "[step:   450] loss: 0.3960\n",
      "Accuracy on training dataset: 46.875 %\n",
      "[step:   460] loss: 0.3657\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:   470] loss: 0.3367\n",
      "Accuracy on training dataset: 80.469 %\n",
      "[step:   480] loss: 0.3425\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:   490] loss: 0.3497\n",
      "Accuracy on training dataset: 71.875 %\n",
      "[step:   500] loss: 0.3365\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:   510] loss: 0.3404\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:   520] loss: 0.3369\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:   530] loss: 0.3369\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:   540] loss: 0.3414\n",
      "Accuracy on training dataset: 75.781 %\n",
      "[step:   550] loss: 0.3301\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:   560] loss: 0.3213\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:   570] loss: 0.3321\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:   580] loss: 0.3357\n",
      "Accuracy on training dataset: 80.469 %\n",
      "[step:   590] loss: 0.3228\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:   600] loss: 0.3232\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:   610] loss: 0.3175\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:   620] loss: 0.3760\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:   630] loss: 0.4470\n",
      "Accuracy on training dataset: 21.875 %\n",
      "[step:   640] loss: 0.4623\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:   650] loss: 0.4438\n",
      "Accuracy on training dataset: 23.438 %\n",
      "[step:   660] loss: 0.4378\n",
      "Accuracy on training dataset: 25.000 %\n",
      "[step:   670] loss: 0.4336\n",
      "Accuracy on training dataset: 27.344 %\n",
      "[step:   680] loss: 0.4160\n",
      "Accuracy on training dataset: 42.188 %\n",
      "[step:   690] loss: 0.4378\n",
      "Accuracy on training dataset: 28.906 %\n",
      "[step:   700] loss: 0.4108\n",
      "Accuracy on training dataset: 42.969 %\n",
      "[step:   710] loss: 0.4424\n",
      "Accuracy on training dataset: 25.000 %\n",
      "[step:   720] loss: 0.3997\n",
      "Accuracy on training dataset: 45.312 %\n",
      "[step:   730] loss: 0.4078\n",
      "Accuracy on training dataset: 42.188 %\n",
      "[step:   740] loss: 0.3830\n",
      "Accuracy on training dataset: 55.469 %\n",
      "[step:   750] loss: 0.3343\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:   760] loss: 0.3137\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:   770] loss: 0.3181\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:   780] loss: 0.3242\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:   790] loss: 0.3119\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:   800] loss: 0.3043\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:   810] loss: 0.3131\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:   820] loss: 0.3174\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:   830] loss: 0.3009\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:   840] loss: 0.3158\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:   850] loss: 0.3083\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:   860] loss: 0.3142\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:   870] loss: 0.3142\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:   880] loss: 0.3156\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:   890] loss: 0.3142\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:   900] loss: 0.3155\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:   910] loss: 0.3035\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:   920] loss: 0.3095\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:   930] loss: 0.3125\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:   940] loss: 0.3110\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:   950] loss: 0.3124\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:   960] loss: 0.3064\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:   970] loss: 0.3109\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:   980] loss: 0.3049\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:   990] loss: 0.3079\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  1000] loss: 0.3093\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  1010] loss: 0.3183\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  1020] loss: 0.3078\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  1030] loss: 0.3168\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  1040] loss: 0.3108\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  1050] loss: 0.3168\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  1060] loss: 0.3183\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  1070] loss: 0.3077\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  1080] loss: 0.3183\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  1090] loss: 0.3198\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  1100] loss: 0.3213\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  1110] loss: 0.3062\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  1120] loss: 0.3167\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  1130] loss: 0.3107\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  1140] loss: 0.3077\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  1150] loss: 0.3077\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  1160] loss: 0.3062\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  1170] loss: 0.3212\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  1180] loss: 0.3122\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  1190] loss: 0.3091\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  1200] loss: 0.3076\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  1210] loss: 0.3091\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  1220] loss: 0.3076\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  1230] loss: 0.3061\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  1240] loss: 0.3031\n",
      "Accuracy on training dataset: 94.531 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  1250] loss: 0.3136\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  1260] loss: 0.3106\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  1270] loss: 0.3166\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  1280] loss: 0.3091\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  1290] loss: 0.3136\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  1300] loss: 0.3121\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  1310] loss: 0.2985\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  1320] loss: 0.3106\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  1330] loss: 0.3060\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  1340] loss: 0.3120\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  1350] loss: 0.3120\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  1360] loss: 0.3165\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  1370] loss: 0.3135\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  1380] loss: 0.3150\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  1390] loss: 0.3120\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  1400] loss: 0.3080\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  1410] loss: 0.2983\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1420] loss: 0.2958\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1430] loss: 0.2941\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1440] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1450] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1460] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1470] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1480] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1490] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1500] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1510] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1520] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1530] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1540] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1550] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1560] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1570] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1580] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1590] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1600] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1610] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1620] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1630] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1640] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1650] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1660] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1670] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1680] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1690] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1700] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1710] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1720] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1730] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1740] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1750] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1760] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1770] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1780] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1790] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1800] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1810] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1820] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1830] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1840] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1850] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1860] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1870] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1880] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1890] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1900] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1910] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1920] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1930] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1940] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1950] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1960] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1970] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1980] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1990] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2000] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2010] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2020] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2030] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2040] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2050] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2060] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2070] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2080] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2090] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2100] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2110] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2120] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2130] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2140] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2150] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2160] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2170] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2180] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2190] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2200] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2210] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2220] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2230] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2240] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2250] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2260] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2270] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2280] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2290] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2300] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2310] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2320] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2330] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2340] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2350] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2360] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2370] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2380] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2390] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2400] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2410] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2420] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2430] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2440] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2450] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2460] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2470] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  2480] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2490] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2500] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2510] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2520] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2530] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2540] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2550] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2560] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2570] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2580] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2590] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2600] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2610] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2620] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2630] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2640] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2650] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2660] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2670] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2680] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2690] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2700] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2710] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2720] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2730] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2740] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2750] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2760] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2770] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2780] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2790] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2800] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2810] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2820] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2830] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2840] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2850] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2860] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2870] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2880] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2890] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2900] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2910] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2920] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2930] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2940] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2950] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2960] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2970] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2980] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2990] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3000] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3010] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3020] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3030] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3040] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3050] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3060] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3070] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3080] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3090] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3100] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3110] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3120] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3130] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3140] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3150] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3160] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3170] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3180] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3190] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3200] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3210] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3220] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3230] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3240] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3250] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3260] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3270] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3280] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3290] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3300] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3310] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3320] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3330] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3340] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3350] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3360] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3370] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3380] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3390] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3400] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3410] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3420] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3430] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3440] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3450] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3460] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3470] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3480] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3490] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3500] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3510] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3520] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3530] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3540] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3550] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3560] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3570] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3580] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3590] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3600] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3610] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3620] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3630] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3640] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3650] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3660] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3670] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3680] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3690] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3700] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  3710] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3720] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3730] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3740] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3750] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3760] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3770] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3780] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3790] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3800] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3810] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3820] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3830] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3840] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3850] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3860] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3870] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3880] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3890] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3900] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3910] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3920] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3930] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3940] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3950] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3960] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3970] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3980] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3990] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4000] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4010] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4020] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4030] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4040] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4050] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4060] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4070] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4080] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4090] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4100] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4110] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4120] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4130] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4140] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4150] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4160] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4170] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4180] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4190] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4200] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4210] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4220] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4230] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4240] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4260] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4300] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4370] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  4940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  6170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  7400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  8630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  9860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step: 10000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "model, losses, accuracies = train(config, config.input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3XucXWV97/HPd++5EHIhQCYKSTABgm2AFiHcvNeCxktBi1WQKl6RIxxp9fQYz0FqI3qqtlhtUQsK1laMF7ykEJsWLFaoYCYYhQQiIYAkgIRLEkJuc/mdP9bae9bs2TOzJ3vv2ZmV7/v12s5az3rWyrNmY3557ooIzMzM9lah1QUwM7OJzYHEzMzq4kBiZmZ1cSAxM7O6OJCYmVldHEjMzKwuDiRm+yFJt0l6Z6vLYfngQGL7NUkPSTpjnP/M/yNpe/rZJakvc75mPMti1ggOJGbjLCI+FRFTImIKcBHws9J5RBxbmV9S2/iX0qx2DiRmw5D0PknrJT0taZmkw9N0SfqcpCckbZN0t6Tj0muvk7RW0rOSNkn6X3vx57ZJCkkfkLQeuC9NXyDp5rQ890k6J3PPv0j6gqQfpX/2zyTNy1xfJGmdpK2SPg+o3t+PWYkDiVkVkl4F/D/gLcBhwMPA0vTyq4GXA8cAB6V5nkqvfRV4f0RMBY4DflxHMc4CTgaOlzQF+A/g68BM4HzgakkvzOR/G/Ax4BDgN8An0neZCXwXWAzMADYCp9ZRLrNBHEjMqjsfuDYi7oqI3cBHgdMlzQV6gKnA7wCKiHsj4rH0vh5ggaRpEfFMRNxVRxk+lT5jJ3A28OuI+HpE9EbEKuAHwJsz+b8bEd0R0QN8AzghTX8DsDoivp9e+1tgcx3lMhvEgcSsusNJaiEARMR2klrHrIj4MfAPwFXAE5KuljQtzXoO8DrgYUk/kXR6HWV4JHP8AuAlkraUPsBbSWpLJY9njncAUzLvUn5WRPST1ErMGsKBxKy6R0n+8gZA0mTgUGATQER8ISJOAhaQNHH9RZq+MiLOJml++gHw7TrKkF2a+xHgloiYnvlMiYhLanjOY8CczLsUgNl1lMtsEAcSM2iXdEDm0wZ8E3iXpBMkdQKfAu6MiIcknSzpVEntwHPALqBfUoek8yUdlDYhbQP6G1TGZcCxkt4mqT39nFLRRzKcG4ETJJ2dlvnPga4GlcvMgcQMWA7szHw+HhE3k3Rc30DyL/qjgHPT/NOAa4BnSJq/ngI+m157O/CQpG0kQ3vPb0QBI2Ir8BrgT9PyPE4yGKCzhnt/S9IM9lngSeAI4M5GlMsMko7CVpfBzMwmMNdIzMysLg4kZmZWFwcSMzOriwOJmZnVZb9YDG7GjBkxd+7cVhfDzGxCWbVq1ZMRMepQ8f0ikMydO5fu7u5WF8PMbEKR9PDoudy0ZWZmdXIgMTOzujiQmJlZXRxIzMysLg4kZmZWFwcSMzOriwOJmZnVxYFknOzY08v37tqIV1s2s7xxIBnFY1t3smNPb/n8r390H2//6ti3cljyr2v50Ld/yc8ffLqRxTMza7n9Ymb73vra7Q/y8X9dC8AdH/1Dnn/QAXz5Jw/s1bMe37YLgB17+hpWPjOzfYFrJCMoBRGAb618ZNC1a297kK07e2p+llu0zCyvHEhGcPfHX827XzKPjmKB9Zu3D7q25Ma1/MV3flnzs8pxRI0rn5nZvsBNWyOYekA7l//RAh7YvJ0Hn9w+5Ppvnt4x5mc6jphZ3rhGUoPnTetk87O7h6RvG1PTVlInkRxKzCxfHEhqMGNKJ09t30N//+COji1jCCQlDiNmljcOJDXomtpJb38M6Vwfywgsd7abWV45kNRgxpROADZvH9q8Vaug1LTVkCKZme0zHEhqUAokT1bpJxmrftdMzCxnHEhq0DW1A6izRpIGkL7+/kYUycxsn9HUQCJpkaR1ktZLWjxCvnMkhaSF6flcSTslrU4/X87kPUnS3ekzv6BxGAZ10KQkkIxllFalUiDp7XOVxMzypWnzSCQVgauAM4GNwEpJyyJibUW+qcClQOUCVg9ExAlVHv0l4H1p/uXAIuBHDS7+IB1tSbzdU0cQKPWR9LvX3cxyppk1klOA9RGxISL2AEuBs6vk+wTwaWDXaA+UdBgwLSLuiGRixteBNzawzFW1F5NKT2/f3jdLlWsk7iQxs5xpZiCZBWQXqNqYppVJOhGYExE3Vbl/nqRfSPqJpJdlnrlxpGc2Q3sx+TX11BFISvocSMwsZ1q2RIqkAnAl8M4qlx8DjoiIpySdBPxA0rFjfP6FwIUARxxxRF1lbSskNZL6mrYSDiRmljfNrJFsAuZkzmenaSVTgeOAWyU9BJwGLJO0MCJ2R8RTABGxCngAOCa9f/YIzyyLiKsjYmFELOzq6qrrRSTRUSzUVyNx05aZ5VQzA8lKYL6keZI6gHOBZaWLEbE1ImZExNyImAvcAZwVEd2SutLOeiQdCcwHNkTEY8A2Saelo7XeAfywie9Q1lZUfX0kpc52BxIzy5mmNW1FRK+kS4AVQBG4NiLWSFoCdEfEshFufzmwRFIP0A9cFBGlrQU/AHwNmEQyWqupI7ZK2osFeupo2lK6ypZrJGaWN03tI4mI5SRDdLNplw+T95WZ4xuAG4bJ103SJDau2osF9tRRIynNdnEfiZnljWe216i9KHp69z6QFNMOewcSM8sbB5IatRcLdTVLlQJJr5dIMbOccSCpUXtRVZu2osaZ6p1tRSBZev6We3/rTnczyw0Hkhq1FwtVm7ZqraWkFRKW/vwR3vNP3Xztvx9qYOnMzFrHgaRG7cPMI6l1bkkp3Dy+LVkJ5tEtOxtVNDOzlnIgqVF7UVVrHz29tdVIKlvAvMGVmeWFA0mN2osF9lRp2uqpufPcfSJmlk8OJDXqaKuzaWtIjcRVEjPLBweSGrUVVHVme81NWxXnDiNmlhcOJDUatrO9xqatIcOEHUnMLCccSGrUXm/TVqMLZGa2j3AgqVH7ME1bte7B7h12zSyvHEhqNFzTVq0LOQ7tI3HblpnlgwNJjZKmrWqd7XvZR2JmlhMOJDUabofEvV3I0aN/zSwvHEhqlAz/bdw8EjOzvHAgqVF7W4Ede/qGpNcaIKKil8QVEjPLCweSGrUXq/+qat2oyjUSM8srB5IaTe2svitxf40Rwos2mlleOZDU6IXPn1o1veZAMqRpy5HEzPKhqYFE0iJJ6yStl7R4hHznSApJC9PzMyWtknR3+vNVmby3ps9cnX5mNvMdShYcPq1qeq2Dtty0ZWZ5Vb29pgEkFYGrgDOBjcBKScsiYm1FvqnApcCdmeQngT+KiEclHQesAGZlrp8fEd3NKns1M6Z0Vk2vuY+k4txNW2aWF82skZwCrI+IDRGxB1gKnF0l3yeATwO7SgkR8YuIeDQ9XQNMklT9b/JxtOjY5w9Jq7Vpy4ttmVleNTOQzAIeyZxvZHCtAkknAnMi4qYRnnMOcFdE7M6kXZc2a31M47ixx8xpQ2PZ3veRmJnlQ8s62yUVgCuBD4+Q51iS2sr7M8nnR8TxwMvSz9uHufdCSd2Sujdv3tyYMldJq3WDRPeRmFleNTOQbALmZM5np2klU4HjgFslPQScBizLdLjPBr4PvCMiHijdFBGb0p/PAteTNKENERFXR8TCiFjY1dXVsJeq1FdjhBhSc3EniZnlRDMDyUpgvqR5kjqAc4FlpYsRsTUiZkTE3IiYC9wBnBUR3ZKmAzcBiyPi9tI9ktokzUiP24E3APc08R0GqdaKVutijN4h0czyqmmBJCJ6gUtIRlzdC3w7ItZIWiLprFFuvwQ4Gri8YphvJ7BC0q+A1SQ1nGua9Q61qFxqa/Ozu7lh1cYh+dy0ZWZ51bThvwARsRxYXpF2+TB5X5k5vgK4YpjHntSo8jVCZZPVe/9pJb/cuJWXHTODmVMPKKd7+K+Z5ZVnttepMpBs2pKOYq6MHK6SmFlOOZCMQbVaRH/FhMRSYCkUBmf2DolmllcOJHWqnNhemuleWQGJgIJjh5nlkAPJGFSrRVQ2bfWXA8ng9CCqjvoyM5voHEjGoGrTVkXAKM0rqZxfEgHFzAMcU8wsLxxI6vSp5ffxis/+Z/m81LRV2eQVAS8++tDyueOImeWFA8kYDPeX/8NP7SgflwNJf2XTFrQVBn7drpGYWV44kDRYqUmrsskrIhw8zCyXHEjGoJZAUIof1bYpUZV8ZmYTnQNJk1RueBUxOBDVurOimdm+zoFkDMYyfLfq8N9MnaTmDbHMzPZxDiRNUm34rwT/8LYXJeetKJSZWRM4kIzBWPrKKze8CpJA8obfOxyp9uXnzcz2dQ4kTVJ11FYaiiJg++7eVhTLzKzhHEjG4OS5h9Scd0gggUFVmutuf6ghZTIzazUHkjE4Y8HzWHXZGVWvVTZVDRmVFZ7Nbmb55EAyRodO6ayaXjncd8jwX8Y26svMbKJwIGmQ3iHzRqr1kZiZ5Y8DSR1OnTfQZ9JTsXl79RrJeJTKzGx8OZDU4fDpk8rHPX0j95GE+0jMLKeaGkgkLZK0TtJ6SYtHyHeOpJC0MJP20fS+dZJeM9Znjoe2zJaHvRU1Em9sZWb7i7ZmPVhSEbgKOBPYCKyUtCwi1lbkmwpcCtyZSVsAnAscCxwO3CzpmPTyqM8cL+1tA3G4p7KzvdrM9nEplZnZ+GpmjeQUYH1EbIiIPcBS4Owq+T4BfBrYlUk7G1gaEbsj4kFgffq8Wp85LtpHqJFUa9pyJDGzPGpmIJkFPJI535imlUk6EZgTETfVeO+ozxxP7cVMjaSyj6TK8r4FN22ZWQ61rLNdUgG4Evhwk55/oaRuSd2bN29u6LOnHdCW/hkDab39lTWSys73geG/B7R7jIOZ5Ucz/0bbBMzJnM9O00qmAscBt0p6CDgNWJZ2uA9372jPLIuIqyNiYUQs7OrqqvNVBvvbt5zAkV2TmX5gRzmtt2+UCYmZ/Uje//KjSmVsaLnMzFqhmYFkJTBf0jxJHSSd58tKFyNia0TMiIi5ETEXuAM4KyK603znSuqUNA+YD/x8tGeOlzMXPI8ff/iVTGovltNK80hKwWJIH0lmP5JSE5c3tzKzPGjaqK2I6JV0CbACKALXRsQaSUuA7ogYNgCk+b4NrAV6gYsjog+g2jOb9Q6jaSsOtG2V+khEMvlw6Mz2gSBT6lrp6w+KBfebmNnE1rRAAhARy4HlFWmXD5P3lRXnnwQ+WcszW6WtMFCh6y3XSAQRQ4f/MhBICoVSjcRVEjOb+NzrW4dBNZL+gRoJDDP8N71alAOJmeWHA0kdqs1sL/d/VFlHvlwjSQ8qO+TNzCYiB5I6ZPs3yvNIyp3tw89sLzdtVWzHa2Y2ETmQ1OHxrQOT8UvzSEqxZaTVf4vDBBszs4nIgaQOxzx/avm4tzxqa2Bf9qzsnu2lGkllh7yZ2UTkQFKHP3jhTFb82cuBavNIRhi1NWw/ipnZxONAUqdpk5IR1L0Vo7ZGWv23WPCERDPLDweSOpUWbuzJziOh2vDfgf1Iyv0obtoysxxwIKlTe6EUSNIaSalpq0pne4mbtswsTxxI6lSalFie2Z6mDxmRNWiJFM8jMbP8cCCpUzmQpEGhMEyQSPa1Sme2e4kUM8sRB5I6DTRtDa6RVB3+m16Ul0gxsxxxIKlToSAKyswjGSZIZHfaLZaXSBmvUpqZNU9NgUTSUZI60+NXSvqgpOnNLdrE0VYs0NM/uEZSdfhvxTLyrpGYWR7UWiO5AeiTdDRwNckuhdc3rVQTTHtBmRpJkja0r31g+K+8aKOZ5UitgaQ/InqBNwF/HxF/ARzWvGJNLG3FwuD9SBhmq930uJDd7N3MbIKrNZD0SDoPuAC4MU1rb06RJp72YoE9fYNntlfrIyldHHaIsJnZBFRrIHkXcDrwyYh4MN1H/Z+bV6yJpb2oco2kZMhkwxgY/jtc85eZ2URU01a7EbEW+CCApIOBqRHx6WYWbCJpK6o8j6QUG4bGkaEbWzmOmFke1Dpq61ZJ0yQdAtwFXCPpyuYWbeJoLxTK80hKtYyRNrYabvMrM7OJqNamrYMiYhvwx8DXI+JU4IzmFWtiaSsOjNoq1TOGDP9l6DLyjiNmlge1BpI2SYcBb2Ggs31UkhZJWidpvaTFVa5fJOluSasl3SZpQZp+fppW+vRLOiG9dmv6zNK1mbWWp1naCoXyDoml4DDSxlbKpJmZTXS1BpIlwArggYhYKelI4P6RbpBUBK4CXgssAM4rBYqM6yPi+Ig4AfgMcCVARHwjIk5I098OPBgRqzP3nV+6HhFP1PgOTdNeFHf9Zgv9/VFurqq21lahvETKQJqZ2URXa2f7d4DvZM43AOeMctspwPo0L5KWAmcDazPP2ZbJP5nqf7eeByytpZyt8suNWwG45qcbMp3tQ/tIKO9H4qYtM8uPWjvbZ0v6vqQn0s8NkmaPctss4JHM+cY0rfLZF0t6gKRG8sEqz3kr8M2KtOvSZq2PSdVn90m6UFK3pO7NmzePUtTGWPf4swOd7ZkaSakJq1RQzyMxszyptWnrOmAZcHj6+dc0rW4RcVVEHAV8BLgse03SqcCOiLgnk3x+RBwPvCz9vH2Y514dEQsjYmFXV1cjijoqSeWgkW3ZKsULVUQSxxEzy4NaA0lXRFwXEb3p52vAaH87byJZk6tkdpo2nKXAGyvSzqWiNhIRm9Kfz5Ks93XK6MUfHxJVm7ZKR6XO9oF5JI4kZjbx1RpInpL0p5KK6edPgadGuWclMF/SPEkdJEFhWTaDpPmZ09eT6cCXVCAZJbY0k9YmaUZ63A68AcjWVlqqIMpRY1AgKTVtVSyR4hqJmeVBTZ3twLuBvwc+R/JX5X8D7xzphojolXQJyWivInBtRKyRtATojohlwCWSzgB6gGdI1vIqeTnwSKmzPtUJrEiDSBG4GbimxndoOqGBGklmxZSBGkmitIuiA4mZ5UGto7YeBs7Kpkn6M+DvRrlvObC8Iu3yzPGlI9x7K3BaRdpzwEm1lLkVCoWBmkjfoBpJ8rOyRuLOdjPLg3p2SPxQw0qRG6q6REqpL2RgP5JSupnZxFdPIPGmGhUKGggag4f/Ds6n8jwShxIzm/jqCST+W7BCQdkaydDr7mw3szwasY9E0rNUDxgCJjWlRBOQNLAne9Xhv6U+kvJ+JB7+a2b5MWIgiYip41WQiaytIHr6IpkfMmIfSXJe8IREM8uRepq2LJVdpWWgj2Tg+kCNpPQzOarW/GVmNtE4kDRCZnhv9VFbA9ezP93ZbmZ54EDSAIuOez6QdLb3l9faqrZo4+Dhv66RmFkeOJA0wN/8ye+Xj6vt2T6kRjIwbqvZRTMza7pal0ixEXS0JfH4q7c9WE6rNmqrpFConm5mNhG5RtIkfVWqJOWZ7e5sN7MccSBpkhgURwZvbFUe/uumLTPLAQeSJqk6IbFi1JZrJGaWBw4kDTJ/5pRB59mmrcpl5EtHHv5rZnngQNIg55w0eAv7QU1b5Y2tSjskjluxzMyazoGkQd7z0nmDzkeekKgheczMJioHkgZpLxb4vdkHlc+rL9o4+KfjiJnlgQNJA3UUB36dfVVGbVFu2vJWu2aWHw4kDVSamAgVHemVNZLyqC1HEjOb+BxIGigbSGpatHGcymVm1kxNDSSSFklaJ2m9pMVVrl8k6W5JqyXdJmlBmj5X0s40fbWkL2fuOSm9Z72kLyi7hnuLtWebtjLLyJeCSqG8Z7uH/5pZfjQtkEgqAlcBrwUWAOeVAkXG9RFxfEScAHwGuDJz7YGIOCH9XJRJ/xLwPmB++lnUrHcYq2wfSbiz3cz2E82skZwCrI+IDRGxB1gKnJ3NEBHbMqeTGaW1R9JhwLSIuCOSv6m/DryxscXee9m60X2PP8vaR5PXq2zaKne2j2PZzMyapZmBZBbwSOZ8Y5o2iKSLJT1AUiP5YObSPEm/kPQTSS/LPHPjaM9Mn3uhpG5J3Zs3b67nPWpWGRhe94WfJunD7kfiUGJmE1/LO9sj4qqIOAr4CHBZmvwYcEREvAj4EHC9pGljfO7VEbEwIhZ2dXU1ttDD6B9m8ayoWCPFTVtmlifNDCSbgDmZ89lp2nCWkjZTRcTuiHgqPV4FPAAck96fXYtktGeOq4FO9erXB4b/umnLzPKjmYFkJTBf0jxJHcC5wLJsBknzM6evB+5P07vSznokHUnSqb4hIh4Dtkk6LR2t9Q7gh018hzEZ2K+9enp5PxLv2W5mOdK0HRIjolfSJcAKoAhcGxFrJC0BuiNiGXCJpDOAHuAZ4IL09pcDSyT1AP3ARRHxdHrtA8DXgEnAj9LPPmG4ZeGH7kfime1mlh9N3Wo3IpYDyyvSLs8cXzrMfTcANwxzrRs4roHFbJjhahhD9iNJ093ZbmZ50PLO9jwZbmrksDPbHUfMLAccSBpoydnH8daFcziya/Kg9KHDf93Zbmb54UDSQIdPn8Sn3/x7VCz2O0KNxKHEzCY+B5Im2NXTByRNV8dc9iO27+oddN3zSMwsTxxImmB378CKjXt6+3n46R1AdqvdUtOWI4mZTXwOJE2wJxNIADqKad9Iej6wRMo4FsrMrEkcSJpgV2/foPOedLvEIYs2OpCYWQ44kDRBT9/gCFFq6hKDxwd7HomZ5YEDyTjYndZQKkdtmZnlgQNJE1z2+t8ddP7va34LVFsixTUSM5v4HEia4L0vO5L7PrGI6951MgA/+XWyH8rQJVJaUDgzswZzIGmSA9qLzJjcWZE6eGa7+0jMLA8cSJqoo23wr3dg1Fby0zUSM8sDB5Imai8O7lXPbmxV0PA7KpqZTSQOJE00tEYyEFiKBdHnpi0zywEHkiYaEkgyx8WCXCMxs1xwIGmijmL1PhKAokSvA4mZ5YADSRMN19kOUCiIPgcSM8sBB5ImGlojGdxH4uG/ZpYHDiRNVCxUH7UF0OYaiZnlRFMDiaRFktZJWi9pcZXrF0m6W9JqSbdJWpCmnylpVXptlaRXZe65NX3m6vQzs5nvUA9VLKqVPS/IgcTM8qGtWQ+WVASuAs4ENgIrJS2LiLWZbNdHxJfT/GcBVwKLgCeBP4qIRyUdB6wAZmXuOz8iuptV9mapHLXlQGJmedDMGskpwPqI2BARe4ClwNnZDBGxLXM6mXR784j4RUQ8mqavASZJqlxvZMIZ1NkuzyMxs3xoZiCZBTySOd/I4FoFAJIulvQA8Bngg1Wecw5wV0TszqRdlzZrfUyV7UcDz71QUrek7s2bN+/9WzRQdj+SnT19fO+uTSz75aMj3GFmtu9reWd7RFwVEUcBHwEuy16TdCzwaeD9meTzI+J44GXp5+3DPPfqiFgYEQu7urqaU/gxyoa8p5/bA8A1/7WhRaUxM2uMZgaSTcCczPnsNG04S4E3lk4kzQa+D7wjIh4opUfEpvTns8D1JE1oE0K1qlNb0btcmdnE1sxAshKYL2mepA7gXGBZNoOk+ZnT1wP3p+nTgZuAxRFxeyZ/m6QZ6XE78Abgnia+Q9O1F0f+Crbu7OErP91Af39w068ecwe9me1zmhZIIqIXuIRkxNW9wLcjYo2kJekILYBLJK2RtBr4EHBBKR04Gri8YphvJ7BC0q+A1SQ1nGua9Q6N8LZTjygf7+zpG3K9coXgSh/+9mquuOlePrX8Xi6+/i6+epubwsxs39K04b8AEbEcWF6Rdnnm+NJh7rsCuGKYx57UsAKOg0+96Xi27NjD8rsfZ8eeoYGkrTByLO9++BkAnt3VC8CjW3Y1vpBmZnVoeWf7/mBSexKvd1YJJMM1bT27q4f+/mDLjh4AIhkZ7WVVzGyf09QaiSUO7CgCsGNPb5WrQV9/DFpO5cntu1l4xc2Dc6Xxw30kZravcY1kHJz4gukAvODQyUOu3XzvE7zj2jsHpT2+dWjz1VPpcOFSjWTrzp7ytXAtxcxayDWScfCmF83m+FkHcfTMqVWv377+qVGf8eP7ngCSGsm9j23jtZ//KZ8/9wQ2P7ubK266l3v+6jVM6fTXaWbjzzWScTJcECmZu/gm5i6+iZ6+fkaqYPT1w9pHk5VlfvLrzfzzHQ8DsGbT1oaV1cxsLBxI9jHbdvaMuAZXf0S5easglfc8eevVd4xL+czMKjmQ7GP6+oM9vf0jXv/h6mR9ru+u2ujOdzNrOQeSFvmnd1df2eU7qzay+dndVa8B9EVw2/ony+fPm3ZAw8tmZjYW7p1tkVccU30hyc+uWDfifX19g2sgP9uQdNR3TZ3wq+yb2QTlGskEs313tbkosGXHHg8DNrOWcCCZYLbs3DMkbf7MKfT0Bc9VmTlvZtZsDiQTzD2btg1JO3z6JACeeW5okDEzazYHkhaqvrfj2JUCSWldLjOz8eRA0kLZOPKCQw8c070vPXpG+Xhm2tH+zA7XSMxs/DmQtFB2u/mPn3XskOvXv+/UQfuZZE1KF4IE6GhLvsbdI8w/MTNrFgeSFsrWSA5oKw65/uKjZvCR1/zOoLTFr03Oe/oGgsak9uTekSYympk1i+eRtFC2j6R0PH/mFO5/Yns5/aAD27n+vacyc1onm7bs4unnksmK23cNDAN+5Qu7WHIj7OnzqC0zG38OJC007YD28vLwpSkgBx/Ywazpk7j0Dwe2s39x2h9y9Myp3HLvbwEG7bbY6RqJmbWQm7Za6FvvP7183NmefBWHTz+A2xe/irecPKfqPVMPaAdgV2b/99LCjQ4kZtYKDiQtdPTMKeXjF82Zzl//8fFc8abjR7znmOcl91zw4rnltFJn+54+z2w3s/HX1EAiaZGkdZLWS1pc5fpFku6WtFrSbZIWZK59NL1vnaTX1PrMiUoS555yxKibU00/sIOH/vr1/MnC2eU010jMrJWaFkgkFYGrgNcCC4DzsoEidX1EHB8RJwCfAa5M710AnAscCywCviipWOMz9wtthYGvrlwjcSAxsxZoZmf7KcD6iNgAIGkpcDawtpQhIrLrfUwGSm0zZwNLI2I38KCk9enzGO2ZE80tH34F9/92++gZK7QXB4Z8FQuiWJBHbZlZSzQzkMwCHsmcbwROrcwk6WLgQ0AH8KrMvdkt/zamadTyzPS5FwIXAhxxRPVJffuCo7qmcFTXlNEzVlCqeVChAAAJr0lEQVTF+iodxYJrJGbWEi3vbI+IqyLiKOAjwGUNfO7VEbEwIhZ2dVXf+yMPjp91EJA0b3lmu5m1QjNrJJuA7BjW2WnacJYCX6rh3rE8M9du+fAryutszZjSMeLOimZmzdLMGslKYL6keZI6SDrPl2UzSJqfOX09cH96vAw4V1KnpHnAfODntTxzf3JU15TyvJLZBx/Ixmd2trhEZrY/alqNJCJ6JV0CrACKwLURsUbSEqA7IpYBl0g6A+gBngEuSO9dI+nbJJ3ovcDFEdEHUO2ZzXqHiWTWwZO4e9PWVhfDzPZDTV0iJSKWA8sr0i7PHF86wr2fBD5ZyzMNZh88iaef28Nzu3uZPMpcFDOzRmp5Z7s1xuyDk/1MfrlxS4tLYmb7G//TNSdePn8GM6Z08rZr7uT0Iw9l9sGTOLJrCs8/qJPJHW1M6Wyjs73IAe0F2goFigUoKJl/UvpZeVwsiKJEoQDFNL1y2LGZmQNJTkw/sIPvf+DFXP1fG/i3NY/T/fDT9DRh7S2JNLikQUal9ORA5f8Z2G+lfE1V0sgupz+Qr/KaGPqMgTJVD261xrxa8mnIn1rPs2pTS9CuOay3OP63+p8f+/M/gG764EvprLLfUSM5kOTInEMO5BNvPI5PvPE4+vqDLTv28Nttu9nV28eunj729Pazq6efvv6gL4L+/hh8nEnr7Q/6I+jrJ/2ZfMrHad6IgeUIkuMoH2dFxKB8kOQdOKbivsy1KvkHcmX/jGx6jUG0hmy1huOoLFxdz6ohT83Pau1ini1fSrTlBWitWv8RVA8HkpwqFsShUzo5dEpnq4tiZjnnznYzM6uLA4mZmdXFgcTMzOriQGJmZnVxIDEzs7o4kJiZWV0cSMzMrC4OJGZmVhe1etbreJC0GXh4L2+fATzZwOJMBH7n/cP+9s772/tC/e/8gogYdYvZ/SKQ1ENSd0QsbHU5xpPfef+wv73z/va+MH7v7KYtMzOriwOJmZnVxYFkdFe3ugAt4HfeP+xv77y/vS+M0zu7j8TMzOriGomZmdXFgcTMzOriQDIMSYskrZO0XtLiVpenUSTNkfSfktZKWiPp0jT9EEn/Ien+9OfBabokfSH9PfxK0omtfYO9J6ko6ReSbkzP50m6M323b0nqSNM70/P16fW5rSz33pI0XdJ3Jd0n6V5Jp+f9e5b05+l/1/dI+qakA/L2PUu6VtITku7JpI35e5V0QZr/fkkX1FMmB5IqJBWBq4DXAguA8yQtaG2pGqYX+HBELABOAy5O320xcEtEzAduSc8h+R3MTz8XAl8a/yI3zKXAvZnzTwOfi4ijgWeA96Tp7wGeSdM/l+abiD4P/FtE/A7w+yTvntvvWdIs4IPAwog4DigC55K/7/lrwKKKtDF9r5IOAf4SOBU4BfjLUvDZKxHhT8UHOB1YkTn/KPDRVperSe/6Q+BMYB1wWJp2GLAuPf5H4LxM/nK+ifQBZqf/B3sVcCMgkhm/bZXfObACOD09bkvzqdXvMMb3PQh4sLLcef6egVnAI8Ah6fd2I/CaPH7PwFzgnr39XoHzgH/MpA/KN9aPayTVlf6DLNmYpuVKWpV/EXAn8LyIeCy99DjwvPQ4L7+LvwP+N9Cfnh8KbImI3vQ8+17ld06vb03zTyTzgM3AdWlz3lckTSbH33NEbAL+BvgN8BjJ97aKfH/PJWP9Xhv6fTuQ7KckTQFuAP4sIrZlr0XyT5TcjAuX9AbgiYhY1eqyjKM24ETgSxHxIuA5Bpo7gFx+zwcDZ5ME0cOByQxtAsq9VnyvDiTVbQLmZM5np2m5IKmdJIh8IyK+lyb/VtJh6fXDgCfS9Dz8Ll4CnCXpIWApSfPW54HpktrSPNn3Kr9zev0g4KnxLHADbAQ2RsSd6fl3SQJLnr/nM4AHI2JzRPQA3yP57vP8PZeM9Xtt6PftQFLdSmB+Otqjg6TDblmLy9QQkgR8Fbg3Iq7MXFoGlEZuXEDSd1JKf0c6+uM0YGumCj0hRMRHI2J2RMwl+S5/HBHnA/8JvDnNVvnOpd/Fm9P8E+pf7hHxOPCIpBemSX8IrCXH3zNJk9Zpkg5M/zsvvXNuv+eMsX6vK4BXSzo4rcm9Ok3bO63uNNpXP8DrgF8DDwD/t9XlaeB7vZSk2vsrYHX6eR1J2/AtwP3AzcAhaX6RjGB7ALibZERMy9+jjvd/JXBjenwk8HNgPfAdoDNNPyA9X59eP7LV5d7Ldz0B6E6/6x8AB+f9ewb+CrgPuAf4Z6Azb98z8E2SPqAekprne/bmewXenb77euBd9ZTJS6SYmVld3LRlZmZ1cSAxM7O6OJCYmVldHEjMzKwuDiRmZlYXBxIzQNL/TVeN/ZWk1ZJObfKfd6ukhWPI/05Jh2fOv5KjhURtgmsbPYtZvkk6HXgDcGJE7JY0A+hocbEqvZNkbsSjABHx3paWxizDNRKzZDXUJyNiN0BEPBkRjwJIulzSynR/i6vTGdOlGsXnJHWne32cLOl76d4OV6R55irZC+QbaZ7vSjqw8g+X9GpJP5N0l6TvpOugZa+/GVgIfCOtLU3K1mgkbZf02bRGdbOkU9LrGySdleYppnlWprWu9zfx92n7GQcSM/h3YI6kX0v6oqRXZK79Q0ScHMn+FpNIai4leyJiIfBlkiUpLgaOA94pqbSK7AuBL0bE7wLbgA9k/+C09nMZcEZEnEgyE/1D2TwR8d00/fyIOCEidlaUfzLJ8h7HAs8CV5BsDfAmYEma5z0ky2OcDJwMvE/SvDH8jsyG5UBi+72I2A6cRLLxz2bgW5LemV7+AyW7591NstjjsZlbS+uv3Q2siYjH0lrNBgYWxHskIm5Pj/+FZImarNNINk+7XdJqknWSXjDGV9gD/FumLD+JZNHCu0n2rYBkLaV3pH/GnSRLaswf459jVpX7SMyAiOgDbgVuTYPGBZKWAl8kWZ/oEUkfJ1mfqWR3+rM/c1w6L/1/q3INospzAf8REefVUfyeGFjrqFyWiOjPrHor4H9GxN4vzGc2DNdIbL8n6YWSsv86PwF4mIGg8WTab/HmITeP7oi0Mx/gbcBtFdfvAF4i6ei0LJMlHVPlOc8CU/fizy9ZAfyPdAsBJB2TbnRlVjfXSMxgCvD3kqaT7Gm/HrgwIrZIuoZktNTjJNsLjNU64GJJ15IsaT5oL/SI2Jw2o31TUmeafBnJytNZXwO+LGknyXaxY/UVkmauu9IBA5uBN+7Fc8yG8Oq/Zk2iZCvjG9OOerPcctOWmZnVxTUSMzOri2skZmZWFwcSMzOriwOJmZnVxYHEzMzq4kBiZmZ1+f/gWglEMvzA7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Trend')\n",
    "\n",
    "plt.plot(losses)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Loss')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('Train Accuracy')\n",
    "\n",
    "plt.plot(accuracies)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, config, input_length):\n",
    "    # Initialize the dataset and data loader (leave the +1)\n",
    "    dataset = PalindromeDataset(input_length+1)\n",
    "    data_loader = DataLoader(dataset, config.batch_size, num_workers=1)\n",
    "    accuracies = []\n",
    "    \n",
    "    for step, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
    "        outputs = model(batch_inputs)\n",
    "        accu = 0.0  \n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            accu = accuracy(outputs, batch_targets)\n",
    "            accuracies.append(accu)\n",
    "            \n",
    "        if step == 2000:\n",
    "            # If you receive a PyTorch data-loader error, check this bug report:\n",
    "            # https://github.com/pytorch/pytorch/pull/9655\n",
    "            break\n",
    "\n",
    "    print('Done testing.')\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done testing.\n"
     ]
    }
   ],
   "source": [
    "test_accuracies = test(model, config, config.input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palindromes Length: T = 11\n",
      "Average accuracy over 2000 sampled test: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.plot(test_accuracies)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(\"Palindromes Length: T = 11\")\n",
    "print(\"Average accuracy over 2000 sampled test: \" + str(np.mean(test_accuracies)) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2\n",
    "\n",
    "In this case, I train and test the LSTM model with the Palindromes' length of 5, 15, 20, 25(T = 5, 15, 20, 25). \n",
    "\n",
    "The LSTM performance is very good even when Palindrome's length is 25. So I do observe a better performance when compared to the RNN I’ve implemented in the previous assignment, which is show in `./rnn_train.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T = 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__:35: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:     0] loss: 0.4606\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:    10] loss: 0.4606\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:    20] loss: 0.4596\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:    30] loss: 0.4562\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:    40] loss: 0.4554\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:    50] loss: 0.4459\n",
      "Accuracy on training dataset: 26.562 %\n",
      "[step:    60] loss: 0.4403\n",
      "Accuracy on training dataset: 23.438 %\n",
      "[step:    70] loss: 0.4405\n",
      "Accuracy on training dataset: 29.688 %\n",
      "[step:    80] loss: 0.4282\n",
      "Accuracy on training dataset: 47.656 %\n",
      "[step:    90] loss: 0.4179\n",
      "Accuracy on training dataset: 54.688 %\n",
      "[step:   100] loss: 0.4152\n",
      "Accuracy on training dataset: 52.344 %\n",
      "[step:   110] loss: 0.3982\n",
      "Accuracy on training dataset: 58.594 %\n",
      "[step:   120] loss: 0.3980\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:   130] loss: 0.3796\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:   140] loss: 0.3673\n",
      "Accuracy on training dataset: 73.438 %\n",
      "[step:   150] loss: 0.3572\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:   160] loss: 0.3608\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:   170] loss: 0.3521\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:   180] loss: 0.3407\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:   190] loss: 0.3429\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:   200] loss: 0.3186\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   210] loss: 0.3188\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   220] loss: 0.3114\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   230] loss: 0.3066\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   240] loss: 0.3032\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   250] loss: 0.3013\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   260] loss: 0.3002\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   270] loss: 0.2984\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   280] loss: 0.2977\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   290] loss: 0.2966\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   300] loss: 0.2962\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   310] loss: 0.2959\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   320] loss: 0.2953\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   330] loss: 0.2951\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   340] loss: 0.2950\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   350] loss: 0.2947\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   360] loss: 0.2946\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   370] loss: 0.2944\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   380] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   390] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   400] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   410] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   420] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   430] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   440] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   450] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   460] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   470] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   480] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   490] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   500] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   510] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   520] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   530] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   540] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   550] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   560] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   570] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   580] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   590] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   600] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   610] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   620] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   630] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   640] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   650] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   660] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   670] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   680] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   690] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   700] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   710] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   720] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   730] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   740] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   750] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   760] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   770] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   780] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   790] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   800] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   810] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   820] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   830] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   840] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   850] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   860] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   870] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   880] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   890] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   900] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   910] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   920] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   930] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   940] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   950] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   960] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   970] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   980] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   990] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1000] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1010] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1020] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1030] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1040] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1050] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1060] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1070] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1080] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1090] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1100] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1110] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1120] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1130] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1140] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1150] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1160] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1170] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1180] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1190] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1200] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1210] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1220] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1230] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  1240] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1250] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1260] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1270] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1280] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1290] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1300] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1310] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1320] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1330] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1340] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1350] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1360] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1370] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1380] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1390] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1400] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1410] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1420] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1430] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1440] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1450] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1460] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1470] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1480] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1490] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1500] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1510] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1520] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1530] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1540] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1550] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1560] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1570] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1580] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1590] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1600] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1610] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1620] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1630] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1640] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1650] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1660] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1670] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1680] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1690] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1700] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1710] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1720] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1730] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1740] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1750] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1760] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1770] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1780] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1790] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1800] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1810] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1820] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1830] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1840] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1850] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1860] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1870] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1880] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1890] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1900] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1910] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1920] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1930] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1940] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1950] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1960] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1970] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1980] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1990] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2000] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2010] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2020] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2030] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2040] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2050] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2060] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2070] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2080] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2090] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2100] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2110] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2120] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2130] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2140] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2150] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2160] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2170] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2180] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2190] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2200] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2210] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2220] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2230] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2240] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2250] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2260] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2270] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2280] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2290] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2300] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2310] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2320] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2330] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2340] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2350] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2360] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2370] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2380] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2390] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2400] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2410] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2420] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2430] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2440] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2450] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2460] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  2470] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2480] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2490] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2500] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2510] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2520] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2530] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2540] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2550] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2560] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2570] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2580] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2590] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2600] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2610] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2620] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2630] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2640] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2650] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2660] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2670] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2680] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2690] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2700] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2710] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2720] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2730] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2740] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2750] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2760] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2770] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2780] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2790] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2800] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2810] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2820] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2830] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2840] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2850] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2860] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2870] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2880] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2890] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2900] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2910] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2920] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2930] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2940] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2950] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2960] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2970] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2980] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2990] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3000] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3010] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3020] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3030] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3040] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3050] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3060] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3070] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3080] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3090] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3100] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3120] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3130] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3140] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3150] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3160] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3170] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3180] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3190] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3210] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  3700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  4930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  6170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  7410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  8650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  9890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step: 10000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "model2, losses2, accuracies2 = train(config, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Trend')\n",
    "\n",
    "plt.plot(losses2)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Loss')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('Train Accuracy')\n",
    "\n",
    "plt.plot(accuracies2)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done testing.\n"
     ]
    }
   ],
   "source": [
    "test_accuracies2 = test(model2, config, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palindromes Length: T = 5\n",
      "Average accuracy over 2000 sampled test: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.plot(test_accuracies2)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(\"Palindromes Length: T = 5\")\n",
    "print(\"Average accuracy over 2000 sampled test: \" + str(np.mean(test_accuracies2)) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T = 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__:35: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:     0] loss: 0.4612\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:    10] loss: 0.4607\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:    20] loss: 0.4608\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:    30] loss: 0.4603\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:    40] loss: 0.4603\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:    50] loss: 0.4601\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:    60] loss: 0.4614\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:    70] loss: 0.4607\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:    80] loss: 0.4602\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:    90] loss: 0.4610\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   100] loss: 0.4608\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:   110] loss: 0.4603\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   120] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   130] loss: 0.4607\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   140] loss: 0.4607\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:   150] loss: 0.4603\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   160] loss: 0.4604\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   170] loss: 0.4608\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   180] loss: 0.4620\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   190] loss: 0.4612\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   200] loss: 0.4587\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   210] loss: 0.4581\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   220] loss: 0.4474\n",
      "Accuracy on training dataset: 21.875 %\n",
      "[step:   230] loss: 0.4463\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:   240] loss: 0.4329\n",
      "Accuracy on training dataset: 33.594 %\n",
      "[step:   250] loss: 0.4293\n",
      "Accuracy on training dataset: 28.125 %\n",
      "[step:   260] loss: 0.4249\n",
      "Accuracy on training dataset: 31.250 %\n",
      "[step:   270] loss: 0.4140\n",
      "Accuracy on training dataset: 47.656 %\n",
      "[step:   280] loss: 0.4271\n",
      "Accuracy on training dataset: 31.250 %\n",
      "[step:   290] loss: 0.4076\n",
      "Accuracy on training dataset: 43.750 %\n",
      "[step:   300] loss: 0.3829\n",
      "Accuracy on training dataset: 57.031 %\n",
      "[step:   310] loss: 0.3936\n",
      "Accuracy on training dataset: 49.219 %\n",
      "[step:   320] loss: 0.3882\n",
      "Accuracy on training dataset: 54.688 %\n",
      "[step:   330] loss: 0.3738\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:   340] loss: 0.3621\n",
      "Accuracy on training dataset: 71.875 %\n",
      "[step:   350] loss: 0.3458\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:   360] loss: 0.3434\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:   370] loss: 0.3342\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:   380] loss: 0.3534\n",
      "Accuracy on training dataset: 73.438 %\n",
      "[step:   390] loss: 0.3461\n",
      "Accuracy on training dataset: 72.656 %\n",
      "[step:   400] loss: 0.3336\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:   410] loss: 0.3283\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:   420] loss: 0.3199\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:   430] loss: 0.3125\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:   440] loss: 0.3070\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:   450] loss: 0.3131\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:   460] loss: 0.3030\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:   470] loss: 0.3014\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:   480] loss: 0.3006\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:   490] loss: 0.2975\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:   500] loss: 0.2956\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   510] loss: 0.2961\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   520] loss: 0.2954\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   530] loss: 0.2947\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   540] loss: 0.2941\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   550] loss: 0.2941\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   560] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   570] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   580] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   590] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   600] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   610] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   620] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   630] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   640] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   650] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   660] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   670] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   680] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   690] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   700] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   710] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   720] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   730] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   740] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   750] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   760] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   770] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   780] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   790] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   800] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   810] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   820] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   830] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   840] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   850] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   860] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   870] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   880] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   890] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   900] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   910] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   920] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   930] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   940] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   950] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   960] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   970] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   980] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:   990] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1000] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1010] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1020] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1030] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1040] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1050] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1060] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1070] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1080] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1090] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1100] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1110] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1120] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1130] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1140] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1150] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1160] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1170] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1180] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1190] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1200] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1210] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1220] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1230] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  1240] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1250] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1260] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1270] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1280] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1290] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1300] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1310] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1320] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1330] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1340] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1350] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1360] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1370] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1380] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1390] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1400] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1410] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1420] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1430] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1440] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1450] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1460] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1470] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1480] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1490] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1500] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1510] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1520] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1530] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1540] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1550] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1560] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1570] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1580] loss: 0.2924\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1590] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1600] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1610] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1620] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1630] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1640] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1650] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1660] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1670] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1680] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1690] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1700] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1710] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1720] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1730] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1740] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1750] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1760] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1770] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1780] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1790] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1800] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1810] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1820] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1830] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1840] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1850] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1860] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1870] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1880] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1890] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1900] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1910] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1920] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1930] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1940] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1950] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1960] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1970] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1980] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  1990] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2000] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2010] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2020] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2030] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2040] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2050] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2060] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2070] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2080] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2090] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2100] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2110] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2120] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2130] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2140] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2150] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2160] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2170] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2180] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2190] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2200] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2210] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2220] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2230] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2240] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2250] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2260] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2270] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2280] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2290] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2300] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2310] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2320] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2330] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2340] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2350] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2360] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2370] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2380] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2390] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2400] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2410] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2420] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2430] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2440] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2450] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2460] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  2470] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2480] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2490] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2500] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2510] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2520] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2530] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2540] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2550] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2560] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2570] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2580] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2590] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2600] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2610] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2620] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2630] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2640] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2650] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2660] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2670] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2680] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2690] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2700] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2710] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2720] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2730] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2740] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2750] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2760] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2770] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2780] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2790] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2800] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2810] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2820] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2830] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2840] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2850] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2860] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2870] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2880] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2890] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2900] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2910] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2920] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2930] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2940] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2950] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2960] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2970] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2980] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  2990] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3000] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3010] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3020] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3030] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3040] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3050] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3060] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3070] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3080] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3090] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3100] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3110] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3120] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3130] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3140] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3150] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3160] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3170] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3180] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3190] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3200] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3210] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3220] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3230] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3240] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3250] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3260] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3270] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3290] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3310] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3320] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3360] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3380] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3420] loss: 0.2923\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  3700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  4930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  6160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  7390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  8620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9010] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9020] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9030] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9040] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9050] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9060] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9070] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9080] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9090] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9100] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9110] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9120] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9130] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9140] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9150] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9160] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9170] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9180] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9190] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9200] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9210] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9220] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9230] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9240] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9250] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9260] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9270] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9280] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9290] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9300] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9310] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9320] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9330] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9340] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9350] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9360] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9370] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9380] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9390] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9400] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9410] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9420] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9430] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9440] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9450] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9460] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9470] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9480] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9490] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9500] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9510] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9520] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9530] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9540] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9550] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9560] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9570] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9580] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9590] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9600] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9610] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9620] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9630] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9640] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9650] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9660] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9670] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9680] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9690] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9700] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9710] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9720] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9730] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9740] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9750] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9760] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9770] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9780] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9790] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9800] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9810] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9820] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9830] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9840] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  9850] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9860] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9870] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9880] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9890] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9900] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9910] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9920] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9930] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9940] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9950] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9960] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9970] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9980] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9990] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step: 10000] loss: 0.2922\n",
      "Accuracy on training dataset: 100.000 %\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "model3, losses3, accuracies3 = train(config, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Trend')\n",
    "\n",
    "plt.plot(losses3)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Loss')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('Train Accuracy')\n",
    "\n",
    "plt.plot(accuracies3)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done testing.\n"
     ]
    }
   ],
   "source": [
    "test_accuracies3 = test(model3, config, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAGWxJREFUeJzt3XuYJXV95/H3RxBBELnMQBAYBxVUNAvioJgARkFFVkWNa0CUIYLjBY260ciueFsMEVaj6yoQROJgEFDQgD6KKCqsUdABUQZRBhBkkGEGBQFBrt/9o6rh2KnuPjNwzmm636/n6edU/U6drm9Xnz6f/tWvLqkqJEka7xGjLkCSND0ZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4GhCSpkwGhh5Ukt/V83Zfkjp75/R/E9z0/yWv7WG6jdp1fWdN1SQ8Xa4+6AGl1VNUGY9NJrgYOrqpvD7GEvwFuB/ZOsmlV/XZYK06ydlXdM6z1SfYgNKMkWSvJ+5JcleTGJCcl2ah9bv0kpyT5XZKbk1yQZOMkHwN2Bo5veyIfm2QVC4FPAFcC+41b9/wkZ7TrvbH3+yR5S5JfJLk1ySVJ/jzJukkqyVY9y52S5LB2eq8kV7Q/zw3AMUnmJvlGklXtz3FGki16Xj8nyYlJViS5KcmpbfsVSV7Qs9y6SX6f5KkPYnNrhjMgNNO8C3ghsCuwFXA38PH2uYNpes1bAnOAtwJ3VdXfAz+m6Y1s0M7/J0m2A3YBvgCcRBMWY889EvgGcBkwD9gaOL197nXAe2gCZUPgVcBNff4884FHtt/v72j+Zo9t17FNu8zHe5Y/FQjwFGBz4NNt+4lA7y60fYDLq+qyPuvQLOQuJs00bwJeW1W/AUjyIeDSJK+nCYu5wBOrailNKKyOA4AfVdWVSb4AfDjJU9sP2V1pPvz/Z1Xd1y7/g/bxYOCIqvpJO//LtrZ1+1jnncDhVXV3O38HcMbYdJJ/Ar7Sfr9tgN2ATavq1naZ89rHE4GfJlmvqu4AXgd8fnV+eM0+9iA0YyQJzX/aX293Id0M/ITmfb4p8FngXOC0JMuTHJFkrdX43q+j6TlQVb8CfsgDvYitgV/1hEOvrWl2Sa2JFT3hQJLHJDkhya+T3AKcTdMbGlvPyp5wuF9VXU2zLV6eZC7wfOCUNaxJs4QBoRmjmksTXwc8v6o26vlat6purKo7q+r9VfUUYHfgvwH7jr18im//PJrdOh9s9++vAHYAXpvkEcC1wPx2erxrgSd2tN9F06t5dE/bn43/scbNH0qz62znqtqQZndaetazWZIN6LaYZjfTvsB3qmrlBMtJgAGhmedY4CNJtgZIslmSl7bTeybZvv0QvwW4Bxj7j/8G4AmTfN+FwNeApwE7tl87AJsAewDfB24FDk/y6CTrJfmL9rXHA4cm2SGN7ZJs1fY2LgH2bwfXXwY8Z4qf7zE0R1HdnGQOcNjYE22v5jzgU0kem2SdJLv3vPY0ml1hb6bZ5SRNyoDQTHMU8G3gO0lupRkH2Kl9bkua/fe3AkuBr9MM6kIz0HtAe+TPUb3fsP2P/K+BT1bVip6vK2h20yxsdwPtTRMay4FfA68AqKrPA/9M8wF9a/u4Ufvt30pz6OxNwMtpQmgyH6XZpfRbmlD6+rjn96MZ1F4GrKAJA9o6bgW+CjwOOHOK9UjEGwZJs0eSI4DNqurgUdei6c+jmKRZoh2cPpCmpyJNyV1M0iyQ5K3A1cCXqupHIy5HDxPuYpIkdbIHIUnq9LAeg5gzZ07Nnz9/1GVI0sPKhRdeeGNVzZ1quYd1QMyfP58lS5aMugxJelhJck0/y7mLSZLUyYCQJHUyICRJnQwISVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwISVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUaWABkeSEJCuTLO1p2yTJt5Isax83HveanZPck+RVg6pLktSfQfYgPgfsNa7tUOCcqtoWOKedByDJWsCRwNkDrEmS1KeBBURVnQf8blzzPsDidnox8PKe594GnA6sHFRNkqT+DXsMYvOqur6dXgFsDpBkS+AVwDFTfYMki5IsSbJk1apVg6tUkma5kQ1SV1UB1c5+AnhPVd3Xx+uOq6oFVbVg7ty5A61RkmaztYe8vhuSbFFV1yfZggd2Jy0ATkkCMAfYO8k9VfXvQ65PktQadg/iTGBhO70QOAOgqrapqvlVNR84DXiL4SBJozXIw1xPBn4IPDnJ8iQHAR8BXpBkGbBnOy9JmoYGtoupqvab4Kk9pnjdgQ99NZKk1eWZ1JKkTgaEJKmTASFJ6mRASJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4GhCSp08ACIskJSVYmWdrTtkmSbyVZ1j5u3Lbvn+RnSS5J8oMkOwyqLklSfwbZg/gcsNe4tkOBc6pqW+Ccdh7gV8Bzq+rPgcOB4wZYlySpDwMLiKo6D/jduOZ9gMXt9GLg5e2yP6iqm9r284GtBlWXJKk/wx6D2Lyqrm+nVwCbdyxzEPCN4ZUkSeqy9qhWXFWVpHrbkjyPJiB2neh1SRYBiwDmzZs30BolaTYbdg/ihiRbALSPK8eeSPJfgOOBfarqtxN9g6o6rqoWVNWCuXPnDrxgSZqthh0QZwIL2+mFwBkASeYBXwZeV1WXD7kmSVKHge1iSnIy8FfAnCTLgQ8AHwG+mOQg4Brg1e3i7wc2BY5OAnBPVS0YVG2SpKkNLCCqar8JntqjY9mDgYMHVYskafV5JrUkqZMBIUnqZEBIkjpNOQaR5BHADsDjgDuApVW1cvJXSZIe7iYMiCRPBN4D7AksA1YB6wLbJbkd+BdgcVXdN4xCJUnDNVkP4sPAMcAbq2r8Gc+bAa8BXscD11aSJM0gEwbEJIep0u5i+sRAKpIkTQt9D1IneVKSf0tyepLnDLIoSdLoTTYGsW5V/bGn6XDgH9rprwI7DrIwSdJoTdaD+GqSA3rm7wbmA48H7h1kUZKk0ZssIPYCNkxyVpLdgXcBLwJeAew/jOIkSaMz2SD1vcCnknweeB/wZuCwqrpyWMVJkkZnsjGIZwPvBu4CjqA5Se4fk1wHHF5VNw+nREnSKEx2HsS/AHsDGwD/WlV/Ceyb5LnAqTS7myRJM9RkAXEPzaD0+jS9CACq6lzg3MGWJUkatckC4jXAG2nC4YBJlpMkzUCTBcSyqvr7yV6cJOMvwyFJmhkmO8z1u0ne1t4v+n5J1kny/CSLeeD+0pKkGWayHsRewOuBk5NsA9xMczXXtYCzgU9U1U8GX6IkaRQmOw/ij8DRwNFJHgnMAe7w8FZJmh2mvGEQQFXdDVw/4FokSdOItxyVJHUyICRJnaYMiPZIpo2HUYwkafropwexOfDjJF9MsleSDLooSdLoTRkQVXUYsC3wWeBAYFmSI5I8ccC1SZJGqK8xiPZs6RXt1z3AxsBpSY4aYG2SpBGa8jDXJG+nuRbTjcDxwLur6u4kjwCW8cBtSCVJM0g/50FsAryyqq7pbayq+5K8ZDBlSZJGrZ9dTN8Afjc2k2TD9mZCVNVlgypMkjRa/QTEMcBtPfO3tW2SpBmsn4D4k0t6V9V99Dd2cUKSlUmW9rRtkuRbSZa1jxu37UnyySRXJPlZkp3W5IeRJD10+gmIq5L8XZJHtl9vB67q43Wfo7kibK9DgXOqalvgnHYe4MU0h9JuCyzCHookjVw/g9RvAj4JHAYUzQf7oqleVFXnJZk/rnkf4K/a6cXA94D3tO0ntj2V85NslGSLqhrIBQI/9NVL+flvbhnEt5akodj+cRvygZc+baDrmDIgqmolsO9DtL7Nez70V9CcpQ2wJXBtz3LL27b/FBBJFtEG1Lx588Y/LUl6iPQzlrAucBDwNJobBgFQVa9/MCuuqkqy2rcrrarjgOMAFixYsEa3Ox106krSTNDPGMTngT8DXgScC2wF3LqG67shyRYA7ePKtv06YOue5bZq2yRJI9JPQDypqt4H/KGqFgP/FXj2Gq7vTB64j/VC4Iye9gPao5l2AX4/qPEHSVJ/+hmkvrt9vDnJ02nGDjab6kVJTqYZkJ6TZDnwAeAjwBeTHARcA7y6XfzrwN7AFcDtwN+uxs8gSRqAfgLiuPZ8hcNo/tPfAHjfVC+qqv0meGqPjmULOKSPWiRJQzJpQLQX5Lulqm4CzgOeMJSqJEkjN+kYRHvWtFdrlaRZqJ9B6m8neVeSrdtLZWySZJOBVyZJGql+xiD+pn3sHSMo3N0kSTNaP2dSbzOMQiRJ00s/Z1If0NVeVSc+9OVIkqaLfnYx7dwzvS7NYaoXAQaEJM1g/exielvvfJKNgFMGVpEkaVro5yim8f4AOC4hSTNcP2MQX6U5agmaQNke+OIgi5IkjV4/YxAf7Zm+B7imqpYPqB5J0jTRT0D8Gri+qv4IkGS9JPOr6uqBViZJGql+xiC+BNzXM39v2yZJmsH6CYi1q+qusZl2ep3BlSRJmg76CYhVSV42NpNkH+DGwZUkSZoO+hmDeBNwUpJPtfPLgc6zqyVJM0c/J8pdCeySZIN2/raBVyVJGrkpdzElOSLJRlV1W1XdlmTjJB8eRnGSpNHpZwzixVV189hMe3e5vQdXkiRpOugnINZK8qixmSTrAY+aZHlJ0gzQzyD1ScA5Sf61nf9bvJKrJM14/QxSH5nkp8CebdPhVfXNwZYlSRq1fnoQVNVZwFkASXZN8umqOmSKl0mSHsb6CogkzwD2A14N/Ar48iCLkiSN3oQBkWQ7mlDYj+bM6VOBVNXzhlSbJGmEJutB/AL4f8BLquoKgCTvHEpVkqSRm+ww11cC1wPfTfKZJHsAGU5ZkqRRmzAgqurfq2pf4CnAd4F3AJslOSbJC4dVoCRpNKY8Ua6q/lBVX6iqlwJbAT8B3jPwyiRJI9XPmdT3q6qbquq4qtpjUAVJkqaH1QqIh0qStydZmuTSJO9o23ZMcn6Si5MsSfKsUdQmSWoMPSCSPB14A/AsYAfgJUmeBBwFfKiqdgTe385LkkakrxPlHmJPBS6oqtsBkpxLc8RUARu2yzwW+M0IapMktUYREEuBf0yyKXAHzaXDl9AcJfXNJB+l6dn8xQhqkyS1hr6LqaouA44Ezqa5vtPFwL3Am4F3VtXWwDuBz3a9PsmidoxiyapVq4ZUtSTNPqmq0RaQHEFzn+t/AjaqqkoS4PdVteFkr12wYEEtWbJkGGVK0oyR5MKqWjDVcqM6immz9nEezfjDF2jGHJ7bLvJ8YNkoapMkNUYxBgFwejsGcTdwSFXdnOQNwP9JsjbwR2DRiGqTJDGigKiq3Travg88cwTlSJI6jGQXkyRp+jMgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwISVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwISVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwISVKnkQREkrcnWZrk0iTv6Gl/W5JftO1HjaI2SVJj7WGvMMnTgTcAzwLuAs5K8jVga2AfYIequjPJZsOuTZL0gKEHBPBU4IKquh0gybnAK4EFwEeq6k6Aqlo5gtokSa1R7GJaCuyWZNMkjwb2puk9bNe2X5Dk3CQ7d704yaIkS5IsWbVq1RDLlqTZZegBUVWXAUcCZwNnARcD99L0ZjYBdgHeDXwxSTpef1xVLaiqBXPnzh1e4ZI0y4xkkLqqPltVz6yq3YGbgMuB5cCXq/Ej4D5gzijqkySNZgyCJJtV1cok82jGH3ahCYTnAd9Nsh2wDnDjKOqTJI0oIIDTk2wK3A0cUlU3JzkBOCHJUpqjmxZWVY2oPkma9UYSEFW1W0fbXcBrR1COJKmDZ1JLkjoZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqlKoadQ1rLMkq4Jo1fPkc4MaHsJyHynStC6Zvbda1eqxr9czEuh5fVXOnWuhhHRAPRpIlVbVg1HWMN13rgulbm3WtHutaPbO5LncxSZI6GRCSpE6zOSCOG3UBE5iudcH0rc26Vo91rZ5ZW9esHYOQJE1uNvcgJEmTMCAkSZ1mZUAk2SvJL5NckeTQEdaxdZLvJvl5kkuTvL1t/2CS65Jc3H7tPYLark5ySbv+JW3bJkm+lWRZ+7jxkGt6cs82uTjJLUneMYrtleSEJCuTLO1p69w+aXyyfb/9LMlOQ67rfyf5RbvuryTZqG2fn+SOnu127JDrmvD3luR/tNvrl0leNOS6Tu2p6eokF7ftw9xeE302DPc9VlWz6gtYC7gSeAKwDvBTYPsR1bIFsFM7/RjgcmB74IPAu0a8na4G5oxrOwo4tJ0+FDhyxL/HFcDjR7G9gN2BnYClU20fYG/gG0CAXYALhlzXC4G12+kje+qa37vcCLZX5++t/Rv4KfAoYJv273WtYdU17vmPAe8fwfaa6LNhqO+x2diDeBZwRVVdVVV3AacA+4yikKq6vqouaqdvBS4DthxFLX3aB1jcTi8GXj7CWvYArqyqNT2T/kGpqvOA341rnmj77AOcWI3zgY2SbDGsuqrq7Kq6p509H9hqEOte3bomsQ9wSlXdWVW/Aq6g+bsdal1JArwaOHkQ657MJJ8NQ32PzcaA2BK4tmd+OdPgQznJfOAZwAVt01vbruIJw96V0yrg7CQXJlnUtm1eVde30yuAzUdQ15h9+dM/3FFvL5h4+0yn99zraf7THLNNkp8kOTfJbiOop+v3Nl22127ADVW1rKdt6Ntr3GfDUN9jszEgpp0kGwCnA++oqluAY4AnAjsC19N0c4dt16raCXgxcEiS3XufrKZfO5JjpJOsA7wM+FLbNB22158Y5faZSJL3AvcAJ7VN1wPzquoZwH8HvpBkwyGWNO1+b+Psx5/+EzL07dXx2XC/YbzHZmNAXAds3TO/Vds2EkkeSfMGOKmqvgxQVTdU1b1VdR/wGQbUvZ5MVV3XPq4EvtLWcMNYt7V9XDnsulovBi6qqhvaGke+vVoTbZ+Rv+eSHAi8BNi//WCh3YXz23b6Qpp9/dsNq6ZJfm/TYXutDbwSOHWsbdjbq+uzgSG/x2ZjQPwY2DbJNu1/ovsCZ46ikHYf52eBy6rqn3vae/cdvgJYOv61A65r/SSPGZumGeRcSrOdFraLLQTOGGZdPf7kP7tRb68eE22fM4ED2iNNdgF+37ObYOCS7AX8A/Cyqrq9p31ukrXa6ScA2wJXDbGuiX5vZwL7JnlUkm3aun40rLpaewK/qKrlYw3D3F4TfTYw7PfYMEbkp9sXzYj/5TT/Abx3hHXsStNF/Blwcfu1N/B54JK2/UxgiyHX9QSao0h+Clw6to2ATYFzgGXAt4FNRrDN1gd+Czy2p23o24smoK4H7qbZ33vQRNuH5siST7fvt0uABUOu6wqa/dNj77Fj22X/uv39XgxcBLx0yHVN+HsD3ttur18CLx5mXW3754A3jVt2mNtros+Gob7HvNSGJKnTbNzFJEnqgwEhSepkQEiSOhkQkqROBoQkqZMBoRktyXvbq2H+rL0C57MHvL7vJen7RvJJDkzyuJ7545NsP5jqpNWz9qgLkAYlyXNozh7eqaruTDKH5gq+08mBNCeI/Qagqg4eaTVSD3sQmsm2AG6sqjsBqurGqvoNQJL3J/lxkqVJjmvPXB3rAXw8yZIklyXZOcmX2+vvf7hdZn6a+yuc1C5zWpJHj195khcm+WGSi5J8qb2uTu/zrwIWACe1vZv1ensgSW5Lcy+HS5N8O8mz2uevSvKydpm12mV+3PaS3jjA7alZxoDQTHY2sHWSy5McneS5Pc99qqp2rqqnA+vR9DTG3FVVC4BjaS5lcAjwdODAJJu2yzwZOLqqngrcAryld8Vtb+UwYM9qLnq4hOYCb/erqtPa9v2rasequmNc/esD36mqpwG3Ah8GXkBzWYr/1S5zEM1lFXYGdgbe0F6eQnrQDAjNWFV1G/BMYBGwCji1vWgdwPOSXJDkEuD5wNN6Xjp2ba5LgEuruTb/nTTX3Rm7INq1VfUf7fS/0VwaodcuNDd4+Y80dyRbSHNzo9VxF3BWTy3nVtXd7fT8tv2FNNfguZjmctCb0lwjSHrQHIPQjFZV9wLfA77XhsHCJKcAR9Ncr+baJB8E1u152Z3t430902PzY38z469RM34+wLeqar8HUf7d9cC1cO6vparua682Oraet1XVNx/EeqRO9iA0Y6W5h3Xvf9M7AtfwQBjc2I4LvGoNvv28dhAc4DXA98c9fz7wl0me1NayfpKuS0PfSnNLyTX1TeDN7aWhSbJdewVe6UGzB6GZbAPg/ybZiOZGOVcAi6rq5iSfoTl6aAXNJeBX1y9pbqR0AvBzmpvf3K+qVrW7s05O8qi2+TCaqwj3+hxwbJI7gOew+o6n2d10UTvQvorR3gpWM4hXc5VWU5pbQH6tHeCWZix3MUmSOtmDkCR1sgchSepkQEiSOhkQkqROBoQkqZMBIUnq9P8B2PG/i/LsDVkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palindromes Length: T = 15\n",
      "Average accuracy over 2000 sampled test: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.plot(test_accuracies3)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(\"Palindromes Length: T = 15\")\n",
    "print(\"Average accuracy over 2000 sampled test: \" + str(np.mean(test_accuracies3)) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T = 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2(config, input_length, batch_size, learning_rate):\n",
    "\n",
    "    # Initialize the model that we are going to use\n",
    "    model = LSTM(input_length, config.input_dim, config.num_hidden, config.num_classes, batch_size)  # fixme\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize the dataset and data loader (leave the +1)\n",
    "    dataset = PalindromeDataset(input_length+1)\n",
    "    data_loader = DataLoader(dataset, batch_size, num_workers=1)\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # fixme\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # fixme\n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    loss = 0.0\n",
    "    counter = 0\n",
    "    flag1 = 0\n",
    "    flag2 = 0\n",
    "\n",
    "    for step, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
    "\n",
    "        # Add more code here ...\n",
    "        optimizer.zero_grad() \n",
    "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)   \n",
    "\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # the following line is to deal with exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=config.max_norm)\n",
    "\n",
    "        # Add more code here ...\n",
    "\n",
    "        loss += loss.item()   # fixme\n",
    "        accu = 0.0  # fixme\n",
    "        \n",
    "\n",
    "        if step % 10 == 0:\n",
    "            # print acuracy/loss here\n",
    "            print('[step: %5d] loss: %.4f' %\n",
    "                          (step, loss / 10))\n",
    "            losses.append(loss / 10)\n",
    "            loss = 0.0\n",
    "            accu = accuracy(outputs, batch_targets)\n",
    "            accuracies.append(accu)\n",
    "            print('Accuracy on training dataset: %.3f %%' % (accu))\n",
    "            \n",
    "            if accu > 90 and flag1 == 0:\n",
    "                flag1 = 1\n",
    "                for params_group in optimizer.param_groups:\n",
    "                    params_group['lr'] = learning_rate / 2\n",
    "            \n",
    "            if accu == 100:\n",
    "                counter += 1\n",
    "                if flag2 == 0:\n",
    "                    falg2 = 1\n",
    "                    for params_group in optimizer.param_groups:\n",
    "                        params_group['lr'] = learning_rate / 4\n",
    "                \n",
    "            if counter == 45:\n",
    "                break\n",
    "            else:\n",
    "                counter = 0\n",
    "\n",
    "        if step == config.train_steps:\n",
    "            # If you receive a PyTorch data-loader error, check this bug report:\n",
    "            # https://github.com/pytorch/pytorch/pull/9655\n",
    "            break\n",
    "\n",
    "    print('Done training.')\n",
    "    \n",
    "    return model, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__:37: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:     0] loss: 0.4609\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:    10] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:    20] loss: 0.4602\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:    30] loss: 0.4609\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:    40] loss: 0.4603\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:    50] loss: 0.4602\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:    60] loss: 0.4602\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:    70] loss: 0.4597\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:    80] loss: 0.4607\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:    90] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   100] loss: 0.4609\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:   110] loss: 0.4604\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   120] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   130] loss: 0.4604\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   140] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   150] loss: 0.4609\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   160] loss: 0.4603\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   170] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   180] loss: 0.4604\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   190] loss: 0.4605\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   200] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   210] loss: 0.4604\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:   220] loss: 0.4605\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   230] loss: 0.4610\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   240] loss: 0.4604\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   250] loss: 0.4604\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   260] loss: 0.4602\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:   270] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   280] loss: 0.4607\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   290] loss: 0.4607\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   300] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   310] loss: 0.4605\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   320] loss: 0.4606\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:   330] loss: 0.4600\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:   340] loss: 0.4606\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   350] loss: 0.4605\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   360] loss: 0.4605\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   370] loss: 0.4608\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   380] loss: 0.4606\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   390] loss: 0.4608\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   400] loss: 0.4603\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   410] loss: 0.4601\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:   420] loss: 0.4606\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   430] loss: 0.4604\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   440] loss: 0.4608\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:   450] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   460] loss: 0.4604\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   470] loss: 0.4606\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   480] loss: 0.4607\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   490] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   500] loss: 0.4605\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   510] loss: 0.4606\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   520] loss: 0.4604\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   530] loss: 0.4603\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   540] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   550] loss: 0.4605\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   560] loss: 0.4607\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   570] loss: 0.4604\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   580] loss: 0.4605\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   590] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   600] loss: 0.4609\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:   610] loss: 0.4604\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   620] loss: 0.4606\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   630] loss: 0.4602\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   640] loss: 0.4606\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   650] loss: 0.4608\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   660] loss: 0.4605\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   670] loss: 0.4606\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   680] loss: 0.4606\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   690] loss: 0.4605\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   700] loss: 0.4606\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   710] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   720] loss: 0.4607\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:   730] loss: 0.4602\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:   740] loss: 0.4602\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   750] loss: 0.4611\n",
      "Accuracy on training dataset: 4.688 %\n",
      "[step:   760] loss: 0.4604\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   770] loss: 0.4607\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   780] loss: 0.4601\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   790] loss: 0.4607\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   800] loss: 0.4601\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   810] loss: 0.4595\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:   820] loss: 0.4582\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   830] loss: 0.4540\n",
      "Accuracy on training dataset: 19.531 %\n",
      "[step:   840] loss: 0.4494\n",
      "Accuracy on training dataset: 17.188 %\n",
      "[step:   850] loss: 0.4444\n",
      "Accuracy on training dataset: 21.875 %\n",
      "[step:   860] loss: 0.4437\n",
      "Accuracy on training dataset: 17.969 %\n",
      "[step:   870] loss: 0.4397\n",
      "Accuracy on training dataset: 28.125 %\n",
      "[step:   880] loss: 0.4488\n",
      "Accuracy on training dataset: 18.750 %\n",
      "[step:   890] loss: 0.4424\n",
      "Accuracy on training dataset: 25.000 %\n",
      "[step:   900] loss: 0.4313\n",
      "Accuracy on training dataset: 29.688 %\n",
      "[step:   910] loss: 0.4397\n",
      "Accuracy on training dataset: 28.906 %\n",
      "[step:   920] loss: 0.4284\n",
      "Accuracy on training dataset: 31.250 %\n",
      "[step:   930] loss: 0.4220\n",
      "Accuracy on training dataset: 32.812 %\n",
      "[step:   940] loss: 0.4401\n",
      "Accuracy on training dataset: 22.656 %\n",
      "[step:   950] loss: 0.4464\n",
      "Accuracy on training dataset: 23.438 %\n",
      "[step:   960] loss: 0.4247\n",
      "Accuracy on training dataset: 32.812 %\n",
      "[step:   970] loss: 0.4233\n",
      "Accuracy on training dataset: 35.156 %\n",
      "[step:   980] loss: 0.4204\n",
      "Accuracy on training dataset: 35.938 %\n",
      "[step:   990] loss: 0.4151\n",
      "Accuracy on training dataset: 42.188 %\n",
      "[step:  1000] loss: 0.4067\n",
      "Accuracy on training dataset: 48.438 %\n",
      "[step:  1010] loss: 0.4204\n",
      "Accuracy on training dataset: 32.812 %\n",
      "[step:  1020] loss: 0.4461\n",
      "Accuracy on training dataset: 18.750 %\n",
      "[step:  1030] loss: 0.4237\n",
      "Accuracy on training dataset: 35.156 %\n",
      "[step:  1040] loss: 0.4123\n",
      "Accuracy on training dataset: 40.625 %\n",
      "[step:  1050] loss: 0.3938\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  1060] loss: 0.4024\n",
      "Accuracy on training dataset: 43.750 %\n",
      "[step:  1070] loss: 0.4030\n",
      "Accuracy on training dataset: 46.094 %\n",
      "[step:  1080] loss: 0.4058\n",
      "Accuracy on training dataset: 54.688 %\n",
      "[step:  1090] loss: 0.3821\n",
      "Accuracy on training dataset: 60.938 %\n",
      "[step:  1100] loss: 0.3876\n",
      "Accuracy on training dataset: 57.031 %\n",
      "[step:  1110] loss: 0.3937\n",
      "Accuracy on training dataset: 54.688 %\n",
      "[step:  1120] loss: 0.3822\n",
      "Accuracy on training dataset: 60.938 %\n",
      "[step:  1130] loss: 0.3835\n",
      "Accuracy on training dataset: 59.375 %\n",
      "[step:  1140] loss: 0.3895\n",
      "Accuracy on training dataset: 56.250 %\n",
      "[step:  1150] loss: 0.3764\n",
      "Accuracy on training dataset: 61.719 %\n",
      "[step:  1160] loss: 0.3753\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:  1170] loss: 0.3872\n",
      "Accuracy on training dataset: 54.688 %\n",
      "[step:  1180] loss: 0.3780\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  1190] loss: 0.3697\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  1200] loss: 0.4009\n",
      "Accuracy on training dataset: 50.000 %\n",
      "[step:  1210] loss: 0.3645\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  1220] loss: 0.3575\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  1230] loss: 0.3726\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  1240] loss: 0.3730\n",
      "Accuracy on training dataset: 64.844 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  1250] loss: 0.3782\n",
      "Accuracy on training dataset: 61.719 %\n",
      "[step:  1260] loss: 0.3554\n",
      "Accuracy on training dataset: 75.781 %\n",
      "[step:  1270] loss: 0.3833\n",
      "Accuracy on training dataset: 58.594 %\n",
      "[step:  1280] loss: 0.3826\n",
      "Accuracy on training dataset: 53.125 %\n",
      "[step:  1290] loss: 0.3622\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:  1300] loss: 0.3605\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:  1310] loss: 0.3538\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:  1320] loss: 0.3747\n",
      "Accuracy on training dataset: 57.031 %\n",
      "[step:  1330] loss: 0.3671\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  1340] loss: 0.3624\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  1350] loss: 0.3541\n",
      "Accuracy on training dataset: 72.656 %\n",
      "[step:  1360] loss: 0.3553\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  1370] loss: 0.3594\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  1380] loss: 0.3515\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:  1390] loss: 0.3611\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  1400] loss: 0.3625\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  1410] loss: 0.3601\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  1420] loss: 0.3477\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:  1430] loss: 0.3606\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  1440] loss: 0.3562\n",
      "Accuracy on training dataset: 72.656 %\n",
      "[step:  1450] loss: 0.3831\n",
      "Accuracy on training dataset: 57.031 %\n",
      "[step:  1460] loss: 0.3408\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:  1470] loss: 0.3583\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  1480] loss: 0.3620\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  1490] loss: 0.3479\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  1500] loss: 0.3548\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  1510] loss: 0.3700\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  1520] loss: 0.3479\n",
      "Accuracy on training dataset: 73.438 %\n",
      "[step:  1530] loss: 0.3658\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  1540] loss: 0.3518\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  1550] loss: 0.3513\n",
      "Accuracy on training dataset: 71.875 %\n",
      "[step:  1560] loss: 0.3476\n",
      "Accuracy on training dataset: 75.781 %\n",
      "[step:  1570] loss: 0.3482\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  1580] loss: 0.3435\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:  1590] loss: 0.3453\n",
      "Accuracy on training dataset: 75.781 %\n",
      "[step:  1600] loss: 0.3559\n",
      "Accuracy on training dataset: 72.656 %\n",
      "[step:  1610] loss: 0.3474\n",
      "Accuracy on training dataset: 73.438 %\n",
      "[step:  1620] loss: 0.3465\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  1630] loss: 0.3551\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  1640] loss: 0.3591\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  1650] loss: 0.3424\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  1660] loss: 0.3534\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  1670] loss: 0.3444\n",
      "Accuracy on training dataset: 75.781 %\n",
      "[step:  1680] loss: 0.3454\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  1690] loss: 0.3453\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:  1700] loss: 0.3554\n",
      "Accuracy on training dataset: 71.094 %\n",
      "[step:  1710] loss: 0.3425\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:  1720] loss: 0.3496\n",
      "Accuracy on training dataset: 71.094 %\n",
      "[step:  1730] loss: 0.3431\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  1740] loss: 0.3451\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  1750] loss: 0.3319\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  1760] loss: 0.3309\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  1770] loss: 0.3460\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:  1780] loss: 0.3445\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  1790] loss: 0.3467\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  1800] loss: 0.3390\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:  1810] loss: 0.3339\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  1820] loss: 0.3328\n",
      "Accuracy on training dataset: 80.469 %\n",
      "[step:  1830] loss: 0.3410\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:  1840] loss: 0.3392\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  1850] loss: 0.3303\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  1860] loss: 0.3412\n",
      "Accuracy on training dataset: 73.438 %\n",
      "[step:  1870] loss: 0.3285\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  1880] loss: 0.3351\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  1890] loss: 0.3462\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:  1900] loss: 0.3297\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  1910] loss: 0.3275\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  1920] loss: 0.3364\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  1930] loss: 0.3385\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:  1940] loss: 0.3227\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  1950] loss: 0.3299\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:  1960] loss: 0.3296\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  1970] loss: 0.3356\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:  1980] loss: 0.4574\n",
      "Accuracy on training dataset: 17.188 %\n",
      "[step:  1990] loss: 0.4462\n",
      "Accuracy on training dataset: 21.094 %\n",
      "[step:  2000] loss: 0.4205\n",
      "Accuracy on training dataset: 34.375 %\n",
      "[step:  2010] loss: 0.3659\n",
      "Accuracy on training dataset: 64.844 %\n",
      "[step:  2020] loss: 0.4141\n",
      "Accuracy on training dataset: 38.281 %\n",
      "[step:  2030] loss: 0.4487\n",
      "Accuracy on training dataset: 20.312 %\n",
      "[step:  2040] loss: 0.4614\n",
      "Accuracy on training dataset: 16.406 %\n",
      "[step:  2050] loss: 0.4675\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2060] loss: 0.4611\n",
      "Accuracy on training dataset: 16.406 %\n",
      "[step:  2070] loss: 0.4525\n",
      "Accuracy on training dataset: 19.531 %\n",
      "[step:  2080] loss: 0.4613\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  2090] loss: 0.4608\n",
      "Accuracy on training dataset: 16.406 %\n",
      "[step:  2100] loss: 0.4399\n",
      "Accuracy on training dataset: 24.219 %\n",
      "[step:  2110] loss: 0.4572\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  2120] loss: 0.4340\n",
      "Accuracy on training dataset: 28.125 %\n",
      "[step:  2130] loss: 0.4370\n",
      "Accuracy on training dataset: 25.781 %\n",
      "[step:  2140] loss: 0.4369\n",
      "Accuracy on training dataset: 25.000 %\n",
      "[step:  2150] loss: 0.4387\n",
      "Accuracy on training dataset: 21.094 %\n",
      "[step:  2160] loss: 0.4386\n",
      "Accuracy on training dataset: 22.656 %\n",
      "[step:  2170] loss: 0.4403\n",
      "Accuracy on training dataset: 21.875 %\n",
      "[step:  2180] loss: 0.4495\n",
      "Accuracy on training dataset: 16.406 %\n",
      "[step:  2190] loss: 0.4412\n",
      "Accuracy on training dataset: 24.219 %\n",
      "[step:  2200] loss: 0.4408\n",
      "Accuracy on training dataset: 21.094 %\n",
      "[step:  2210] loss: 0.4459\n",
      "Accuracy on training dataset: 17.969 %\n",
      "[step:  2220] loss: 0.4086\n",
      "Accuracy on training dataset: 39.844 %\n",
      "[step:  2230] loss: 0.4209\n",
      "Accuracy on training dataset: 34.375 %\n",
      "[step:  2240] loss: 0.4141\n",
      "Accuracy on training dataset: 40.625 %\n",
      "[step:  2250] loss: 0.4107\n",
      "Accuracy on training dataset: 37.500 %\n",
      "[step:  2260] loss: 0.3864\n",
      "Accuracy on training dataset: 53.125 %\n",
      "[step:  2270] loss: 0.3925\n",
      "Accuracy on training dataset: 46.875 %\n",
      "[step:  2280] loss: 0.3672\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  2290] loss: 0.3425\n",
      "Accuracy on training dataset: 73.438 %\n",
      "[step:  2300] loss: 0.3455\n",
      "Accuracy on training dataset: 73.438 %\n",
      "[step:  2310] loss: 0.3372\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:  2320] loss: 0.3410\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  2330] loss: 0.3245\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  2340] loss: 0.3305\n",
      "Accuracy on training dataset: 80.469 %\n",
      "[step:  2350] loss: 0.3387\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:  2360] loss: 0.3274\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:  2370] loss: 0.3285\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  2380] loss: 0.3244\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  2390] loss: 0.3339\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:  2400] loss: 0.3390\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:  2410] loss: 0.3300\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  2420] loss: 0.3273\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  2430] loss: 0.3259\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  2440] loss: 0.3396\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  2450] loss: 0.3248\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  2460] loss: 0.3300\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  2470] loss: 0.3326\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:  2480] loss: 0.3338\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:  2490] loss: 0.3308\n",
      "Accuracy on training dataset: 80.469 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  2500] loss: 0.3273\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:  2510] loss: 0.3295\n",
      "Accuracy on training dataset: 80.469 %\n",
      "[step:  2520] loss: 0.3301\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  2530] loss: 0.3389\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:  2540] loss: 0.3272\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  2550] loss: 0.3352\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:  2560] loss: 0.3241\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  2570] loss: 0.3322\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  2580] loss: 0.3360\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:  2590] loss: 0.3231\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  2600] loss: 0.3314\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  2610] loss: 0.3304\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:  2620] loss: 0.3296\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  2630] loss: 0.3308\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  2640] loss: 0.3259\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  2650] loss: 0.3183\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  2660] loss: 0.3417\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  2670] loss: 0.3314\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  2680] loss: 0.3301\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  2690] loss: 0.3334\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  2700] loss: 0.3256\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  2710] loss: 0.3280\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  2720] loss: 0.3249\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  2730] loss: 0.3345\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  2740] loss: 0.3275\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  2750] loss: 0.3233\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  2760] loss: 0.3249\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  2770] loss: 0.3218\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  2780] loss: 0.3225\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  2790] loss: 0.3263\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  2800] loss: 0.3220\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  2810] loss: 0.3189\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  2820] loss: 0.3189\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  2830] loss: 0.3148\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  2840] loss: 0.3282\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  2850] loss: 0.3191\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  2860] loss: 0.3187\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  2870] loss: 0.3181\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  2880] loss: 0.3188\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  2890] loss: 0.3241\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  2900] loss: 0.3255\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  2910] loss: 0.3185\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  2920] loss: 0.3166\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  2930] loss: 0.3198\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  2940] loss: 0.3196\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  2950] loss: 0.3185\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  2960] loss: 0.3163\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  2970] loss: 0.3242\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  2980] loss: 0.3205\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  2990] loss: 0.3142\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  3000] loss: 0.3151\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  3010] loss: 0.3241\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  3020] loss: 0.3147\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  3030] loss: 0.3197\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  3040] loss: 0.3186\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  3050] loss: 0.3202\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  3060] loss: 0.3201\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  3070] loss: 0.3247\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  3080] loss: 0.3179\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  3090] loss: 0.3181\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  3100] loss: 0.3233\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  3110] loss: 0.3127\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  3120] loss: 0.3165\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  3130] loss: 0.3228\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  3140] loss: 0.3174\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  3150] loss: 0.3165\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  3160] loss: 0.3192\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  3170] loss: 0.3156\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  3180] loss: 0.3156\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  3190] loss: 0.3131\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  3200] loss: 0.3107\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  3210] loss: 0.3222\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  3220] loss: 0.3128\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  3230] loss: 0.3172\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  3240] loss: 0.3244\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  3250] loss: 0.3133\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  3260] loss: 0.3118\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  3270] loss: 0.3160\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  3280] loss: 0.3197\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  3290] loss: 0.3142\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  3300] loss: 0.3138\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  3310] loss: 0.3115\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  3320] loss: 0.3189\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  3330] loss: 0.3207\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  3340] loss: 0.3160\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  3350] loss: 0.3172\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  3360] loss: 0.3191\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  3370] loss: 0.3183\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  3380] loss: 0.3124\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  3390] loss: 0.3157\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  3400] loss: 0.3180\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  3410] loss: 0.3172\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  3420] loss: 0.3092\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  3430] loss: 0.3157\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  3440] loss: 0.3106\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  3450] loss: 0.3127\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  3460] loss: 0.3142\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  3470] loss: 0.3124\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  3480] loss: 0.3130\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  3490] loss: 0.3118\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  3500] loss: 0.3135\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  3510] loss: 0.3175\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  3520] loss: 0.3117\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  3530] loss: 0.3086\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  3540] loss: 0.3142\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  3550] loss: 0.3168\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  3560] loss: 0.3118\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  3570] loss: 0.3126\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  3580] loss: 0.3110\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  3590] loss: 0.3141\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  3600] loss: 0.3090\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  3610] loss: 0.3118\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  3620] loss: 0.3057\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  3630] loss: 0.3123\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  3640] loss: 0.3183\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  3650] loss: 0.3096\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  3660] loss: 0.3132\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  3670] loss: 0.3096\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  3680] loss: 0.3109\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  3690] loss: 0.3110\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  3700] loss: 0.3107\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  3710] loss: 0.3132\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  3720] loss: 0.3094\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  3730] loss: 0.3087\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  3740] loss: 0.3118\n",
      "Accuracy on training dataset: 94.531 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  3750] loss: 0.3123\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  3760] loss: 0.3077\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  3770] loss: 0.3106\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  3780] loss: 0.3046\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  3790] loss: 0.3136\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  3800] loss: 0.3112\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  3810] loss: 0.3087\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  3820] loss: 0.3111\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  3830] loss: 0.3009\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3840] loss: 0.3091\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  3850] loss: 0.3052\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  3860] loss: 0.3088\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  3870] loss: 0.3046\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  3880] loss: 0.3066\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  3890] loss: 0.3083\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  3900] loss: 0.3048\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  3910] loss: 0.3090\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  3920] loss: 0.3051\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  3930] loss: 0.3094\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  3940] loss: 0.3053\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  3950] loss: 0.3109\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  3960] loss: 0.3074\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  3970] loss: 0.3067\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  3980] loss: 0.3050\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  3990] loss: 0.3074\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4000] loss: 0.3040\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  4010] loss: 0.3087\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  4020] loss: 0.3066\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4030] loss: 0.3012\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4040] loss: 0.3059\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4050] loss: 0.3112\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  4060] loss: 0.3043\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4070] loss: 0.3027\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4080] loss: 0.3048\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4090] loss: 0.3050\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4100] loss: 0.3041\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4110] loss: 0.3047\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4120] loss: 0.3044\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4130] loss: 0.3062\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4140] loss: 0.3058\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4150] loss: 0.3038\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4160] loss: 0.3036\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4170] loss: 0.3044\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4180] loss: 0.3025\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  4190] loss: 0.3075\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4200] loss: 0.3090\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  4210] loss: 0.3040\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4220] loss: 0.3038\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4230] loss: 0.3030\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4240] loss: 0.3063\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  4250] loss: 0.3061\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4260] loss: 0.3065\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4270] loss: 0.3038\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4280] loss: 0.3051\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4290] loss: 0.3045\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4300] loss: 0.3050\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4310] loss: 0.3028\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4320] loss: 0.3032\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4330] loss: 0.3015\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4340] loss: 0.3075\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  4350] loss: 0.3003\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4360] loss: 0.3021\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4370] loss: 0.3038\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4380] loss: 0.3000\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4390] loss: 0.3036\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4400] loss: 0.3082\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  4410] loss: 0.3018\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4420] loss: 0.3061\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  4430] loss: 0.3005\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4440] loss: 0.3030\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4450] loss: 0.3033\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4460] loss: 0.3030\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4470] loss: 0.3025\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4480] loss: 0.3021\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4490] loss: 0.3011\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4500] loss: 0.3017\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4510] loss: 0.3017\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4520] loss: 0.3029\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4530] loss: 0.2987\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4540] loss: 0.3017\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4550] loss: 0.3005\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4560] loss: 0.2990\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4570] loss: 0.3024\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4580] loss: 0.3004\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4590] loss: 0.2997\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4600] loss: 0.3020\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4610] loss: 0.3024\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4620] loss: 0.2989\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4630] loss: 0.2998\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4640] loss: 0.3000\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4650] loss: 0.3011\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4660] loss: 0.3003\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4670] loss: 0.3029\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4680] loss: 0.3010\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4690] loss: 0.2978\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4700] loss: 0.3000\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4710] loss: 0.2998\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4720] loss: 0.3001\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4730] loss: 0.2982\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4740] loss: 0.3007\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4750] loss: 0.2987\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4760] loss: 0.2976\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4770] loss: 0.3024\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4780] loss: 0.3017\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  4790] loss: 0.2976\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4800] loss: 0.3004\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4810] loss: 0.3017\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4820] loss: 0.3003\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4830] loss: 0.3005\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  4840] loss: 0.3014\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4850] loss: 0.2980\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4860] loss: 0.3001\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4870] loss: 0.2979\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4880] loss: 0.2987\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4890] loss: 0.2981\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4900] loss: 0.2983\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4910] loss: 0.2972\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4920] loss: 0.2997\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4930] loss: 0.2982\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4940] loss: 0.2976\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4950] loss: 0.2997\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4960] loss: 0.2980\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  4970] loss: 0.2979\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  4980] loss: 0.2973\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  4990] loss: 0.2981\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5000] loss: 0.2977\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5010] loss: 0.2979\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5020] loss: 0.2970\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5030] loss: 0.2970\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5040] loss: 0.3008\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  5050] loss: 0.2978\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5060] loss: 0.2965\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5070] loss: 0.2970\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5080] loss: 0.2959\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5090] loss: 0.2982\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5100] loss: 0.2959\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5110] loss: 0.2999\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  5120] loss: 0.2978\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5130] loss: 0.2973\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5140] loss: 0.2972\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5150] loss: 0.2964\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5160] loss: 0.2980\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5170] loss: 0.2977\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5180] loss: 0.2975\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5190] loss: 0.2986\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5200] loss: 0.2975\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5210] loss: 0.2962\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5220] loss: 0.2962\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5230] loss: 0.2963\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5240] loss: 0.2976\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5250] loss: 0.2958\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5260] loss: 0.2961\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5270] loss: 0.2968\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5280] loss: 0.2956\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5290] loss: 0.2948\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5300] loss: 0.2982\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  5310] loss: 0.2962\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5320] loss: 0.2965\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5330] loss: 0.2970\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5340] loss: 0.2968\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5350] loss: 0.2965\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5360] loss: 0.2956\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5370] loss: 0.2959\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5380] loss: 0.2955\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5390] loss: 0.2967\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5400] loss: 0.2963\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5410] loss: 0.2960\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5420] loss: 0.2953\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5430] loss: 0.2959\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5440] loss: 0.2958\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5450] loss: 0.2951\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5460] loss: 0.2971\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5470] loss: 0.2969\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5480] loss: 0.2958\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5490] loss: 0.2968\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5500] loss: 0.2966\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5510] loss: 0.2951\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5520] loss: 0.2977\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  5530] loss: 0.2983\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  5540] loss: 0.2956\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5550] loss: 0.2952\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5560] loss: 0.2962\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5570] loss: 0.2954\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5580] loss: 0.2952\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5590] loss: 0.2954\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5600] loss: 0.2949\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5610] loss: 0.2951\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5620] loss: 0.2951\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5630] loss: 0.2950\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5640] loss: 0.2957\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5650] loss: 0.2950\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5660] loss: 0.2948\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5670] loss: 0.2950\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5680] loss: 0.2959\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5690] loss: 0.2954\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5700] loss: 0.2966\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5710] loss: 0.2951\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5720] loss: 0.2952\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5730] loss: 0.2965\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5740] loss: 0.2955\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5750] loss: 0.2947\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5760] loss: 0.2958\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5770] loss: 0.2943\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5780] loss: 0.2954\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5790] loss: 0.2947\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5800] loss: 0.2947\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5810] loss: 0.2950\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5820] loss: 0.2951\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5830] loss: 0.2955\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5840] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5850] loss: 0.2947\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5860] loss: 0.2947\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5870] loss: 0.2951\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5880] loss: 0.2943\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5890] loss: 0.2946\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5900] loss: 0.2963\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  5910] loss: 0.2953\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5920] loss: 0.2945\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5930] loss: 0.2948\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5940] loss: 0.2956\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5950] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5960] loss: 0.2951\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5970] loss: 0.2946\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5980] loss: 0.2945\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  5990] loss: 0.3734\n",
      "Accuracy on training dataset: 56.250 %\n",
      "[step:  6000] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6010] loss: 0.2949\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6020] loss: 0.2956\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6030] loss: 0.2946\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6040] loss: 0.2957\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6050] loss: 0.2944\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6060] loss: 0.2943\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6070] loss: 0.2945\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6080] loss: 0.2948\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6090] loss: 0.2958\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  6100] loss: 0.2941\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6110] loss: 0.2944\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6120] loss: 0.2944\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6130] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6140] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6150] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6160] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6170] loss: 0.2941\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6180] loss: 0.2943\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6190] loss: 0.2946\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6200] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6210] loss: 0.2941\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  6220] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6230] loss: 0.2952\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6240] loss: 0.2952\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6250] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6260] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6270] loss: 0.2947\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6280] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6290] loss: 0.2950\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6300] loss: 0.2943\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6310] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6320] loss: 0.2944\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6330] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6340] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6350] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6360] loss: 0.2941\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6370] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6380] loss: 0.2945\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6390] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6400] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6410] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6420] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6430] loss: 0.2951\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  6440] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6450] loss: 0.2943\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6460] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6470] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6480] loss: 0.2947\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6490] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6500] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6510] loss: 0.2949\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  6520] loss: 0.2945\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6530] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6540] loss: 0.2945\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6550] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6560] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6570] loss: 0.2943\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6580] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6590] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6600] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6610] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6620] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6630] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6640] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6650] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6660] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6670] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6680] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6690] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6700] loss: 0.2942\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6710] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6720] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6730] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6740] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6750] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6760] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6770] loss: 0.2941\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6780] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6790] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6800] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6810] loss: 0.2952\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  6820] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6830] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6840] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6850] loss: 0.2941\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6860] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6870] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6880] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6890] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6900] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6910] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6920] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6930] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6940] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6950] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6960] loss: 0.2938\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6970] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6980] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  6990] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7000] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7010] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7020] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7030] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7040] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7050] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7060] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7070] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7080] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7090] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7100] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7110] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7120] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7130] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7140] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7150] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7160] loss: 0.2940\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7170] loss: 0.2936\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7180] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7190] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7200] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7210] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7220] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7230] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7240] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7250] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7260] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7270] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7280] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7290] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7300] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7310] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7320] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7330] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7340] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7350] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7360] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7370] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7380] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7390] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7400] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7410] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7420] loss: 0.2935\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7430] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7440] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  7450] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7460] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7470] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7480] loss: 0.2934\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7490] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7500] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7510] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7520] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7530] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7540] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7550] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7560] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7570] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7580] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7590] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7600] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7610] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7620] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7630] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7640] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7650] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7660] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7670] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7680] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7690] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7700] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7710] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7720] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7730] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7740] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7750] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7760] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7770] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7780] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7790] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7800] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7810] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7820] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7830] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7840] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7850] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7860] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7870] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7880] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7890] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7900] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7910] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7920] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7930] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7940] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7950] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7960] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7970] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7980] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  7990] loss: 0.2932\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8000] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8010] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8020] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8030] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8040] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8050] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8060] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8070] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8080] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8090] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8100] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8110] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8120] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8130] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8140] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8150] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8160] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8170] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8180] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8190] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8200] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8210] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8220] loss: 0.2933\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8230] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8240] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8250] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8260] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8270] loss: 0.2931\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8280] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8290] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8300] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8310] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8320] loss: 0.2937\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8330] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8340] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8350] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8360] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8370] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8380] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8390] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8400] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8410] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8420] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8430] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8440] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8450] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8460] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8470] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8480] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8490] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8500] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8510] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8520] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8530] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8540] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8550] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8560] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8570] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8580] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8590] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8600] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8610] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8620] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8630] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8640] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8650] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8660] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8670] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  8680] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8690] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8700] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8710] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8720] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8730] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8740] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8750] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8760] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8770] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8780] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8790] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8800] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8810] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8820] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8830] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8840] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8850] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8860] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8870] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8880] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8890] loss: 0.2929\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8900] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8910] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8920] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8930] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8940] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8950] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8960] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8970] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8980] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8990] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9000] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9010] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9020] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9030] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9040] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9050] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9060] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9070] loss: 0.2930\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9080] loss: 0.2928\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9090] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9100] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9110] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9120] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9130] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9140] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9150] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9160] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9170] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9180] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9190] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9200] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9210] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9220] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9230] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9240] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9250] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9260] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9270] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9280] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9290] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9300] loss: 0.2939\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9310] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9320] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9330] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9340] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9350] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9360] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9370] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9380] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9390] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9400] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9410] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9420] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9430] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9440] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9450] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9460] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9470] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9480] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9490] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9500] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9510] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9520] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9530] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9540] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9550] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9560] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9570] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9580] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9590] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9600] loss: 0.4286\n",
      "Accuracy on training dataset: 32.031 %\n",
      "[step:  9610] loss: 0.2969\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9620] loss: 0.3405\n",
      "Accuracy on training dataset: 75.781 %\n",
      "[step:  9630] loss: 0.3434\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  9640] loss: 0.3318\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  9650] loss: 0.3136\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  9660] loss: 0.2946\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9670] loss: 0.2955\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9680] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9690] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9700] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9710] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9720] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9730] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9740] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9750] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9760] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9770] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9780] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9790] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9800] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9810] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9820] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9830] loss: 0.2927\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9840] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9850] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9860] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9870] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9880] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9890] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9900] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  9910] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9920] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9930] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9940] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9950] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9960] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9970] loss: 0.2925\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9980] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9990] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step: 10000] loss: 0.2926\n",
      "Accuracy on training dataset: 100.000 %\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "model4, losses4, accuracies4 = train_2(config, 19, 128, 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xl8XHW5+PHPM5PJ0jTdaAqlaUlLy1IKFAhlX2TfpAqIVBRQfiICwhW83nLZrmURvIJXFFEQ9CpCL7JIZatlKQqyNIXajRbSBbo33dI0+8w8vz/OmeTMZJJMOpnMmeR5v17z6pzvWeZ7ctJ58t1FVTHGGGN2VyDbGTDGGJPbLJAYY4xJiwUSY4wxabFAYowxJi0WSIwxxqTFAokxxpi0WCAxph8SkbdF5Ips58P0DRZITL8mIqtF5LRe/sz/FJFd7qtRRCKe7SW9mRdjeoIFEmN6mareo6oDVXUgcDXwbmxbVQ9KPF5E8no/l8akzgKJMR0QkW+LSJWIbBORWSKyt5suIvIzEdksIjtFZJGITHL3nSMiS0WkVkTWicgPduNz80REReQaEakClrnpE0XkNTc/y0TkQs85T4jIgyLyivvZ74rIWM/+s0RkuYjUiMjPAUn352NMjAUSY5IQkVOAHwMXAyOBz4CZ7u4zgBOB/YDB7jFb3X2PAd9R1RJgEvBGGtk4HzgSOFhEBgJzgD8AI4BLgUdEZH/P8V8DbgOGAZ8Dd7r3MgJ4BpgODAfWAkelkS9j4lggMSa5S4HHVfVDVW0CbgaOEZFyoAUoAQ4ARFU/VtUN7nktwEQRGaSq21X1wzTycI97jQZgKvCJqv5BVcOqOh/4C3CR5/hnVLVSVVuAPwGT3fTzgAWq+ry7736gOo18GRPHAokxye2NUwoBQFV34ZQ6RqnqG8AvgYeAzSLyiIgMcg+9EDgH+ExE3hKRY9LIwxrP+32A40RkR+wFfBWntBSz0fO+HhjouZfWa6lqFKdUYkyPsEBiTHLrcb68ARCRYmAPYB2Aqj6oqkcAE3GquP7dTZ+nqlNxqp/+AjydRh68U3OvAV5X1SGe10BVvS6F62wARnvuJQCUpZEvY+JYIDEGQiJS6HnlAU8B3xSRySJSANwDvK+qq0XkSBE5SkRCQB3QCERFJF9ELhWRwW4V0k4g2kN5nAUcJCJfE5GQ+5qS0EbSkReBySIy1c3z94HSHsqXMRZIjAFeBho8r/9S1ddwGq6fxfmLfl/gEvf4QcCjwHac6q+twH+7+74BrBaRnThdey/tiQyqag1wJvB1Nz8bcToDFKRw7iacarD/BrYAY4D3eyJfxoDTUJjtPBhjjMlhViIxxhiTFgskxhhj0mKBxBhjTFoyGkg80zJUicj0To670J0SosLdvlREFnheURGZ7O6b614ztm9EJu/BGGNM5zLW2C4iQeAT4HScwU/zgGmqujThuBLgJSAfuE5VKxP2Hwz8RVX3dbfnAj9IPK4zw4cP1/Ly8t2/GWOM6Yfmz5+/RVW77CqeyVlFpwBVqroSQERm4kzzsDThuDuB+3AHdCUxjbY5jnZLeXk5lZUpxx1jjDGAiHzW9VGZrdoaRfwUD2vdtFYicjgwWlVf6uQ6X8UZHOb1O7da6zYRSTqLqYhcJSKVIlJZXW3TChljTKZkrbHdnabhAeCmTo45CqhX1cWe5EtV9WDgBPf1jWTnquojqlqhqhWlpTaI1xhjMiWTgWQdnvl9cOb2WefZjk2zPVdEVgNHA7NiDe6uS0gojahqbK6jWuBJnCo0Y4wxWZLJQDIPmCAiY0UkHycozIrtVNUaVR2uquWqWg68B5wfa0R3SywX42kfcRf8Ge6+D+FMj+0trRhjjOllGWtsV9WwiFwHzAaCOGs7LBGRGUClqs7q/AqcCKyJNda7CoDZbhAJAq/hzHlkjDEmS/rFXFsVFRVqvbaMMaZ7RGS+qlZ0dZyNbDfGGJMWCySmnW11zby4cH22s2GMyRGZHJBoctR3n5jP+6u2MaV8GCMGFWY7O8YYn7MSiWln9dY6AFqifb/9zBiTPgskpp1Y/GgO99QqscaYvswCiWkn1pGvsSWS3YwYY3KCBRLTTtSNJBZIjDGpsEBi2okFkgYLJMaYFFgg6URtYwvb6poBUFW8gzcbmiNptSFsq2sm4tPG7KibrzeXbc5yTowxucC6/3agJRJl6i/fYeWWOg4tG8yyjbU0haOUFORx1LhhvPax8yV7aNlgFq/fyYQRA2lsibB6az1DBoQYO7yY4vw8WiJRdjWFWbJ+JwBHjR1GTUMLyzbWAnDDqRP4/un7Ze0+O/PoP1ZxccVoJuxZku2sGGN8zEokHQgFA1x4RBkA/1pbQ5Nb+qhtCrcGkdi+SFRZtrGW1VvrAdhR38JHn+/g7aotvL9qW2sQAVi0rqY1iAD8/PVP2e6WevwiL9j2a7F2e0MWc2KMyQVWIunENSfvyyFlg3lreTWDikJ875TxNLZEiagysCCPqs27KC0pYPG6GgryAkzcexAfrNrG8eOHkxcMsKspTCSiDCzMY0d9M03hKHsPKaKuKczidTV8sGob98/5hNeXbeYiN2j5QTDQtlaYX6vfjDH+YYGkEyLCCRNKOWFC28JYRfnB1vfjRwwE4Ljxw1vTTt5/ROv7gQVtP949Bha0vi8uyOOocXtQUT6MX7+1gsXravwVSDyLToYtkBhjumBVW1kUDAh7DS5kc21jtrMCwF0vLqV8+ktxJZKfzfkkizkyxuQCCyRZVlIYorYxnO1sAPDbt1cB8VVbyzfVdnS4McYAFkiyrqQwj50+CSQx1i5ijOkOCyRZNqgwRG1jS7azEafJ5tgyxnSDBZIsKynM803VVoxNjWKM6Q4LJFnmBBJ/lUjqmv0V2Iwx/pbRQCIiZ4nIchGpEpHpnRx3oYioiFS42+Ui0iAiC9zXrz3HHiEii9xrPiji6auag0oKQzS2RGmJ+Kc6Sa2JxBjTDRkLJCISBB4CzgYmAtNEZGKS40qAG4D3E3atUNXJ7utqT/rDwLeBCe7rrEzkv7eUFDpjTfxQveXprMW0KaOzlxFjTE7JZIlkClClqitVtRmYCUxNctydwH1Al4MpRGQkMEhV31NnBsU/AF/qwTz3upLCEIAvqrdCnqlRCvLaBl6qKmEflZiMMf6SyUAyCljj2V7rprUSkcOB0ar6UpLzx4rIRyLyloic4Lnm2s6u6bn2VSJSKSKV1dXVu30TmRYrkSxYsyPrjdzeQFIYagsk9766jPG3vGLBxBiTVNYa20UkADwA3JRk9wZgjKoeBtwIPCkig7pzfVV9RFUrVLWitLS06xOyZJBbIrlh5gJ++MzCrOYlFGyr2yrIa/vV+P07qwFotG7BxpgkMhlI1gHeivYyNy2mBJgEzBWR1cDRwCwRqVDVJlXdCqCq84EVwH7u+WWdXDPnxEokAG9XbcliTuJn/c33BJJY43uTdQs2xiSRyUAyD5ggImNFJB+4BJgV26mqNao6XFXLVbUceA84X1UrRaTUbaxHRMbhNKqvVNUNwE4ROdrtrXUZ8EIG7yHjYiUScEaUPzt/LUvW17Cjvpmdvdxuku8JJHmelvfYiok2UNEYk0zGZv9V1bCIXAfMBoLA46q6RERmAJWqOquT008EZohICxAFrlbVbe6+a4DfA0XAK+4rZ3lLJNGoctOf/9W6LQKrfnxur+Ulz1O15S2dxALJsfe+wep7ey8/xpjckNFp5FX1ZeDlhLTbOzj2ZM/7Z4FnOziuEqdKrE8Y6AkkiVO29/Z4joBnSI4npmBTbxljOmMj27PM21OqKZzdNgjv0M5AIKfHeRpjepEFEh8YObgQyP5f/t4SiYURY0yqLJD4gHclxWzyFkJyfOYZY0wvskDiA97le7PJWyIJWCAxxqTIAokPeEeRQ/wYjt4UV7XliSNDBoSSHG2MMQ4LJD5wwF4lcduhLDV0Bzy/Dd4s7KjP/jxgxhj/skDiA7ece2DcdrbaJ+JLJMKtCfmyjlzGmGQskPhAQV6QPYrzW7ez9X0tCb22hg5oy9PZk/bKeq8yY4w/WSDxCe+o8mxFEm+JIyAS106yv1v9prbqlTEmgQUSn8jzNFBkq0QS12sr0LbY1tjhxYibKyuVGGMSWSDxiYAPnkTcOBKEbXXNAHzxkJGt+6xEYoxJ5IOvLwMQ9MG4DUno/ru93gkkQ4vzW6dMsRKJMSaRBRKf8H4/Z6/Xlve98O0TxnHo6CFMnTyqtb0kaiUSY0wCf8zNYeK+oLNVOEkckDh62ABeuPa4uH0WR4wxiaxE4hNRz5pRvmhsT4hmASuRGGM6YIHEJ+654OBsZyF+GvmEaBYLLBZIjDGJLJD4xEn7lWY7CwmlkOTlImtsN8YkskDiQ9lqbA8GvFVb8ftag4wFEmOyYuHaHcxZuinb2UjKGtt9KHttJN731kZijJ+c/8t3AFh977lZzkl7GS2RiMhZIrJcRKpEZHonx10oIioiFe726SIyX0QWuf+e4jl2rnvNBe5rRCbvIRuy9VWdOI7Eq20ciQUSY0y8jJVIRCQIPAScDqwF5onILFVdmnBcCXAD8L4neQvwRVVdLyKTgNnAKM/+S1W1MlN5z7ZsjR7vrEQiYgMSjTHJZbJEMgWoUtWVqtoMzASmJjnuTuA+oDGWoKofqep6d3MJUCQiBRnMq69EsvRt3dHCVs4+51+bIsUYkyiTgWQUsMazvZb4UgUicjgwWlVf6uQ6FwIfqmqTJ+13brXWbdJBy7SIXCUilSJSWV1dvZu3kB3NkbZBJb35xZ24HkmyfVYiMcYkylqvLREJAA8AN3VyzEE4pZXveJIvVdWDgRPc1zeSnauqj6hqhapWlJZmv2ttdzS2tAWS3iyddDaOJLZpbSTGmESZDCTrgNGe7TI3LaYEmATMFZHVwNHALE+DexnwPHCZqq6InaSq69x/a4EncarQ+qywD6q5vNsWRowxiTIZSOYBE0RkrIjkA5cAs2I7VbVGVYerarmqlgPvAeeraqWIDAFeAqar6juxc0QkT0SGu+9DwHnA4gzeQ68qG1rULq3FU83VmxLrC1snbbS6LWNMgowFElUNA9fh9Lj6GHhaVZeIyAwROb+L068DxgO3J3TzLQBmi8hCYAFOCefRTN1Db3v+muP4xbTD4tJaIr33xd3ZDMSxEsnc5Zt7LT/GmNyQ0QGJqvoy8HJC2u0dHHuy5/1dwF0dXPaInsqf35SWFHD2pL3i0rJVImk3st39k+O2F5bwjWPKez0/xhj/silSfCYvGP9ImsO9GEg8RZLEEolkbby9McbvLJD4XHMvlkjUE0kSSyRqzezGmA5YIPG5rDW2J5RIwr3YVmOMyS0WSHwo31O91RLuxcb2uKqt+H3ZGm1vjPE/CyQ+VJDX9lh6s2rLK3EcSbbGsxhj/M8CiQ/lewJJb1ZteUskiW0kViIxxnTEAokPBTzf4tkbkBgfSSyQGGM6YoHEh4KSnUDi7ZllbSTGmFRZIPEh75K3vTmOJL5qq+M2EpsmxRjjZYHEh7yBJEs1W0lKJG0ZqWlo6eXcGGP8zAKJD3kDSTjam1VbbTorkeywQGKM8bBA4kPeHlPZWy0xftubj0gvBjdjjP9ZIPGhvEDbY+nNQNLZgMTBRaHW99mqbjPG+JMFEh8KxLWRZKdEkjhFyhXHlnPy/s5Kk9aDyxjjZYHEh4Z4/vrv3RHl3kkb4wNJXjDAtCljAFtu1xgTzwKJD/38kslcefxYAGb8dSnvrtjaK58bV7WVZH9sfIsFEmOMlwUSHxoxqJDrT5kAOHNtTXv0vV7PQ2KJBNp6k1nVljHGywKJTwWDbV/kxfnBXvnM+KV22++Ptd1YicQY42WBxKe806QMLyno9c9PFkhiebJeW8YYr4wGEhE5S0SWi0iViEzv5LgLRURFpMKTdrN73nIRObO718x13kGJ40sH9spnqnbc2A5t67Zb1ZYxxitjgUREgsBDwNnARGCaiExMclwJcAPwvidtInAJcBBwFvArEQmmes2+IM8TSPYaXNj6fnNtY698ftI2EmtsN8YkkckSyRSgSlVXqmozMBOYmuS4O4H7AO835FRgpqo2qeoqoMq9XqrXzHnesSSxL+5n5q9lyt2v8681OzLymdZGYozZHZkMJKOANZ7ttW5aKxE5HBitqi+leG6X1/Rc+yoRqRSRyurq6t27A5+IVSXFugF/sqk2I5/T2ch2aCulWNWWMcYra43tIhIAHgBuysT1VfURVa1Q1YrS0tJMfESvyUbjdmfdf61EYozxysvgtdcBoz3bZW5aTAkwCZjrTsexFzBLRM7v4tzOrtkn9dYXd1zVVpL91mvLGJNMJksk84AJIjJWRPJxGs9nxXaqao2qDlfVclUtB94DzlfVSve4S0SkQETGAhOAD7q6Zl+Vjaok67VljElVxkokqhoWkeuA2UAQeFxVl4jIDKBSVTsMAO5xTwNLgTBwrapGAJJdM1P34BcRt0SiZPYLvKvuv1a1ZYxJJpNVW6jqy8DLCWm3d3DsyQnbdwN3p3LNvi5xadvEmXkzopMBiRZIjDFeNrI9B2Snaqt9mlivLWNMEhZIcsDflm7ioTerMv453oKGVW0ZY1JlgcTHPrrtdIpCzoSNT7z3GRluIonT2VxbMz9Y036nMabfskDiY0OL88nPcx5RKBhojSOZaiHRTha2grZeW++v2pahHBhjcpEFEp9rCkcACHmmle+VtvZkJZJkDSfGmH7PAonPNbY4o/9WVNfRnOGRgPErJHY8aaMxxnhZIMkhLy3c0Guf1VmvLWOM8bJAYlql2mvLGGO8LJDkoKy1kViJxBiTREqBRET2FZEC9/3JInK9iAzJbNZMb/P22kpWjZXpKVqMMbkp1RLJs0BERMYDj+DMwPtkxnJlsqKrcYZDBuQDUBiygqwxpk2q3whRVQ0DXwZ+oar/DozMXLaMX33x0L3Ze3BRtrNhjPGRVANJi4hMAy4HXnTTQpnJkumKIFSu3tZuMsdwJEpLGl2EU6m4CkjbbMTGGAOpB5JvAscAd6vqKneNkD9mLlumM1Wbd3HRr9/lthcWx6V/4f65TLjllYx+dlDEJm00xsRJKZCo6lJVvV5VnxKRoUCJqt6X4byZDsRKHbMWrI9LX7OtIb0LpxAfAgFpVxIyxvRvqfbamisig0RkGPAh8KiIPJDZrBmAuT84mccur4hLi5UIsvF1HhSxqi1jTJxUq7YGq+pO4ALgD6p6FHBa5rJlYsqHF3PqgXvGpYVjgaSHv9BT6d4bCIAVSIwxXqkGkjwRGQlcTFtju8mScNSp2urp7/NU4lJArGrLGBMv1UAyA2ed9BWqOk9ExgGfZi5bpjN/W7IJSO2Lv6cFA1a1ZYyJl9Ka7ar6Z+DPnu2VwIWZypTp3ObapoxcN7Xuv9ZryxgTL9XG9jIReV5ENruvZ0WkLIXzzhKR5SJSJSLTk+y/WkQWicgCEXlbRCa66Ze6abFXVEQmu/vmuteM7RvR3ZvuK7IxZUkwIFkpCRlj/CvVqq3fAbOAvd3XX920DolIEHgIOBuYCEyLBQqPJ1X1YFWdDPwEeABAVf+kqpPd9G8Aq1R1gee8S2P7VXVzivfQ5/T0F3oqjfcBwUokxpg4qQaSUlX9naqG3dfvgdIuzpkCVKnqSlVtBmYCU70HuD3BYopJXrsyzT23Xzt94p7t0prCUf76r/VJjs6cgLWRGGMSpBpItorI10Uk6L6+Dmzt4pxRwBrP9lo3LY6IXCsiK3BKJNcnuc5XgacS0n7nVmvdJh2stiQiV4lIpYhUVldXd5FV/3v40sOTpn/vqY967DNSCQ9B67WV85rCETbXNmY7G6YPSTWQfAun6+9GYANwEXBFT2RAVR9S1X2B/wBu9e4TkaOAelX1zgVyqaoeDJzgvr7RwXUfUdUKVa0oLe2q8OR/ecHMz7ibSkEjGBCiViLJad994kOm3P16trNh+pBUp0j5TFXPV9VSVR2hql+i615b63Cmm48pc9M6MhP4UkLaJSSURlR1nftvLc5U9lNSuAXTQ0SEqPb8YEjTe95Y1m+bFU2GpPNn7o1d7J8HTBCRsSKSjxMUZnkPEJEJns1z8YxNEZEATilopictT0SGu+9DwHlA/MyFZrelWrUFNrrdGNMmpXEkHeh03VVVDYvIdTgDGYPA46q6RERmAJWqOgu4TkROA1qA7TjT1MecCKxxx6zEFACz3SASBF4DHk3jHvoEVY1b0XBl9S4GFYUYPrCgxz8rVsMWiaqt4W6MAdILJF3+TaqqLwMvJ6Td7nl/QyfnzgWOTkirA47obkb7uqZwlMJQsHX7lPvfojAUYNmdZ3fvQql0/w3ESiRWJDHGODqt2hKRWhHZmeRVizOexPSi1248KWl6Y0skSdruL3DVmYBYIDHGxOu0RKKqJb2VEdO1sqHJl7jtqaDRnTYSG5RojInJfJ9Sk3ENSUokuyOl2X9jVVuZKfCYXmQ970xPsUCSQwLJx15S1xROmp6syitdQTcLNro999kjND3FAkkO6aiT1M7GlqTpT7z3WbeuH5sE8oZTJ3R4TNAa2/sMe4Kmp1ggySEdlUhqG5OXSJrC3a9/OvWAEXz/9P063B/rZmzTpOQ+q9rKTX58bhZIckiggyJJR4Gku1KdIgWsaqsvsCdoeooFkj5g1ZZdXPLIu+3Sd+cvlw4KPa2s15Yx2eXHv+HSGZBofOLXb61M+sXe3V+47vTa8uMvs+kee4amp1iJpA/oqHTQ3UKDc3jnRZJY7ZqVSHJfNlbYNOnz41OzQNKHxb4omsPRlBvHu6zasjaSPsMeoekpFkj6sNgXxX63vsK1T36YwvGpLLVrvbaMySbrtWV6ldL2S/fK4o0pndPVfL5t40jSyJjxBR9+H5kcZYEkxwwZEOrW8fXNPTu63dpI+g5rI8lNfnxqFkhyzLvTT039YFVeWLC+W9fvqo3EZv81xiSyQJJjivKDjChJbcGqhetq+M/nF7VuH/qjv/FmJ8usdmtAopVIcp79LZCb/PjcLJDkoKe/cwy3nzexy+N21MfPwVXT0MJ9ry7r9Bzpqvuv9drqM+wJmp5igSQHlQ8v5lvHj92tczuarwtSqzOPjWz3Y88R0z32DHOTH9u2LJDksKe/cwwf/GfHbSbJYkZnbSCqqbeRRGw9kpznv68jkwo/xv+MBhIROUtElotIlYhMT7L/ahFZJCILRORtEZnoppeLSIObvkBEfu055wj3nCoReVCkq6++vmvK2GGMGFTY4f6mJCsndlYigRQCifsbY20kuc+PX0gmN2UskIhIEHgIOBuYCEyLBQqPJ1X1YFWdDPwEeMCzb4WqTnZfV3vSHwa+DUxwX2dl6h5yxfgRA5OmL92ws11aR2uaQPeW2rVeW32APULTQzJZIpkCVKnqSlVtBmYCU70HqKr3m66YLn61RWQkMEhV31OngvcPwJd6Ntu557UbT0r52MXr2wcXr64a221hq77Dj3XtJjdlMpCMAtZ4tte6aXFE5FoRWYFTIrnes2usiHwkIm+JyAmea67t6pruda8SkUoRqayurk7nPvqUSFRpCicfpJhK46vYNPLGZJUf/4bLemO7qj6kqvsC/wHc6iZvAMao6mHAjcCTIjKom9d9RFUrVLWitLS0ZzOd4xqbO2kpT3HSRiuR5D57hKanZDKQrANGe7bL3LSOzMStplLVJlXd6r6fD6wA9nPPL+vGNfuNVAcpAjS0dFAiSeHcoPXa6jMsjuQmP1ZJZjKQzAMmiMhYEckHLgFmeQ8QkQmezXOBT930UrexHhEZh9OovlJVNwA7ReRot7fWZcALGbyHnDHnxpMo32MAAH+8cgqf3HV2h8f+4o1PKZ/+EptrG2kOR3nuw7VOtZZ2PWljrNeWlUhyn40jMT0lYyskqmpYRK4DZgNB4HFVXSIiM4BKVZ0FXCcipwEtwHbgcvf0E4EZItICRIGrVXWbu+8a4PdAEfCK++r3BheFGDm4iNVb6wmIkJ/X8d8If3r/cwBWb6ln7vLV/GruCooLnF+FrnpTx7oPv7hwA2cetFcP5d5kg4WR3OTH+J/RpXZV9WXg5YS02z3vb+jgvGeBZzvYVwlM6sFs9hmxaUtSHVkTDMDGnY0A1DaGU6vacttI/vqv9fxi2mG7k03jE378QjK5KeuN7abnxKoqgilGkobmtpUTY+NLuqza6r/jP/scP9a1m66t39GQ7Sy0Y4GkD4n1yI1NrHjy/qVU7DO0w+Prm8Nt54ikVGde1xROO5/GJyyO5KTTf/b3bGehHQskfUisATxWuvj9N6fwzHePZb89nZHvwYRh7Q0tkdZzYgWNrgocew3ueEqWZD76fDtrttV36xxjTG7JaBuJ6V3e0oXXC9ceT0NLhIEFeZz6wFzWbHOKxtOfXdTaFTggktIfqHsOKuSECcP5x6db2NUUZmBB579CX/7VPwFYfe+53bsZk3FWIDE9xUokfchlR+8DQPkexXHpRflBhhXnk58XIBRse+Te8SRO1VbXbSQA//h0CwCP/n1l+pk2WWON7aanWCDpQy48oozV957L0OL8Do/pKFC0NrZ3ozE9LyAs27iTi3/zLuXTXwJgy64mdja2dHGm8QNrbDc9xaq2DADf/dOHABzRSeN8ovvnfML9cz6JS6u46zVKCvJY9KMzezR/pudZicT0FCuR9DNdlTh6onNvrfXsygkWR0xPsUDSz/REoDhu/B49cBWTbTZFiukpFkhMvBQizT1fPjhpuveLqXz6S74cOGWM6XkWSPqZrtrS8zpbQjF2TDD5r004YY2Seau3tb7fuquJLbuaus6g6TVWIDE9xRrb+5muVkAckN/1r0RHweaZ+Wvjtos91zrirtcAG09iTE+IRrV1Bgs/sBJJP9NViaQoP9jlNRJHyMfc/Nyibl/LZI+VSHJXS9RfCwJZIOmnDi0bnDS9OIUv/1AgtV8bW47X32wcSe5qifjr2Vkg6WcK3HVKbjxj/6T7i1Ko2goGUytS3/3Sx6lnzPQ6K5HkrpawlUhMFn31yDFAxyWPASmUSFJpkAdYvqk29YyZXmdxJHcldmzJNgsk/czXjhrDGzedREX5MO744sR2+/M76JHl1VEbiclE849dAAAYRklEQVQtNo4kd/ltqWsLJP3QuFJnWvlvHje23b4pY4d1eX6qC2clo6qoqq1rYkwarERifGXs8PiZgkcPG9DlOel0O2wKR/ntP1Zx0B2z2ewu82uyw19fRaY7Iv2psV1EzhKR5SJSJSLTk+y/WkQWicgCEXlbRCa66aeLyHx333wROcVzzlz3mgvc14hM3kNf9+YPTuaAvUp67fPqmyO8tGgDAOts5HtW+ax2xHRDxGcPL2OBRESCwEPA2cBEYFosUHg8qaoHq+pk4CfAA276FuCLqnowcDnwx4TzLlXVye5rc6buob94+foTeuQ6XzmijFf/rfNrHX7nHBas2QFAczhqVVxZ5a8vI5O6SD8aRzIFqFLVlaraDMwEpnoPUNWdns1i3N9sVf1IVde76UuAIhEpyGBe+7WeGiE7ckgRB+w1KOXjv/n7eRx0x+y4tEhUWbWlrkfyYzrnsz9qTTf0pzaSUcAaz/ZaNy2OiFwrIitwSiTXJ7nOhcCHquqdqOl3brXWbdLBvOgicpWIVIpIZXV19e7fhenSLeccCMDRHTTUfytJoz441VwAq7bUce8ry/j9O6u4+6WP+cJP58ZVe23a2cjx973BagswPcpfX0WmO8I+ayPJ+lxbqvoQ8JCIfA24FacqCwAROQi4DzjDc8qlqrpOREqAZ4FvAH9Ict1HgEcAKioq/PVT7wMeu7yCIQNCLFxbwxXHlnNxxWgGDwgBUDa0iLXb2wLBoKI8nrvmWC5w129P9N0n5rNsY/yYk1++UcWPL3BmGX5hwTrWbm/gifc+49bz2ndZNrvHSiS5qz91/10HjPZsl7lpHZkJfCm2ISJlwPPAZaq6Ipauquvcf2uBJ3Gq0EwvO/XAPTlin2F887ixiEhrEAH4xw+/ALQNXGwORztt0I+VTLye+uDz1uV7X1q0EbDxKz3NpkjJXf2pamseMEFExopIPnAJMMt7gIhM8GyeC3zqpg8BXgKmq+o7nuPzRGS4+z4EnAcszuA99BsvXHscr9zQM43uIsLqe8/lB2c607C0RKIMyM9jSnnXY1QSLVy7g3+5jfN+mu3UmGzy2zx2GQskqhoGrgNmAx8DT6vqEhGZISLnu4ddJyJLRGQBcCNt1VrXAeOB2xO6+RYAs0VkIbAAp4TzaKbuoT85dPQQDhyZekN5KmKj5GMTzP30K4cyrrS4s1Pa2VrX3Po+LyAs27iTB+Z8YqOye4D9CHOX3wJJRttIVPVl4OWEtNs972/o4Ly7gLs6uOwRPZZBk1Ehd4LI5ojTVXHMHgO47Oh9+K+/Lo077vNt9R1eo76prdrrpUUb+MUbVQBcdeI4BhZkvYkvp1kgyV1+CyQ2st1kTL47S7B3ptKhxfndusa1T37Y+n5ldVuvrUsffc9KJWmyNpLc1Z/aSEw/N6y4wP23LXgcPmZoj1z7X2tr+MZjH9DY4pRY6pvDrX+lNYejTLz9Vf7yUWd9O4zF4dzVnwYkmn7utANH8N8XHcL3T9+vNc07l9cx4/ZI6/pvV23hyfc/JxpVJt4+m//3v/PYWNPI9vpm6psj3GXroZg+KuKvOGKBxGSOiPCVitEUhpKvcfLHK5P33H7s8oqUP2NHQwu1jc40K28ur+boH79Oi/u/LIUZ8fs1K5HkLiuRGOPKCwZ4+jvH8B9nHRCXnp+X+q/lg69/2m7yxwZ3XEo6090b42fWRmKMx5Sxw/juyfvGpcUGHnbUVfjcQ0bGbT8zf23c9kp3KpVUlwTur6yxPXdZry3T771244m8+L3j49JGDSkC4PWbTqKpxSm2lw1NvjbKwIR15R9/Z1Xc9nf+OB+ANdsaWLu9ntrGFlZU7+LhuSusp5eH/Shyl98CiXXEN71u/Ij206X8/YdfQFXJCwZYtsGZd6ujdeV3NDQnTU/m+PveZFxpMbsaw2yubeLiijL2GGgTSYNN2pjLrGrLmCSCASHPbR0/5YARnD1pL24598Ckx26vb+nWtVdW17G51pk8OvavsTXbc1nUAokxnSvKD/Lw14+gbOgABrilkru/PIlpU5w5QM+YuCcL/+uMzi7RoW889kHr+w8/387Tlc5KBw/8bTkVd81JM+e5xV9fRaY7rERiTDd87xRnXs8LDivjxxccwpIfncmVx49lUGHbbMMzph5E2dCilK63ZVdbieSCX/2THz6zEIAH36hiy67Uq8z6AiuQ5C6/tZFYIDG+dvVJ41j143MocksmxQV5JK5ldtkx5RS5Y1XOmLhnl9d8dfEGnvrg89btXZ7lfr1VBq8u3sjGmsa08u9v/voyMqmzEokx3SAi7QJHzI8vOJg7px4EwGFjhgDw1SPblsB57ppj+f5p+7U77+onPuTm5xa1bk/yLPfb5M4LVtPQwtVPzOfqJ+a3O//peWtYuHbHbtyNMT3Db20k1mvL5KxpU8a0vp8xdRJfPqyMY/Ztm3bl8DFDOWz0EOZ8vJHF63amdM365jBF+UGWuys21jS0New/XbmGaFSZ7gah1fee2xO3kTVWtZW7rERiTAYUhoJxQSRGRJh51TEpX6ehJcKbyze3Tm2/aksd3/zdB2yubeSHzyxsDSJ9gb++ikx3+G2KFCuRmD6vO+uWLFm/s3VAY8yby6v525JN7Y5dsGYHk0cPSTt/2WIlktzlt0kbLZCYPucP35rC+oT5t1KVGERibv1L+xWdP9lUm+OBxCJJrrISiTEZduJ+pe3STjtwBK99vLl1uzAU4L4LD0EV/u3/FuzeB+X493COZ79fszYSY7Lgt5cfyfdOGQ/A90/bj2V3ns3UyaOYOnlvDtgrfsqW75w4LqVr/vDZhbz96RYenruC5nAUVaUpHOH9lVt5/eO2qjC/9bCJsQJJ7upX40hE5CwRWS4iVSIyPcn+q0VkkYgsEJG3RWSiZ9/N7nnLReTMVK9pTEdiX5ze3sQiEjf78BXHlnPzOQdSGHL+a3z96DG8duNJfGH/9qUcgK8/9j73vbqM/W59hbE3v8xhM+bw1Ufe48r/rQTglucXMeHWV/h0U21mbioNNvtv7vJbIMlY1ZaIBIGHgNOBtcA8EZmlqks9hz2pqr92jz8feAA4yw0olwAHAXsDr4lIbEBAV9c0JqmoG0kCCcNSYuNUzjtkJHd80flb5p3/OIUdDS3sWzoQgHsvPISj7nm9y8+od9dCcd6H+dP7zsDH03/2dw4fM4Tnrjku7vhIVNle38xwm0jSdEN/qtqaAlSp6kpVbQZmAlO9B6iqt3N/MW3VtlOBmarapKqrgCr3el1e05iOxLoHH1k+LC79jIl7csHho7jtvImtQWWPgQWtQQRgz0GFSa/Z0QzFABNvnx23/eHnO1hZvSsu7ad/W07FXa+xo76ZDTW710Fgt/nru8h0Q78pkQCjgDWe7bXAUYkHici1wI1APnCK59z3Es4d5b7v8pruda8CrgIYM2ZMskNMP3PChFKW/OhMihO6AxeGgjxw8eQuz3/5+hM458F/APCTCw/h6HF7sMfAfA66Y3YXZ7Y55f63Wt8/fkUFryza4Lx/exUPvlHFM1cfQ0VCoMsUf30Vme6I+KyBK+u9tlT1IeAhEfkacCtweQ9d9xHgEYCKigp//dRN1iQGke6YuPcgVt5zDhFVQu6U98m60J60XylvfVLd5fUef3t169T5j73tLM71dtUWXlq0gUPKBvPlw8qoawqzaWcj4zylo5gtu5r4ZGMtx44fvlv347PvItMNkYi/Hl4mA8k6YLRnu8xN68hM4OEUzu3ONY3pUYGAEKCtkUVEuP8rh7LnoEKOGjeMD1ZtY6/BhZzqKXl05O2qLa3v69y2lf957dPWtBWb61iwZgdvV22h6u6zW4NOJKq0RKKc9JM3qWuOxO3rDmtsz11+ayPJZCCZB0wQkbE4X/aXAF/zHiAiE1Q19j/nXCD2fhbwpIg8gNPYPgH4AJCurmlMb7vwiLLW98eNH46q8u9n7s9pB+7J9OcW8tHnO3js8grW72jg/jmfsCPFhbl++WZV6/uTfzqXa04ez/CB+bxdtYU/vPtZ676tdc2tbTh1TWGefP9z9turhJMSxtM0tkQoDLW16ViJJHf1mwGJqhoWkeuA2UAQeFxVl4jIDKBSVWcB14nIaUALsB23Wss97mlgKRAGrlXVCECya2bqHozZHSLCtV9wxqw8n9BL62tH7cOqLXWMHzGQz7bWMW/1dn7w53/FHXP0uGG8t3JbXNra7Q385/PJ5/k66p7XWfyjM5m9eCM3ea517sEjuf/iQ1m1pY765ggXPvxP/v3M/Vv3/89rn1AYCvLJplq+fvQ+ad2z6T17DSrkM3cuOL+Q/jBNQkVFhVZWVmY7G8YktXlnI1PueZ0LDhvFLeceSElhiL98tI47X1xKrbtWSqrtLrvrg1tOZdiAfJZu2Mm/zVzAyCGF7D24iLu+PIm8QIC65jDTn13I14/ah+WbahlUGOKcg0e2rhOT6C8frWPO0k08dOnhGctzf3PwHbP5SsVoHn/HaU9bcc85BBP7svcwEZmvqhVdHmeBxBj/mvbIexw2Zgg/OGN/Vm+tY8uuZp764HMurhjNnKWbKAgFmPnB563r2A8ZEOKAvUralWgy6aC9B3FxxWjue3UZlx1TzoD8IA/M+QSACw4fxcjBhQREiESViytGUz68mI01jYwoKWDt9gZmvLiULx46kn32KE5r7jJV5bOt9ZQPL97tazS2RGhsiTBkQP5uX2N3fPT5dkLBAJNGDe7wmEl3zOZiTyCpvPW0jI8/skDiYYHE9Dc1DS088d5nnH/o3vzm7ysozs9jythh/ObvK/l0Uy0KfPPYsWyubeSfK7ayemtdr7WZBAPS4TiIUFAYVBiisSXCwMI8IlFlW10z++1ZQk1DCxtqGpk6eW8Wrq1h39JiRISSgjwGFYX4c+Ua6pojjBtezJHlw5jz8SbOmLgn2+ubKQwFiUSVfUsH0hyJkhcQyoYWsW57Ax9vrKWkMI8xwwa0dna480uT2GfYAAIibK1rojAUZFBhiFDQKQF8trWeMXsMoCgUpL45woD8IIWhAE3hKEWhIDUNLYwrHUh+MMDOxhZ2NYXZVNNIQSjIoWWDaWiJEI4o+XkBWiJRJs+YA8CyO8+Ka8fymnTHbL565GjGjxjIzc8t4oVrj+OgvQcR+1EGA9LjJRQLJB4WSIzpmqq2W42yJRKlORxlwZodHDZmCOGo0tgSoa4pwuJ1NYwoKWBDTSN7DiqkpqGFvIBQXJDHpp2NbKhp5ICRJazeUseyDbUMGRBi5ZY6QkFhZ0OYYcVOx4HigiAnTijl8231RKLKkAEhikJ5fL6tjl1NEUJBYeuuZta5MzqLOB0F8oMBmpPMp14UCtLQ0jbDwID8YNyMA35XnB8koko4ooi4q4TirN757RPGcs7BI/nyr/7Z7ry8gFCQF0CJnw7ow9tO7zA4dSXVQJL1cSTGGH9ItqRxKBggFAxwnGesyqDCEJTA2FSrkPbv+pBUNbZEKMgLsLMxzKDCPGoaWhgyIJ+G5kjr/GjhqBIUobYxTH5egKL8INGosrWu2enyrLCjoQWB1tJEQV4QRVm+sZaBbgmnrilMOKoMKgyxta4JBQIitISjfLp5F3sNLqAolEcwIKzbXk8gIAwuChGOKFt2NbG5tolBhSEKQwHy8wLUN0coCgXZXt9MSWEeARHqmiIUhAJsr2umKD9IOKLsanLyneeWLqLaNl7pKxWj2be0mB+dfxCrttQxqDCPhpYIqrSWTEScaYBEhGhUW6+TSVYiMcYYk1SqJRKbRt4YY0xaLJAYY4xJiwUSY4wxabFAYowxJi0WSIwxxqTFAokxxpi0WCAxxhiTFgskxhhj0tIvBiSKSDXwWZcHJjcc2NLlUX2L3XP/0N/uub/dL6R/z/uoamlXB/WLQJIOEalMZWRnX2L33D/0t3vub/cLvXfPVrVljDEmLRZIjDHGpMUCSdceyXYGssDuuX/ob/fc3+4XeumerY3EGGNMWqxEYowxJi0WSIwxxqTFAkkHROQsEVkuIlUiMj3b+ekpIjJaRN4UkaUiskREbnDTh4nIHBH51P13qJsuIvKg+3NYKCKHZ/cOdp+IBEXkIxF50d0eKyLvu/f2fyKS76YXuNtV7v7ybOZ7d4nIEBF5RkSWicjHInJMX3/OIvJ99/d6sYg8JSKFfe05i8jjIrJZRBZ70rr9XEXkcvf4T0Xk8nTyZIEkCREJAg8BZwMTgWkiMjG7ueoxYeAmVZ0IHA1c697bdOB1VZ0AvO5ug/MzmOC+rgIe7v0s95gbgI892/cBP1PV8cB24Eo3/Upgu5v+M/e4XPRz4FVVPQA4FOfe++xzFpFRwPVAhapOAoLAJfS95/x74KyEtG49VxEZBtwBHAVMAe6IBZ/doqr2SngBxwCzPds3AzdnO18ZutcXgNOB5cBIN20ksNx9/xtgmuf41uNy6QWUuf/BTgFeBARnxG9e4jMHZgPHuO/z3OMk2/fQzfsdDKxKzHdffs7AKGANMMx9bi8CZ/bF5wyUA4t397kC04DfeNLjjuvuy0okycV+IWPWuml9iluUPwx4H9hTVTe4uzYCe7rv+8rP4n+AHwJRd3sPYIeqht1t73213rO7v8Y9PpeMBaqB37nVeb8VkWL68HNW1XXAT4HPgQ04z20+ffs5x3T3ufbo87ZA0k+JyEDgWeDfVHWnd586f6L0mX7hInIesFlV52c7L70oDzgceFhVDwPqaKvuAPrkcx4KTMUJonsDxbSvAurzsvFcLZAktw4Y7dkuc9P6BBEJ4QSRP6nqc27yJhEZ6e4fCWx20/vCz+I44HwRWQ3MxKne+jkwRETy3GO899V6z+7+wcDW3sxwD1gLrFXV993tZ3ACS19+zqcBq1S1WlVbgOdwnn1ffs4x3X2uPfq8LZAkNw+Y4Pb2yMdpsJuV5Tz1CBER4DHgY1V9wLNrFhDruXE5TttJLP0yt/fH0UCNpwidE1T1ZlUtU9VynGf5hqpeCrwJXOQelnjPsZ/FRe7xOfWXu6puBNaIyP5u0qnAUvrwc8ap0jpaRAa4v+exe+6zz9mju891NnCGiAx1S3JnuGm7J9uNRn59AecAnwArgFuynZ8evK/jcYq9C4EF7uscnLrh14FPgdeAYe7xgtODbQWwCKdHTNbvI437Pxl40X0/DvgAqAL+DBS46YXudpW7f1y2872b9zoZqHSf9V+AoX39OQM/ApYBi4E/AgV97TkDT+G0AbXglDyv3J3nCnzLvfcq4Jvp5MmmSDHGGJMWq9oyxhiTFgskxhhj0mKBxBhjTFoskBhjjEmLBRJjjDFpsUBiDCAit7izxi4UkQUiclSGP2+uiFR04/grRGRvz/Zv+9BEoibH5XV9iDF9m4gcA5wHHK6qTSIyHMjPcrYSXYEzNmI9gKr+v6zmxhgPK5EY48yGukVVmwBUdYuqrgcQkdtFZJ67vsUj7ojpWIniZyJS6a71caSIPOeu7XCXe0y5OGuB/Mk95hkRGZD44SJyhoi8KyIfisif3XnQvPsvAiqAP7mlpSJviUZEdonIf7slqtdEZIq7f6WInO8eE3SPmeeWur6TwZ+n6WcskBgDfwNGi8gnIvIrETnJs++XqnqkOutbFOGUXGKaVbUC+DXOlBTXApOAK0QkNovs/sCvVPVAYCdwjfeD3dLPrcBpqno4zkj0G73HqOozbvqlqjpZVRsS8l+MM73HQUAtcBfO0gBfBma4x1yJMz3GkcCRwLdFZGw3fkbGdMgCien3VHUXcATOwj/VwP+JyBXu7i+Is3reIpzJHg/ynBqbf20RsERVN7ilmpW0TYi3RlXfcd8/gTNFjdfROIunvSMiC3DmSdqnm7fQDLzqyctb6kxauAhn3Qpw5lK6zP2M93Gm1JjQzc8xJilrIzEGUNUIMBeY6waNy0VkJvArnPmJ1ojIf+HMzxTT5P4b9byPbcf+byXOQZS4LcAcVZ2WRvZbtG2uo9a8qGrUM+utAN9T1d2fmM+YDliJxPR7IrK/iHj/Op8MfEZb0Njitltc1O7kro1xG/MBvga8nbD/PeA4ERnv5qVYRPZLcp1aoGQ3Pj9mNvBddwkBRGQ/d6ErY9JmJRJjYCDwCxEZgrOmfRVwlaruEJFHcXpLbcRZXqC7lgPXisjjOFOax62FrqrVbjXaUyJS4CbfijPztNfvgV+LSAPOcrHd9Vucaq4P3Q4D1cCXduM6xrRjs/8akyHiLGX8ottQb0yfZVVbxhhj0mIlEmOMMWmxEokxxpi0WCAxxhiTFgskxhhj0mKBxBhjTFoskBhjjEnL/weywdCww7PLYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Trend')\n",
    "\n",
    "plt.plot(losses4)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Loss')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('Train Accuracy')\n",
    "\n",
    "plt.plot(accuracies4)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done testing.\n"
     ]
    }
   ],
   "source": [
    "test_accuracies4 = test(model4, config, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palindromes Length: T = 20\n",
      "Average accuracy over 2000 sampled test: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.plot(test_accuracies3)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(\"Palindromes Length: T = 20\")\n",
    "print(\"Average accuracy over 2000 sampled test: \" + str(np.mean(test_accuracies4)) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T = 25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__:37: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:     0] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:    10] loss: 0.4609\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:    20] loss: 0.4606\n",
      "Accuracy on training dataset: 16.406 %\n",
      "[step:    30] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:    40] loss: 0.4608\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:    50] loss: 0.4604\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:    60] loss: 0.4608\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:    70] loss: 0.4606\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:    80] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:    90] loss: 0.4606\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:   100] loss: 0.4604\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:   110] loss: 0.4603\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   120] loss: 0.4608\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:   130] loss: 0.4603\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:   140] loss: 0.4604\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   150] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   160] loss: 0.4603\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   170] loss: 0.4601\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   180] loss: 0.4601\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:   190] loss: 0.4602\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:   200] loss: 0.4604\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   210] loss: 0.4610\n",
      "Accuracy on training dataset: 3.125 %\n",
      "[step:   220] loss: 0.4605\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   230] loss: 0.4606\n",
      "Accuracy on training dataset: 4.688 %\n",
      "[step:   240] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   250] loss: 0.4605\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   260] loss: 0.4601\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   270] loss: 0.4604\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   280] loss: 0.4605\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   290] loss: 0.4606\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   300] loss: 0.4604\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:   310] loss: 0.4604\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   320] loss: 0.4605\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   330] loss: 0.4603\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   340] loss: 0.4606\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   350] loss: 0.4607\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   360] loss: 0.4607\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   370] loss: 0.4606\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:   380] loss: 0.4603\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:   390] loss: 0.4607\n",
      "Accuracy on training dataset: 4.688 %\n",
      "[step:   400] loss: 0.4604\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   410] loss: 0.4605\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   420] loss: 0.4603\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   430] loss: 0.4608\n",
      "Accuracy on training dataset: 4.688 %\n",
      "[step:   440] loss: 0.4608\n",
      "Accuracy on training dataset: 4.688 %\n",
      "[step:   450] loss: 0.4604\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   460] loss: 0.4607\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:   470] loss: 0.4603\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:   480] loss: 0.4609\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:   490] loss: 0.4606\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   500] loss: 0.4607\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   510] loss: 0.4604\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   520] loss: 0.4601\n",
      "Accuracy on training dataset: 17.188 %\n",
      "[step:   530] loss: 0.4605\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   540] loss: 0.4607\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   550] loss: 0.4608\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:   560] loss: 0.4603\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:   570] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   580] loss: 0.4603\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   590] loss: 0.4607\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   600] loss: 0.4605\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   610] loss: 0.4605\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   620] loss: 0.4606\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   630] loss: 0.4608\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   640] loss: 0.4604\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:   650] loss: 0.4603\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:   660] loss: 0.4605\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:   670] loss: 0.4604\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   680] loss: 0.4604\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:   690] loss: 0.4606\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   700] loss: 0.4602\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   710] loss: 0.4604\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   720] loss: 0.4606\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:   730] loss: 0.4600\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   740] loss: 0.4603\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   750] loss: 0.4608\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   760] loss: 0.4608\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   770] loss: 0.4607\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   780] loss: 0.4605\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   790] loss: 0.4604\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   800] loss: 0.4607\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   810] loss: 0.4603\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:   820] loss: 0.4609\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   830] loss: 0.4608\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:   840] loss: 0.4604\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   850] loss: 0.4606\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   860] loss: 0.4609\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:   870] loss: 0.4606\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:   880] loss: 0.4608\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:   890] loss: 0.4605\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   900] loss: 0.4605\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:   910] loss: 0.4605\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   920] loss: 0.4609\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:   930] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   940] loss: 0.4604\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:   950] loss: 0.4604\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:   960] loss: 0.4607\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:   970] loss: 0.4602\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:   980] loss: 0.4607\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:   990] loss: 0.4605\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1000] loss: 0.4607\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1010] loss: 0.4603\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  1020] loss: 0.4606\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  1030] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  1040] loss: 0.4605\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1050] loss: 0.4606\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1060] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1070] loss: 0.4605\n",
      "Accuracy on training dataset: 4.688 %\n",
      "[step:  1080] loss: 0.4605\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  1090] loss: 0.4606\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1100] loss: 0.4607\n",
      "Accuracy on training dataset: 4.688 %\n",
      "[step:  1110] loss: 0.4604\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1120] loss: 0.4606\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  1130] loss: 0.4606\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1140] loss: 0.4605\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:  1150] loss: 0.4606\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1160] loss: 0.4605\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  1170] loss: 0.4605\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1180] loss: 0.4606\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:  1190] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1200] loss: 0.4607\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  1210] loss: 0.4606\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  1220] loss: 0.4605\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1230] loss: 0.4605\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1240] loss: 0.4604\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1250] loss: 0.4606\n",
      "Accuracy on training dataset: 7.812 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  1260] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1270] loss: 0.4606\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1280] loss: 0.4603\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1290] loss: 0.4606\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  1300] loss: 0.4605\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1310] loss: 0.4605\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1320] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1330] loss: 0.4603\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  1340] loss: 0.4605\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1350] loss: 0.4607\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:  1360] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1370] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1380] loss: 0.4604\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  1390] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1400] loss: 0.4605\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  1410] loss: 0.4605\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1420] loss: 0.4604\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  1430] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1440] loss: 0.4608\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1450] loss: 0.4606\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1460] loss: 0.4606\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1470] loss: 0.4603\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  1480] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1490] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1500] loss: 0.4606\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1510] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  1520] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1530] loss: 0.4605\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1540] loss: 0.4606\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  1550] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1560] loss: 0.4605\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  1570] loss: 0.4604\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1580] loss: 0.4609\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1590] loss: 0.4602\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1600] loss: 0.4607\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1610] loss: 0.4601\n",
      "Accuracy on training dataset: 17.188 %\n",
      "[step:  1620] loss: 0.4606\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1630] loss: 0.4600\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:  1640] loss: 0.4603\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1650] loss: 0.4601\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  1660] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1670] loss: 0.4609\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:  1680] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1690] loss: 0.4603\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1700] loss: 0.4604\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1710] loss: 0.4604\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1720] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  1730] loss: 0.4602\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  1740] loss: 0.4607\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1750] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1760] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  1770] loss: 0.4603\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  1780] loss: 0.4609\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:  1790] loss: 0.4607\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1800] loss: 0.4609\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:  1810] loss: 0.4610\n",
      "Accuracy on training dataset: 5.469 %\n",
      "[step:  1820] loss: 0.4602\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1830] loss: 0.4602\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  1840] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1850] loss: 0.4603\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:  1860] loss: 0.4602\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1870] loss: 0.4603\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  1880] loss: 0.4598\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1890] loss: 0.4602\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1900] loss: 0.4598\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  1910] loss: 0.4571\n",
      "Accuracy on training dataset: 19.531 %\n",
      "[step:  1920] loss: 0.4541\n",
      "Accuracy on training dataset: 21.875 %\n",
      "[step:  1930] loss: 0.4574\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  1940] loss: 0.4599\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  1950] loss: 0.4522\n",
      "Accuracy on training dataset: 17.188 %\n",
      "[step:  1960] loss: 0.4640\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  1970] loss: 0.4592\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  1980] loss: 0.4637\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  1990] loss: 0.4613\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2000] loss: 0.4610\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2010] loss: 0.4588\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2020] loss: 0.4595\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  2030] loss: 0.4645\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2040] loss: 0.4583\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2050] loss: 0.4621\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2060] loss: 0.4609\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2070] loss: 0.4610\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2080] loss: 0.4606\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2090] loss: 0.4599\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2100] loss: 0.4608\n",
      "Accuracy on training dataset: 3.906 %\n",
      "[step:  2110] loss: 0.4596\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:  2120] loss: 0.4607\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2130] loss: 0.4618\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  2140] loss: 0.4627\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2150] loss: 0.4610\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  2160] loss: 0.4614\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2170] loss: 0.4604\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2180] loss: 0.4612\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2190] loss: 0.4598\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  2200] loss: 0.4606\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2210] loss: 0.4603\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2220] loss: 0.4611\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  2230] loss: 0.4594\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:  2240] loss: 0.4604\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2250] loss: 0.4593\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2260] loss: 0.4605\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2270] loss: 0.4595\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2280] loss: 0.4598\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  2290] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2300] loss: 0.4612\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2310] loss: 0.4623\n",
      "Accuracy on training dataset: 2.344 %\n",
      "[step:  2320] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  2330] loss: 0.4612\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2340] loss: 0.4606\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2350] loss: 0.4602\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2360] loss: 0.4607\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2370] loss: 0.4611\n",
      "Accuracy on training dataset: 3.906 %\n",
      "[step:  2380] loss: 0.4616\n",
      "Accuracy on training dataset: 3.125 %\n",
      "[step:  2390] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2400] loss: 0.4607\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2410] loss: 0.4610\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  2420] loss: 0.4609\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2430] loss: 0.4609\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2440] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2450] loss: 0.4612\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2460] loss: 0.4610\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2470] loss: 0.4602\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2480] loss: 0.4603\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  2490] loss: 0.4609\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  2500] loss: 0.4602\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2510] loss: 0.4610\n",
      "Accuracy on training dataset: 9.375 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  2520] loss: 0.4602\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:  2530] loss: 0.4609\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2540] loss: 0.4605\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2550] loss: 0.4602\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2560] loss: 0.4606\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2570] loss: 0.4603\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  2580] loss: 0.4602\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2590] loss: 0.4610\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2600] loss: 0.4608\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2610] loss: 0.4607\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2620] loss: 0.4606\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2630] loss: 0.4602\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:  2640] loss: 0.4604\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2650] loss: 0.4603\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2660] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2670] loss: 0.4604\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  2680] loss: 0.4602\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  2690] loss: 0.4602\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2700] loss: 0.4600\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2710] loss: 0.4602\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2720] loss: 0.4605\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2730] loss: 0.4606\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2740] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2750] loss: 0.4603\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  2760] loss: 0.4607\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2770] loss: 0.4604\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  2780] loss: 0.4605\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:  2790] loss: 0.4604\n",
      "Accuracy on training dataset: 8.594 %\n",
      "[step:  2800] loss: 0.4606\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2810] loss: 0.4603\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  2820] loss: 0.4606\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  2830] loss: 0.4605\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  2840] loss: 0.4604\n",
      "Accuracy on training dataset: 11.719 %\n",
      "[step:  2850] loss: 0.4606\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  2860] loss: 0.4604\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:  2870] loss: 0.4607\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  2880] loss: 0.4609\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2890] loss: 0.4606\n",
      "Accuracy on training dataset: 13.281 %\n",
      "[step:  2900] loss: 0.4604\n",
      "Accuracy on training dataset: 9.375 %\n",
      "[step:  2910] loss: 0.4608\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2920] loss: 0.4606\n",
      "Accuracy on training dataset: 10.938 %\n",
      "[step:  2930] loss: 0.4607\n",
      "Accuracy on training dataset: 7.812 %\n",
      "[step:  2940] loss: 0.4604\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:  2950] loss: 0.4606\n",
      "Accuracy on training dataset: 6.250 %\n",
      "[step:  2960] loss: 0.4605\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2970] loss: 0.4606\n",
      "Accuracy on training dataset: 7.031 %\n",
      "[step:  2980] loss: 0.4604\n",
      "Accuracy on training dataset: 10.156 %\n",
      "[step:  2990] loss: 0.4605\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:  3000] loss: 0.4600\n",
      "Accuracy on training dataset: 14.844 %\n",
      "[step:  3010] loss: 0.4602\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  3020] loss: 0.4595\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  3030] loss: 0.4600\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:  3040] loss: 0.4590\n",
      "Accuracy on training dataset: 17.969 %\n",
      "[step:  3050] loss: 0.4589\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  3060] loss: 0.4572\n",
      "Accuracy on training dataset: 25.000 %\n",
      "[step:  3070] loss: 0.4552\n",
      "Accuracy on training dataset: 20.312 %\n",
      "[step:  3080] loss: 0.4555\n",
      "Accuracy on training dataset: 12.500 %\n",
      "[step:  3090] loss: 0.4491\n",
      "Accuracy on training dataset: 24.219 %\n",
      "[step:  3100] loss: 0.4495\n",
      "Accuracy on training dataset: 17.188 %\n",
      "[step:  3110] loss: 0.4497\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:  3120] loss: 0.4514\n",
      "Accuracy on training dataset: 15.625 %\n",
      "[step:  3130] loss: 0.4495\n",
      "Accuracy on training dataset: 16.406 %\n",
      "[step:  3140] loss: 0.4470\n",
      "Accuracy on training dataset: 18.750 %\n",
      "[step:  3150] loss: 0.4474\n",
      "Accuracy on training dataset: 14.062 %\n",
      "[step:  3160] loss: 0.4466\n",
      "Accuracy on training dataset: 17.188 %\n",
      "[step:  3170] loss: 0.4419\n",
      "Accuracy on training dataset: 25.000 %\n",
      "[step:  3180] loss: 0.4369\n",
      "Accuracy on training dataset: 25.000 %\n",
      "[step:  3190] loss: 0.4419\n",
      "Accuracy on training dataset: 17.969 %\n",
      "[step:  3200] loss: 0.4429\n",
      "Accuracy on training dataset: 25.781 %\n",
      "[step:  3210] loss: 0.4361\n",
      "Accuracy on training dataset: 25.000 %\n",
      "[step:  3220] loss: 0.4348\n",
      "Accuracy on training dataset: 32.812 %\n",
      "[step:  3230] loss: 0.4402\n",
      "Accuracy on training dataset: 20.312 %\n",
      "[step:  3240] loss: 0.4305\n",
      "Accuracy on training dataset: 32.031 %\n",
      "[step:  3250] loss: 0.4280\n",
      "Accuracy on training dataset: 36.719 %\n",
      "[step:  3260] loss: 0.4254\n",
      "Accuracy on training dataset: 39.062 %\n",
      "[step:  3270] loss: 0.4314\n",
      "Accuracy on training dataset: 31.250 %\n",
      "[step:  3280] loss: 0.4306\n",
      "Accuracy on training dataset: 34.375 %\n",
      "[step:  3290] loss: 0.4355\n",
      "Accuracy on training dataset: 28.906 %\n",
      "[step:  3300] loss: 0.4310\n",
      "Accuracy on training dataset: 31.250 %\n",
      "[step:  3310] loss: 0.4250\n",
      "Accuracy on training dataset: 36.719 %\n",
      "[step:  3320] loss: 0.4139\n",
      "Accuracy on training dataset: 43.750 %\n",
      "[step:  3330] loss: 0.4075\n",
      "Accuracy on training dataset: 49.219 %\n",
      "[step:  3340] loss: 0.4138\n",
      "Accuracy on training dataset: 44.531 %\n",
      "[step:  3350] loss: 0.4223\n",
      "Accuracy on training dataset: 34.375 %\n",
      "[step:  3360] loss: 0.4213\n",
      "Accuracy on training dataset: 32.031 %\n",
      "[step:  3370] loss: 0.4291\n",
      "Accuracy on training dataset: 34.375 %\n",
      "[step:  3380] loss: 0.4227\n",
      "Accuracy on training dataset: 32.031 %\n",
      "[step:  3390] loss: 0.4127\n",
      "Accuracy on training dataset: 43.750 %\n",
      "[step:  3400] loss: 0.4146\n",
      "Accuracy on training dataset: 45.312 %\n",
      "[step:  3410] loss: 0.4021\n",
      "Accuracy on training dataset: 51.562 %\n",
      "[step:  3420] loss: 0.4007\n",
      "Accuracy on training dataset: 53.125 %\n",
      "[step:  3430] loss: 0.4017\n",
      "Accuracy on training dataset: 47.656 %\n",
      "[step:  3440] loss: 0.4029\n",
      "Accuracy on training dataset: 48.438 %\n",
      "[step:  3450] loss: 0.4014\n",
      "Accuracy on training dataset: 52.344 %\n",
      "[step:  3460] loss: 0.3975\n",
      "Accuracy on training dataset: 55.469 %\n",
      "[step:  3470] loss: 0.3992\n",
      "Accuracy on training dataset: 46.875 %\n",
      "[step:  3480] loss: 0.4081\n",
      "Accuracy on training dataset: 45.312 %\n",
      "[step:  3490] loss: 0.4006\n",
      "Accuracy on training dataset: 48.438 %\n",
      "[step:  3500] loss: 0.3997\n",
      "Accuracy on training dataset: 52.344 %\n",
      "[step:  3510] loss: 0.3918\n",
      "Accuracy on training dataset: 53.125 %\n",
      "[step:  3520] loss: 0.3833\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:  3530] loss: 0.3731\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  3540] loss: 0.3791\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  3550] loss: 0.3859\n",
      "Accuracy on training dataset: 57.812 %\n",
      "[step:  3560] loss: 0.3879\n",
      "Accuracy on training dataset: 57.812 %\n",
      "[step:  3570] loss: 0.4009\n",
      "Accuracy on training dataset: 47.656 %\n",
      "[step:  3580] loss: 0.3906\n",
      "Accuracy on training dataset: 58.594 %\n",
      "[step:  3590] loss: 0.3838\n",
      "Accuracy on training dataset: 57.031 %\n",
      "[step:  3600] loss: 0.3778\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  3610] loss: 0.3791\n",
      "Accuracy on training dataset: 57.812 %\n",
      "[step:  3620] loss: 0.3778\n",
      "Accuracy on training dataset: 61.719 %\n",
      "[step:  3630] loss: 0.3669\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  3640] loss: 0.3776\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  3650] loss: 0.3795\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  3660] loss: 0.3633\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  3670] loss: 0.3752\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  3680] loss: 0.3824\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:  3690] loss: 0.3794\n",
      "Accuracy on training dataset: 61.719 %\n",
      "[step:  3700] loss: 0.3743\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:  3710] loss: 0.3722\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  3720] loss: 0.3922\n",
      "Accuracy on training dataset: 50.781 %\n",
      "[step:  3730] loss: 0.3672\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  3740] loss: 0.3789\n",
      "Accuracy on training dataset: 57.812 %\n",
      "[step:  3750] loss: 0.3657\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  3760] loss: 0.3749\n",
      "Accuracy on training dataset: 64.062 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  3770] loss: 0.3900\n",
      "Accuracy on training dataset: 54.688 %\n",
      "[step:  3780] loss: 0.3775\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  3790] loss: 0.3805\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:  3800] loss: 0.3941\n",
      "Accuracy on training dataset: 50.000 %\n",
      "[step:  3810] loss: 0.3622\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  3820] loss: 0.3828\n",
      "Accuracy on training dataset: 50.781 %\n",
      "[step:  3830] loss: 0.3681\n",
      "Accuracy on training dataset: 60.938 %\n",
      "[step:  3840] loss: 0.3743\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:  3850] loss: 0.3825\n",
      "Accuracy on training dataset: 53.906 %\n",
      "[step:  3860] loss: 0.3720\n",
      "Accuracy on training dataset: 61.719 %\n",
      "[step:  3870] loss: 0.3760\n",
      "Accuracy on training dataset: 58.594 %\n",
      "[step:  3880] loss: 0.3768\n",
      "Accuracy on training dataset: 62.500 %\n",
      "[step:  3890] loss: 0.3722\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  3900] loss: 0.3659\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  3910] loss: 0.3605\n",
      "Accuracy on training dataset: 64.844 %\n",
      "[step:  3920] loss: 0.3699\n",
      "Accuracy on training dataset: 62.500 %\n",
      "[step:  3930] loss: 0.3610\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  3940] loss: 0.3901\n",
      "Accuracy on training dataset: 49.219 %\n",
      "[step:  3950] loss: 0.3756\n",
      "Accuracy on training dataset: 56.250 %\n",
      "[step:  3960] loss: 0.3637\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  3970] loss: 0.3631\n",
      "Accuracy on training dataset: 58.594 %\n",
      "[step:  3980] loss: 0.3714\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:  3990] loss: 0.3668\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  4000] loss: 0.3700\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  4010] loss: 0.3724\n",
      "Accuracy on training dataset: 59.375 %\n",
      "[step:  4020] loss: 0.3601\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  4030] loss: 0.3729\n",
      "Accuracy on training dataset: 56.250 %\n",
      "[step:  4040] loss: 0.3631\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4050] loss: 0.3725\n",
      "Accuracy on training dataset: 59.375 %\n",
      "[step:  4060] loss: 0.3774\n",
      "Accuracy on training dataset: 56.250 %\n",
      "[step:  4070] loss: 0.3704\n",
      "Accuracy on training dataset: 61.719 %\n",
      "[step:  4080] loss: 0.3674\n",
      "Accuracy on training dataset: 62.500 %\n",
      "[step:  4090] loss: 0.3487\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  4100] loss: 0.3567\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4110] loss: 0.3665\n",
      "Accuracy on training dataset: 64.844 %\n",
      "[step:  4120] loss: 0.3734\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  4130] loss: 0.3713\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  4140] loss: 0.3587\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:  4150] loss: 0.3508\n",
      "Accuracy on training dataset: 72.656 %\n",
      "[step:  4160] loss: 0.3683\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4170] loss: 0.3675\n",
      "Accuracy on training dataset: 64.844 %\n",
      "[step:  4180] loss: 0.3590\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  4190] loss: 0.3633\n",
      "Accuracy on training dataset: 64.844 %\n",
      "[step:  4200] loss: 0.3673\n",
      "Accuracy on training dataset: 61.719 %\n",
      "[step:  4210] loss: 0.3716\n",
      "Accuracy on training dataset: 62.500 %\n",
      "[step:  4220] loss: 0.3705\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4230] loss: 0.3683\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  4240] loss: 0.3716\n",
      "Accuracy on training dataset: 60.938 %\n",
      "[step:  4250] loss: 0.3655\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  4260] loss: 0.3647\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4270] loss: 0.3729\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:  4280] loss: 0.3526\n",
      "Accuracy on training dataset: 71.875 %\n",
      "[step:  4290] loss: 0.3612\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4300] loss: 0.3672\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  4310] loss: 0.3529\n",
      "Accuracy on training dataset: 71.094 %\n",
      "[step:  4320] loss: 0.3622\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4330] loss: 0.3668\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4340] loss: 0.3676\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4350] loss: 0.3604\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  4360] loss: 0.3647\n",
      "Accuracy on training dataset: 62.500 %\n",
      "[step:  4370] loss: 0.3666\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  4380] loss: 0.3669\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4390] loss: 0.3518\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  4400] loss: 0.3559\n",
      "Accuracy on training dataset: 71.094 %\n",
      "[step:  4410] loss: 0.3727\n",
      "Accuracy on training dataset: 62.500 %\n",
      "[step:  4420] loss: 0.3586\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:  4430] loss: 0.3579\n",
      "Accuracy on training dataset: 71.875 %\n",
      "[step:  4440] loss: 0.3541\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:  4450] loss: 0.3573\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:  4460] loss: 0.3711\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4470] loss: 0.3748\n",
      "Accuracy on training dataset: 58.594 %\n",
      "[step:  4480] loss: 0.3559\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:  4490] loss: 0.3706\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4500] loss: 0.3593\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  4510] loss: 0.3593\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4520] loss: 0.3611\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  4530] loss: 0.3575\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  4540] loss: 0.3590\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  4550] loss: 0.3550\n",
      "Accuracy on training dataset: 71.875 %\n",
      "[step:  4560] loss: 0.3561\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4570] loss: 0.3678\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4580] loss: 0.3684\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  4590] loss: 0.3731\n",
      "Accuracy on training dataset: 59.375 %\n",
      "[step:  4600] loss: 0.3580\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4610] loss: 0.3590\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  4620] loss: 0.3452\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  4630] loss: 0.3667\n",
      "Accuracy on training dataset: 60.938 %\n",
      "[step:  4640] loss: 0.3574\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4650] loss: 0.3657\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  4660] loss: 0.3548\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4670] loss: 0.3689\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  4680] loss: 0.3603\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  4690] loss: 0.3737\n",
      "Accuracy on training dataset: 60.938 %\n",
      "[step:  4700] loss: 0.3653\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  4710] loss: 0.3657\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  4720] loss: 0.3596\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  4730] loss: 0.3636\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  4740] loss: 0.3761\n",
      "Accuracy on training dataset: 57.031 %\n",
      "[step:  4750] loss: 0.3634\n",
      "Accuracy on training dataset: 64.844 %\n",
      "[step:  4760] loss: 0.3464\n",
      "Accuracy on training dataset: 73.438 %\n",
      "[step:  4770] loss: 0.3549\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  4780] loss: 0.3627\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  4790] loss: 0.3508\n",
      "Accuracy on training dataset: 71.875 %\n",
      "[step:  4800] loss: 0.3511\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  4810] loss: 0.3601\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  4820] loss: 0.3432\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:  4830] loss: 0.3553\n",
      "Accuracy on training dataset: 67.969 %\n",
      "[step:  4840] loss: 0.3569\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  4850] loss: 0.3563\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:  4860] loss: 0.3726\n",
      "Accuracy on training dataset: 57.812 %\n",
      "[step:  4870] loss: 0.3513\n",
      "Accuracy on training dataset: 71.875 %\n",
      "[step:  4880] loss: 0.3589\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  4890] loss: 0.3616\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  4900] loss: 0.3544\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:  4910] loss: 0.3569\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  4920] loss: 0.3534\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:  4930] loss: 0.3437\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  4940] loss: 0.3646\n",
      "Accuracy on training dataset: 64.844 %\n",
      "[step:  4950] loss: 0.3662\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  4960] loss: 0.3555\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:  4970] loss: 0.3623\n",
      "Accuracy on training dataset: 67.188 %\n",
      "[step:  4980] loss: 0.3729\n",
      "Accuracy on training dataset: 57.812 %\n",
      "[step:  4990] loss: 0.3608\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  5000] loss: 0.3721\n",
      "Accuracy on training dataset: 60.156 %\n",
      "[step:  5010] loss: 0.3741\n",
      "Accuracy on training dataset: 59.375 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  5020] loss: 0.4353\n",
      "Accuracy on training dataset: 22.656 %\n",
      "[step:  5030] loss: 0.3747\n",
      "Accuracy on training dataset: 60.938 %\n",
      "[step:  5040] loss: 0.3601\n",
      "Accuracy on training dataset: 64.062 %\n",
      "[step:  5050] loss: 0.3650\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  5060] loss: 0.3564\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:  5070] loss: 0.3536\n",
      "Accuracy on training dataset: 68.750 %\n",
      "[step:  5080] loss: 0.3556\n",
      "Accuracy on training dataset: 65.625 %\n",
      "[step:  5090] loss: 0.3548\n",
      "Accuracy on training dataset: 66.406 %\n",
      "[step:  5100] loss: 0.3567\n",
      "Accuracy on training dataset: 69.531 %\n",
      "[step:  5110] loss: 0.3502\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:  5120] loss: 0.3536\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:  5130] loss: 0.3638\n",
      "Accuracy on training dataset: 62.500 %\n",
      "[step:  5140] loss: 0.3482\n",
      "Accuracy on training dataset: 72.656 %\n",
      "[step:  5150] loss: 0.3601\n",
      "Accuracy on training dataset: 63.281 %\n",
      "[step:  5160] loss: 0.3666\n",
      "Accuracy on training dataset: 58.594 %\n",
      "[step:  5170] loss: 0.3494\n",
      "Accuracy on training dataset: 70.312 %\n",
      "[step:  5180] loss: 0.3460\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:  5190] loss: 0.3468\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:  5200] loss: 0.3554\n",
      "Accuracy on training dataset: 75.000 %\n",
      "[step:  5210] loss: 0.3520\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  5220] loss: 0.3500\n",
      "Accuracy on training dataset: 74.219 %\n",
      "[step:  5230] loss: 0.3415\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  5240] loss: 0.3415\n",
      "Accuracy on training dataset: 75.781 %\n",
      "[step:  5250] loss: 0.3538\n",
      "Accuracy on training dataset: 72.656 %\n",
      "[step:  5260] loss: 0.3547\n",
      "Accuracy on training dataset: 72.656 %\n",
      "[step:  5270] loss: 0.3416\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:  5280] loss: 0.3346\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  5290] loss: 0.3391\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:  5300] loss: 0.3423\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:  5310] loss: 0.3323\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  5320] loss: 0.3398\n",
      "Accuracy on training dataset: 78.125 %\n",
      "[step:  5330] loss: 0.3354\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  5340] loss: 0.3382\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:  5350] loss: 0.3411\n",
      "Accuracy on training dataset: 77.344 %\n",
      "[step:  5360] loss: 0.3314\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  5370] loss: 0.3376\n",
      "Accuracy on training dataset: 76.562 %\n",
      "[step:  5380] loss: 0.3350\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:  5390] loss: 0.3339\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:  5400] loss: 0.3320\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  5410] loss: 0.3365\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:  5420] loss: 0.3311\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  5430] loss: 0.3380\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  5440] loss: 0.3347\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  5450] loss: 0.3275\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  5460] loss: 0.3370\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:  5470] loss: 0.3415\n",
      "Accuracy on training dataset: 78.906 %\n",
      "[step:  5480] loss: 0.3358\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  5490] loss: 0.3319\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  5500] loss: 0.3314\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  5510] loss: 0.3263\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  5520] loss: 0.3362\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  5530] loss: 0.3221\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  5540] loss: 0.3339\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  5550] loss: 0.3331\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  5560] loss: 0.3437\n",
      "Accuracy on training dataset: 79.688 %\n",
      "[step:  5570] loss: 0.3355\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  5580] loss: 0.3265\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  5590] loss: 0.3258\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  5600] loss: 0.3355\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  5610] loss: 0.3348\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  5620] loss: 0.3365\n",
      "Accuracy on training dataset: 81.250 %\n",
      "[step:  5630] loss: 0.3243\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  5640] loss: 0.3259\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  5650] loss: 0.3297\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  5660] loss: 0.3324\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  5670] loss: 0.3255\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  5680] loss: 0.3232\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  5690] loss: 0.3288\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  5700] loss: 0.3315\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  5710] loss: 0.3319\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  5720] loss: 0.3252\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  5730] loss: 0.3130\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  5740] loss: 0.3201\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  5750] loss: 0.3210\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  5760] loss: 0.3338\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:  5770] loss: 0.3266\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  5780] loss: 0.3337\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:  5790] loss: 0.3300\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  5800] loss: 0.3277\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  5810] loss: 0.3199\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  5820] loss: 0.3260\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  5830] loss: 0.3214\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  5840] loss: 0.3293\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  5850] loss: 0.3201\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  5860] loss: 0.3270\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  5870] loss: 0.3222\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  5880] loss: 0.3240\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  5890] loss: 0.3276\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  5900] loss: 0.3253\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  5910] loss: 0.3237\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  5920] loss: 0.3252\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  5930] loss: 0.3217\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  5940] loss: 0.3186\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  5950] loss: 0.3247\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  5960] loss: 0.3247\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  5970] loss: 0.3284\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  5980] loss: 0.3193\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  5990] loss: 0.3125\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  6000] loss: 0.3263\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  6010] loss: 0.3225\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6020] loss: 0.3254\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  6030] loss: 0.3140\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  6040] loss: 0.3168\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  6050] loss: 0.3149\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  6060] loss: 0.3192\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6070] loss: 0.3197\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6080] loss: 0.3189\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6090] loss: 0.3248\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  6100] loss: 0.3164\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  6110] loss: 0.3224\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6120] loss: 0.3175\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6130] loss: 0.3193\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6140] loss: 0.3119\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  6150] loss: 0.3236\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6160] loss: 0.3279\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  6170] loss: 0.3142\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  6180] loss: 0.3187\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6190] loss: 0.3178\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6200] loss: 0.3194\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  6210] loss: 0.3225\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6220] loss: 0.3132\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  6230] loss: 0.3153\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  6240] loss: 0.3156\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6250] loss: 0.3187\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6260] loss: 0.3218\n",
      "Accuracy on training dataset: 87.500 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  6270] loss: 0.3165\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6280] loss: 0.3175\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6290] loss: 0.3169\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6300] loss: 0.3240\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  6310] loss: 0.3164\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6320] loss: 0.3267\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  6330] loss: 0.3166\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6340] loss: 0.3156\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6350] loss: 0.3268\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  6360] loss: 0.3163\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6370] loss: 0.3199\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6380] loss: 0.3251\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  6390] loss: 0.3144\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6400] loss: 0.3250\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  6410] loss: 0.3129\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  6420] loss: 0.3228\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6430] loss: 0.3143\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  6440] loss: 0.3218\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6450] loss: 0.3207\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6460] loss: 0.3161\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  6470] loss: 0.3186\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6480] loss: 0.3190\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6490] loss: 0.3147\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6500] loss: 0.3231\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  6510] loss: 0.3272\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  6520] loss: 0.3158\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6530] loss: 0.3211\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6540] loss: 0.3125\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  6550] loss: 0.3196\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6560] loss: 0.3160\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  6570] loss: 0.3183\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6580] loss: 0.3150\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6590] loss: 0.3207\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6600] loss: 0.3081\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  6610] loss: 0.3250\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  6620] loss: 0.3095\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  6630] loss: 0.3134\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  6640] loss: 0.3184\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  6650] loss: 0.3139\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  6660] loss: 0.3156\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6670] loss: 0.3193\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6680] loss: 0.3179\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6690] loss: 0.3189\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  6700] loss: 0.3113\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  6710] loss: 0.3266\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  6720] loss: 0.3111\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  6730] loss: 0.3050\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  6740] loss: 0.3119\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  6750] loss: 0.3186\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6760] loss: 0.3179\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6770] loss: 0.3199\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6780] loss: 0.3202\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6790] loss: 0.3114\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  6800] loss: 0.3144\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6810] loss: 0.3170\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  6820] loss: 0.3141\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  6830] loss: 0.3236\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  6840] loss: 0.3143\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  6850] loss: 0.3063\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  6860] loss: 0.3044\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  6870] loss: 0.3194\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6880] loss: 0.3192\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6890] loss: 0.3187\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  6900] loss: 0.3151\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6910] loss: 0.3139\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  6920] loss: 0.3054\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  6930] loss: 0.3110\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  6940] loss: 0.3259\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  6950] loss: 0.3122\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  6960] loss: 0.3197\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6970] loss: 0.3198\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  6980] loss: 0.3245\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  6990] loss: 0.3112\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7000] loss: 0.3071\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  7010] loss: 0.3189\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  7020] loss: 0.3147\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  7030] loss: 0.3192\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7040] loss: 0.3206\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7050] loss: 0.3129\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  7060] loss: 0.3176\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7070] loss: 0.3191\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7080] loss: 0.3228\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  7090] loss: 0.3101\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7100] loss: 0.3154\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7110] loss: 0.3149\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7120] loss: 0.3106\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7130] loss: 0.3205\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  7140] loss: 0.3100\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  7150] loss: 0.3199\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7160] loss: 0.3113\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7170] loss: 0.3202\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7180] loss: 0.3174\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7190] loss: 0.3193\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  7200] loss: 0.3208\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  7210] loss: 0.3201\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7220] loss: 0.3174\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  7230] loss: 0.3156\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7240] loss: 0.3163\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7250] loss: 0.3152\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7260] loss: 0.3300\n",
      "Accuracy on training dataset: 82.031 %\n",
      "[step:  7270] loss: 0.3215\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  7280] loss: 0.3092\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  7290] loss: 0.3164\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7300] loss: 0.3217\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  7310] loss: 0.3187\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7320] loss: 0.3169\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7330] loss: 0.3117\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  7340] loss: 0.3271\n",
      "Accuracy on training dataset: 82.812 %\n",
      "[step:  7350] loss: 0.3094\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  7360] loss: 0.3120\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  7370] loss: 0.3104\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7380] loss: 0.3149\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7390] loss: 0.3218\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  7400] loss: 0.3163\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7410] loss: 0.3104\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7420] loss: 0.3114\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  7430] loss: 0.3108\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7440] loss: 0.3239\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  7450] loss: 0.3115\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  7460] loss: 0.3066\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  7470] loss: 0.3137\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  7480] loss: 0.3241\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  7490] loss: 0.3161\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7500] loss: 0.3187\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7510] loss: 0.3215\n",
      "Accuracy on training dataset: 85.938 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  7520] loss: 0.3132\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  7530] loss: 0.3102\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7540] loss: 0.3171\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  7550] loss: 0.3150\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7560] loss: 0.3131\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  7570] loss: 0.3118\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  7580] loss: 0.3159\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7590] loss: 0.3084\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  7600] loss: 0.3109\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  7610] loss: 0.3162\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7620] loss: 0.3090\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  7630] loss: 0.3050\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  7640] loss: 0.3244\n",
      "Accuracy on training dataset: 84.375 %\n",
      "[step:  7650] loss: 0.3236\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  7660] loss: 0.3067\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  7670] loss: 0.3204\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  7680] loss: 0.3159\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7690] loss: 0.3192\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7700] loss: 0.3163\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7710] loss: 0.3071\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  7720] loss: 0.3174\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  7730] loss: 0.3140\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7740] loss: 0.3101\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7750] loss: 0.3202\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  7760] loss: 0.3122\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  7770] loss: 0.3136\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7780] loss: 0.3148\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7790] loss: 0.3211\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  7800] loss: 0.3162\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  7810] loss: 0.3141\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7820] loss: 0.3069\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  7830] loss: 0.3167\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  7840] loss: 0.3078\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  7850] loss: 0.3142\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7860] loss: 0.3138\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7870] loss: 0.3110\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  7880] loss: 0.3180\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7890] loss: 0.3074\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  7900] loss: 0.3168\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  7910] loss: 0.3127\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  7920] loss: 0.3093\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  7930] loss: 0.3184\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7940] loss: 0.3121\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  7950] loss: 0.3187\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  7960] loss: 0.3193\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  7970] loss: 0.3053\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  7980] loss: 0.3144\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  7990] loss: 0.3156\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  8000] loss: 0.3061\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  8010] loss: 0.3047\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  8020] loss: 0.3038\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  8030] loss: 0.3163\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  8040] loss: 0.3190\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  8050] loss: 0.3101\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  8060] loss: 0.3179\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  8070] loss: 0.3139\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  8080] loss: 0.3117\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  8090] loss: 0.3168\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  8100] loss: 0.3143\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  8110] loss: 0.3093\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  8120] loss: 0.3149\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  8130] loss: 0.3063\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  8140] loss: 0.3203\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  8150] loss: 0.3223\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  8160] loss: 0.3104\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  8170] loss: 0.3132\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  8180] loss: 0.3106\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  8190] loss: 0.3119\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  8200] loss: 0.3090\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  8210] loss: 0.3177\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  8220] loss: 0.3139\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  8230] loss: 0.3218\n",
      "Accuracy on training dataset: 85.156 %\n",
      "[step:  8240] loss: 0.3142\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  8250] loss: 0.3176\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  8260] loss: 0.3079\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8270] loss: 0.3188\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  8280] loss: 0.3203\n",
      "Accuracy on training dataset: 85.938 %\n",
      "[step:  8290] loss: 0.3076\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8300] loss: 0.3140\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  8310] loss: 0.3165\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  8320] loss: 0.3183\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  8330] loss: 0.3105\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  8340] loss: 0.3097\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  8350] loss: 0.3165\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  8360] loss: 0.3105\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  8370] loss: 0.3107\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  8380] loss: 0.3119\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  8390] loss: 0.3163\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  8400] loss: 0.3137\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  8410] loss: 0.3113\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  8420] loss: 0.3150\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  8430] loss: 0.3133\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  8440] loss: 0.3120\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  8450] loss: 0.3078\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8460] loss: 0.3074\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8470] loss: 0.3145\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  8480] loss: 0.3070\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  8490] loss: 0.3170\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  8500] loss: 0.3174\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  8510] loss: 0.3177\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  8520] loss: 0.3168\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  8530] loss: 0.3223\n",
      "Accuracy on training dataset: 83.594 %\n",
      "[step:  8540] loss: 0.3097\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  8550] loss: 0.3102\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  8560] loss: 0.3132\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  8570] loss: 0.3165\n",
      "Accuracy on training dataset: 87.500 %\n",
      "[step:  8580] loss: 0.3107\n",
      "Accuracy on training dataset: 90.625 %\n",
      "[step:  8590] loss: 0.3087\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8600] loss: 0.3081\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8610] loss: 0.3126\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  8620] loss: 0.3114\n",
      "Accuracy on training dataset: 89.844 %\n",
      "[step:  8630] loss: 0.3148\n",
      "Accuracy on training dataset: 88.281 %\n",
      "[step:  8640] loss: 0.3192\n",
      "Accuracy on training dataset: 86.719 %\n",
      "[step:  8650] loss: 0.3089\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  8660] loss: 0.3093\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8670] loss: 0.3078\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8680] loss: 0.3161\n",
      "Accuracy on training dataset: 89.062 %\n",
      "[step:  8690] loss: 0.3113\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  8700] loss: 0.3111\n",
      "Accuracy on training dataset: 92.188 %\n",
      "[step:  8710] loss: 0.3110\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8720] loss: 0.3130\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  8730] loss: 0.3118\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8740] loss: 0.3182\n",
      "Accuracy on training dataset: 91.406 %\n",
      "[step:  8750] loss: 0.3107\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  8760] loss: 0.3084\n",
      "Accuracy on training dataset: 94.531 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  8770] loss: 0.3116\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8780] loss: 0.3112\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  8790] loss: 0.3108\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  8800] loss: 0.3086\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  8810] loss: 0.3048\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  8820] loss: 0.3099\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  8830] loss: 0.3085\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  8840] loss: 0.3098\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  8850] loss: 0.3099\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  8860] loss: 0.3112\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  8870] loss: 0.3062\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  8880] loss: 0.3098\n",
      "Accuracy on training dataset: 92.969 %\n",
      "[step:  8890] loss: 0.3042\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  8900] loss: 0.3139\n",
      "Accuracy on training dataset: 93.750 %\n",
      "[step:  8910] loss: 0.3067\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  8920] loss: 0.3065\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  8930] loss: 0.3033\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  8940] loss: 0.3050\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  8950] loss: 0.3008\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  8960] loss: 0.3096\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  8970] loss: 0.3052\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  8980] loss: 0.3083\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  8990] loss: 0.3084\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  9000] loss: 0.3044\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9010] loss: 0.3081\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9020] loss: 0.3063\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9030] loss: 0.3076\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  9040] loss: 0.3073\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9050] loss: 0.3039\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9060] loss: 0.3090\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9070] loss: 0.3084\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9080] loss: 0.3076\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9090] loss: 0.3077\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9100] loss: 0.3025\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9110] loss: 0.3088\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  9120] loss: 0.3039\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9130] loss: 0.3072\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9140] loss: 0.3050\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9150] loss: 0.3092\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  9160] loss: 0.3061\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9170] loss: 0.3100\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9180] loss: 0.3076\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9190] loss: 0.3067\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9200] loss: 0.3044\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9210] loss: 0.3055\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9220] loss: 0.3106\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9230] loss: 0.3078\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9240] loss: 0.3089\n",
      "Accuracy on training dataset: 94.531 %\n",
      "[step:  9250] loss: 0.3048\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9260] loss: 0.3034\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9270] loss: 0.3030\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9280] loss: 0.3067\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9290] loss: 0.3037\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9300] loss: 0.3081\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9310] loss: 0.3056\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9320] loss: 0.3079\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9330] loss: 0.3047\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9340] loss: 0.3091\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9350] loss: 0.3086\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9360] loss: 0.3054\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9370] loss: 0.3050\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9380] loss: 0.3039\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9390] loss: 0.3011\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9400] loss: 0.3058\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9410] loss: 0.3031\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9420] loss: 0.3074\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9430] loss: 0.3099\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  9440] loss: 0.3019\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9450] loss: 0.3024\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9460] loss: 0.3083\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9470] loss: 0.3059\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9480] loss: 0.3023\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9490] loss: 0.3028\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9500] loss: 0.3056\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9510] loss: 0.3056\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9520] loss: 0.3025\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9530] loss: 0.3042\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9540] loss: 0.3031\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9550] loss: 0.3111\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  9560] loss: 0.3076\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9570] loss: 0.3046\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9580] loss: 0.3063\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9590] loss: 0.3021\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9600] loss: 0.3004\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9610] loss: 0.3063\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9620] loss: 0.3015\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9630] loss: 0.3041\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9640] loss: 0.3056\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9650] loss: 0.3010\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9660] loss: 0.3022\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9670] loss: 0.3055\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9680] loss: 0.3020\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9690] loss: 0.3049\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9700] loss: 0.3048\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9710] loss: 0.3042\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9720] loss: 0.3009\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9730] loss: 0.3073\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9740] loss: 0.3026\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9750] loss: 0.3044\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9760] loss: 0.3069\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9770] loss: 0.3009\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9780] loss: 0.3049\n",
      "Accuracy on training dataset: 96.094 %\n",
      "[step:  9790] loss: 0.3059\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9800] loss: 0.3054\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9810] loss: 0.2998\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9820] loss: 0.2999\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9830] loss: 0.3047\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9840] loss: 0.3051\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9850] loss: 0.3022\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9860] loss: 0.3036\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9870] loss: 0.3005\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9880] loss: 0.3025\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step:  9890] loss: 0.3017\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9900] loss: 0.3014\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9910] loss: 0.3060\n",
      "Accuracy on training dataset: 95.312 %\n",
      "[step:  9920] loss: 0.3003\n",
      "Accuracy on training dataset: 100.000 %\n",
      "[step:  9930] loss: 0.3034\n",
      "Accuracy on training dataset: 96.875 %\n",
      "[step:  9940] loss: 0.3017\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9950] loss: 0.3018\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9960] loss: 0.3003\n",
      "Accuracy on training dataset: 98.438 %\n",
      "[step:  9970] loss: 0.3021\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9980] loss: 0.3001\n",
      "Accuracy on training dataset: 99.219 %\n",
      "[step:  9990] loss: 0.3024\n",
      "Accuracy on training dataset: 97.656 %\n",
      "[step: 10000] loss: 0.3030\n",
      "Accuracy on training dataset: 96.875 %\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "model5, losses5, accuracies5 = train_2(config, 24, 128, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Trend')\n",
    "\n",
    "plt.plot(losses5)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Loss')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('Train Accuracy')\n",
    "\n",
    "plt.plot(accuracies5)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done testing.\n"
     ]
    }
   ],
   "source": [
    "test_accuracies5 = test(model5, config, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palindromes Length: T = 25\n",
      "Average accuracy over 2000 sampled test: 98.56576492537313 %\n"
     ]
    }
   ],
   "source": [
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.plot(test_accuracies5)\n",
    "    \n",
    "plt.xlabel('Sample time')\n",
    "    \n",
    "plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(\"Palindromes Length: T = 25\")\n",
    "print(\"Average accuracy over 2000 sampled test: \" + str(np.mean(test_accuracies5)) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Here follows a figure which shows the accuracy on test data w.r.t. corresponding palindromes length, comparing between RNN and LSTM.\n",
    "\n",
    "For vanilla RNN, as can be seen from `./train_RNN.ipynb`:\n",
    "\n",
    "- when palindromes length = 5, accuracy on test data = 100%  (learning rate: 0.001)\n",
    "- when palindromes length = 11, accuracy on test data = 89.87%  (learning rate: 0.001)\n",
    "- when palindromes length = 15, accuracy on test data = 90.09%  (learning rate: 0.0004)\n",
    "- when palindromes length = 20, accuracy on test data = 89.98%  (learning rate: 0.0002)\n",
    "- when palindromes length = 25, accuracy on test data = 9.94%  (learning rate: 0.0001)\n",
    "\n",
    "For LSTM, as can be seen here:\n",
    "\n",
    "- when palindromes length = 5, accuracy on test data = 100%  (learning rate: 0.001)\n",
    "- when palindromes length = 11, accuracy on test data = 100%  (learning rate: 0.001)\n",
    "- when palindromes length = 15, accuracy on test data = 100%  (learning rate: 0.001)\n",
    "- when palindromes length = 20, accuracy on test data = 100%  (learning rate: 0.0002)\n",
    "- when palindromes length = 25, accuracy on test data = 98.57%  (learning rate: 0.0001)\n",
    "\n",
    "*Tips: When input sequence length increases, I use the smaller learning rate.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1 = [5, 11, 15, 20, 25]\n",
    "p2 = [100, 89.87, 90.09, 89.98, 9.94]\n",
    "\n",
    "plt.title('Accuracy w.r.t. palindromes length.')\n",
    "plt.plot(p1, p2, color='red', label='RNN')\n",
    "plt.scatter(p1, p2, color='red')\n",
    "\n",
    "p3 = [100, 100, 100, 100, 98.57]\n",
    "plt.plot(p1, p3, color='green', label='LSTM')\n",
    "plt.scatter(p1, p3, color='green')\n",
    "\n",
    "plt.xlabel('Palindromes Length')\n",
    "    \n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
